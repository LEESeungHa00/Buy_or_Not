import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import requests
from datetime import datetime, timedelta, timezone
from functools import reduce
import json
import urllib.request
from google.oauth2 import service_account
from google.cloud import bigquery, language_v1
import pandas_gbq
import feedparser
from urllib.parse import quote
from statsmodels.tsa.seasonal import seasonal_decompose
from prophet import Prophet
from prophet.plot import plot_plotly

# ==============================================================================
# --- 1. Constants and Configuration ---
# ==============================================================================
BQ_DATASET = "data_explorer"
BQ_TABLE_NAVER = "naver_trends_cache"
BQ_TABLE_NEWS = "news_sentiment_google_nlp" # ìƒˆ í…Œì´ë¸” ì´ë¦„

KAMIS_FULL_DATA = {
    'ìŒ€': {'cat_code': '100', 'item_code': '111', 'kinds': {'20kg': '01', 'ë°±ë¯¸': '02'}},
    'ê°ì': {'cat_code': '100', 'item_code': '152', 'kinds': {'ìˆ˜ë¯¸(ë…¸ì§€)': '01', 'ìˆ˜ë¯¸(ì‹œì„¤)': '04'}},
    'ë°°ì¶”': {'cat_code': '200', 'item_code': '211', 'kinds': {'ë´„': '01', 'ì—¬ë¦„': '02', 'ê°€ì„': '03'}},
    'ì–‘íŒŒ': {'cat_code': '200', 'item_code': '245', 'kinds': {'ì–‘íŒŒ': '00', 'í–‡ì–‘íŒŒ': '02'}},
    'ì‚¬ê³¼': {'cat_code': '400', 'item_code': '411', 'kinds': {'í›„ì§€': '05', 'ì•„ì˜¤ë¦¬': '06'}},
    'ë°”ë‚˜ë‚˜': {'cat_code': '400', 'item_code': '418', 'kinds': {'ìˆ˜ì…': '02'}},
    'ì•„ë³´ì¹´ë„': {'cat_code': '400', 'item_code': '430', 'kinds': {'ìˆ˜ì…': '00'}},
    'ê³ ë“±ì–´': {'cat_code': '600', 'item_code': '611', 'kinds': {'ìƒì„ ': '01', 'ëƒ‰ë™': '02'}},
}

# ==============================================================================
# --- 2. GCP Connection and Helper Functions ---
# ==============================================================================

@st.cache_resource
def get_bq_connection():
    """BigQueryì— ì—°ê²°í•˜ê³  í´ë¼ì´ì–¸íŠ¸ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        creds_dict = st.secrets["gcp_service_account"]
        creds = service_account.Credentials.from_service_account_info(creds_dict)
        client = bigquery.Client(credentials=creds, project=creds.project_id)
        return client
    except Exception as e:
        st.error(f"Google BigQuery ì—°ê²° ì‹¤íŒ¨: secrets.tomlì„ í™•ì¸í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}")
        return None

@st.cache_resource
def get_gcp_nlp_client():
    """Google Cloud Natural Language API í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        creds_dict = st.secrets["gcp_service_account"]
        creds = service_account.Credentials.from_service_account_info(creds_dict)
        nlp_client = language_v1.LanguageServiceClient(credentials=creds)
        return nlp_client
    except Exception as e:
        st.error(f"Google NLP ì—°ê²° ì‹¤íŒ¨: {e}")
        return None

def ensure_news_table_exists(client):
    """ìƒˆë¡œìš´ ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼ì— ë§ëŠ” ìŠ¤í‚¤ë§ˆë¡œ BigQuery í…Œì´ë¸”ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤."""
    project_id = client.project
    full_table_id = f"{project_id}.{BQ_DATASET}.{BQ_TABLE_NEWS}"
    try:
        client.get_table(full_table_id)
    except Exception:
        st.write(f"ë‰´ìŠ¤ ë¶„ì„ í…Œì´ë¸” '{full_table_id}'ì„ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        schema = [
            bigquery.SchemaField("ë‚ ì§œ", "DATE"),
            bigquery.SchemaField("Title", "STRING"),
            bigquery.SchemaField("Keyword", "STRING"),
            bigquery.SchemaField("Sentiment", "FLOAT"),
            bigquery.SchemaField("Magnitude", "FLOAT"),
            bigquery.SchemaField("InsertedAt", "TIMESTAMP"),
        ]
        table = bigquery.Table(full_table_id, schema=schema)
        client.create_table(table)
        st.success(f"í…Œì´ë¸” '{BQ_TABLE_NEWS}' ìƒì„± ì™„ë£Œ.")

def call_naver_api(url, body, naver_keys):
    """ë„¤ì´ë²„ APIë¥¼ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", naver_keys['id'])
        request.add_header("X-Naver-Client-Secret", naver_keys['secret'])
        request.add_header("Content-Type", "application/json")
        response = urllib.request.urlopen(request, data=body.encode("utf-8"))
        if response.getcode() == 200:
            return json.loads(response.read().decode('utf-8'))
        return None
    except Exception as e:
        st.error(f"Naver API ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# ==============================================================================
# --- 3. Main Data Fetching Functions ---
# ==============================================================================

def get_trade_data_from_bq(client, categories):
    """ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ìˆ˜ì¶œì… ë°ì´í„°ë¥¼ BigQueryì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    # (ì´í•˜ í•¨ìˆ˜ ë‚´ìš©ì€ ì´ì „ê³¼ ë™ì¼)
    if not categories: return pd.DataFrame()
    project_id = client.project
    table_id = f"{project_id}.{BQ_DATASET}.tds_data"
    try:
        query_params = [bigquery.ArrayQueryParameter("categories", "STRING", categories)]
        sql = f"SELECT * FROM `{table_id}` WHERE Category IN UNNEST(@categories)"
        job_config = bigquery.QueryJobConfig(query_parameters=query_params)
        df = client.query(sql, job_config=job_config).to_dataframe()
        for col in df.columns:
            if 'price' in col.lower() or 'value' in col.lower() or 'volume' in col.lower():
                df[col] = pd.to_numeric(df[col], errors='coerce')
        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
        return df
    except Exception as e:
        st.error(f"BigQueryì—ì„œ TDS ë°ì´í„° ì½ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

@st.cache_data(ttl=3600)
def fetch_naver_trends_data(_client, keywords, start_date, end_date, naver_keys):
    """BigQuery ìºì‹œë¥¼ í™œìš©í•˜ì—¬ ë„¤ì´ë²„ ë°ì´í„°ë© ë°ì´í„°ë¥¼ ê¸´ ê¸°ê°„ì— ëŒ€í•´ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    # (ì´í•˜ í•¨ìˆ˜ ë‚´ìš©ì€ ì´ì „ê³¼ ë™ì¼)
    project_id = _client.project
    table_id = f"{project_id}.{BQ_DATASET}.{BQ_TABLE_NAVER}"

    try:
        sql = f"SELECT * FROM `{table_id}` ORDER BY ë‚ ì§œ"
        df_cache = _client.query(sql).to_dataframe()
        if not df_cache.empty:
            df_cache['ë‚ ì§œ'] = pd.to_datetime(df_cache['ë‚ ì§œ'])
    except Exception:
        df_cache = pd.DataFrame(columns=['ë‚ ì§œ'])

    fetch_start_date = start_date
    if not df_cache.empty:
        last_cached_date = df_cache['ë‚ ì§œ'].max()
        if start_date > last_cached_date:
            fetch_start_date = start_date
        elif end_date > last_cached_date:
            fetch_start_date = last_cached_date + timedelta(days=1)
        else:
            fetch_start_date = end_date + timedelta(days=1)

    new_data_list = []
    if fetch_start_date <= end_date:
        current_start = fetch_start_date
        while current_start <= end_date:
            current_end = current_start + timedelta(days=89)
            if current_end > end_date:
                current_end = end_date
            
            # (ì´í•˜ API í˜¸ì¶œ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼)
            NAVER_SHOPPING_CAT_MAP = {'ì•„ë³´ì¹´ë„': "50000007", 'ë°”ë‚˜ë‚˜': "50000007", 'ì‚¬ê³¼': "50000007"}
            all_data_chunk = []
            for keyword in keywords:
                keyword_dfs = []
                body_search = json.dumps({"startDate": current_start.strftime('%Y-%m-%d'), "endDate": current_end.strftime('%Y-%m-%d'), "timeUnit": "date", "keywordGroups": [{"groupName": keyword, "keywords": [keyword]}]})
                search_res = call_naver_api("https://openapi.naver.com/v1/datalab/search", body_search, naver_keys)
                if search_res and search_res.get('results') and search_res['results'][0]['data']:
                    df_search = pd.DataFrame(search_res['results'][0]['data'])
                    if not df_search.empty: keyword_dfs.append(df_search.rename(columns={'period': 'ë‚ ì§œ', 'ratio': f'NaverSearch_{keyword}'}))
                
                if keyword.lower().replace(' ', '') in NAVER_SHOPPING_CAT_MAP:
                    category_id = NAVER_SHOPPING_CAT_MAP[keyword.lower().replace(' ', '')]
                    body_shop = json.dumps({"startDate": current_start.strftime('%Y-%m-%d'),"endDate": current_end.strftime('%Y-%m-%d'), "timeUnit": "date", "category": [{"name": keyword, "param": [category_id]}], "keyword": [{"name": keyword, "param": [keyword]}]})
                    shop_res = call_naver_api("https://openapi.naver.com/v1/datalab/shopping/categories", body_shop, naver_keys)
                    if shop_res and shop_res.get('results') and shop_res['results'][0]['data']:
                        df_shop = pd.DataFrame(shop_res['results'][0]['data'])
                        if not df_shop.empty: keyword_dfs.append(df_shop.rename(columns={'period': 'ë‚ ì§œ', 'ratio': f'NaverShop_{keyword}'}))
                
                if keyword_dfs:
                    for df in keyword_dfs: df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
                    merged_df = reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), keyword_dfs)
                    all_data_chunk.append(merged_df)
            
            if all_data_chunk:
                new_data_list.append(reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), all_data_chunk))
            current_start = current_end + timedelta(days=1)
    else:
        st.sidebar.success("âœ”ï¸ ë„¤ì´ë²„ íŠ¸ë Œë“œ: ëª¨ë“  ë°ì´í„°ê°€ ìºì‹œì— ìˆìŠµë‹ˆë‹¤.")

    if new_data_list:
        non_empty_dfs = [df for df in new_data_list if not df.empty]
        if non_empty_dfs:
            df_new = pd.concat(non_empty_dfs, ignore_index=True)
            df_new['ë‚ ì§œ'] = pd.to_datetime(df_new['ë‚ ì§œ'])
            df_combined = pd.concat([df_cache, df_new], ignore_index=True).drop_duplicates(subset=['ë‚ ì§œ'], keep='last').sort_values(by='ë‚ ì§œ').reset_index(drop=True)
            pandas_gbq.to_gbq(df_combined, table_id, project_id=project_id, if_exists="replace", credentials=_client._credentials)
            df_final = df_combined
        else:
            df_final = df_cache
    else:
        df_final = df_cache

    if df_final.empty: return pd.DataFrame()
    return df_final[(df_final['ë‚ ì§œ'] >= start_date) & (df_final['ë‚ ì§œ'] <= end_date)].reset_index(drop=True)

def fetch_kamis_data(client, item_info, start_date, end_date, kamis_keys):
    """KAMISì—ì„œ ê¸°ê°„ë³„ ë„ë§¤ ê°€ê²© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    # (ì´í•˜ í•¨ìˆ˜ ë‚´ìš©ì€ ì´ì „ê³¼ ë™ì¼)
    start_str, end_str = start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')
    item_code, kind_code = item_info['item_code'], item_info['kind_code']
    url = (f"http://www.kamis.or.kr/service/price/xml.do?action=periodWholesaleProductList"
           f"&p_product_cls_code=01&p_startday={start_str}&p_endday={end_str}"
           f"&p_item_category_code={item_info['cat_code']}&p_item_code={item_code}&p_kind_code={kind_code}"
           f"&p_product_rank_code={item_info['rank_code']}&p_convert_kg_yn=Y"
           f"&p_cert_key={kamis_keys['key']}&p_cert_id={kamis_keys['id']}&p_returntype=json")
    try:
        response = requests.get(url, timeout=20)
        if response.status_code == 200 and "data" in response.json() and "item" in response.json()["data"]:
            price_data = response.json()["data"]["item"]
            if not price_data: return pd.DataFrame()
            df_new = pd.DataFrame(price_data)[['regday', 'price']].rename(columns={'regday': 'ë‚ ì§œ', 'price': 'ë„ë§¤ê°€ê²©_ì›'})
            df_new['ë‚ ì§œ'] = pd.to_datetime(df_new['ë‚ ì§œ'])
            df_new['ë„ë§¤ê°€ê²©_ì›'] = pd.to_numeric(df_new['ë„ë§¤ê°€ê²©_ì›'].str.replace(',', ''), errors='coerce')
            return df_new
    except Exception as e:
        st.sidebar.error(f"KAMIS API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
    return pd.DataFrame()

def fetch_and_analyze_news_lightweight(_bq_client, _nlp_client, keyword, days_limit=7):
    """ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  Google NLPë¡œ ë¶„ì„ í›„, BigQueryì— ìºì‹±í•˜ëŠ” ê²½ëŸ‰í™”ëœ í•¨ìˆ˜."""
    # (ì´í•˜ í•¨ìˆ˜ ë‚´ìš©ì€ ì´ì „ê³¼ ë™ì¼)
    project_id = _bq_client.project
    full_table_id = f"{project_id}.{BQ_DATASET}.{BQ_TABLE_NEWS}"

    try:
        time_limit = datetime.now(timezone.utc) - timedelta(days=days_limit)
        query = f"SELECT * FROM `{full_table_id}` WHERE Keyword = @keyword AND InsertedAt >= @time_limit ORDER BY ë‚ ì§œ DESC"
        job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ScalarQueryParameter("keyword", "STRING", keyword), bigquery.ScalarQueryParameter("time_limit", "TIMESTAMP", time_limit)])
        df_cache = _bq_client.query(query, job_config=job_config).to_dataframe()
    except Exception:
        df_cache = pd.DataFrame()

    if not df_cache.empty:
        st.sidebar.success(f"âœ”ï¸ '{keyword}' ìµœì‹  ë‰´ìŠ¤ ê²°ê³¼ë¥¼ ìºì‹œì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return df_cache

    st.sidebar.warning(f"'{keyword}'ì— ëŒ€í•œ ìµœì‹  ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
    all_news = []
    rss_url = f"https://news.google.com/rss/search?q={quote(keyword)}&hl=ko&gl=KR&ceid=KR:ko"
    feed = feedparser.parse(rss_url)
    
    for entry in feed.entries[:20]:
        title = entry.get('title', '')
        if not title: continue
        try:
            pub_date = pd.to_datetime(entry.get('published')).date()
            all_news.append({"ë‚ ì§œ": pub_date, "Title": title})
        except: continue
    
    if not all_news: return pd.DataFrame()

    df_new = pd.DataFrame(all_news).drop_duplicates(subset=["Title"])
    
    def analyze_sentiment_with_google(text_content, nlp_client):
        if not text_content or not nlp_client: return 0.0, 0.0
        document = language_v1.Document(content=text_content, type_=language_v1.Document.Type.PLAIN_TEXT)
        response = nlp_client.analyze_sentiment(request={'document': document})
        return response.document_sentiment.score, response.document_sentiment.magnitude

    sentiments = [analyze_sentiment_with_google(title, _nlp_client) for title in df_new['Title']]
    df_new['Sentiment'] = [s[0] for s in sentiments]
    df_new['Magnitude'] = [s[1] for s in sentiments]
    df_new['Keyword'] = keyword
    df_new['InsertedAt'] = datetime.now(timezone.utc)
    
    if not df_new.empty:
        df_to_gbq = df_new[["ë‚ ì§œ", "Title", "Keyword", "Sentiment", "Magnitude", "InsertedAt"]]
        pandas_gbq.to_gbq(df_to_gbq, full_table_id, project_id=project_id, if_exists="append", credentials=_bq_client._credentials)
    
    return df_to_gbq

# ==============================================================================
# --- 4. Streamlit App Main Logic ---
# ==============================================================================

st.set_page_config(layout="wide")
st.title("ğŸ“Š ë°ì´í„° íƒìƒ‰ ë° í†µí•© ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

# --- Initialize GCP Clients ---
bq_client = get_bq_connection()
nlp_client = get_gcp_nlp_client()
if bq_client is None or nlp_client is None:
    st.stop()

# --- Initialize News Table in BigQuery ---
ensure_news_table_exists(bq_client)

# --- Initialize Session State ---
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False

# --- Sidebar UI ---
st.sidebar.header("âš™ï¸ ë¶„ì„ ì„¤ì •")

# 1. Select Main Data
st.sidebar.subheader("1. ë¶„ì„ ëŒ€ìƒ í’ˆëª© ì„ íƒ")
categories = get_categories_from_bq(bq_client)
if not categories:
    st.sidebar.warning("BigQueryì— ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    selected_categories = st.sidebar.multiselect(
        "ë¶„ì„í•  í’ˆëª© ì¹´í…Œê³ ë¦¬ ì„ íƒ", categories, default=st.session_state.get('selected_categories', [])
    )
    if st.sidebar.button("ğŸš€ ì„ íƒ í’ˆëª© ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"):
        if not selected_categories:
            st.sidebar.warning("ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        else:
            df = get_trade_data_from_bq(bq_client, selected_categories)
            if df is not None and not df.empty:
                st.session_state.raw_trade_df = df
                st.session_state.data_loaded = True
                st.session_state.selected_categories = selected_categories
                st.rerun() # Reload to update the main page
            else:
                st.sidebar.error("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

# Stop if main data is not loaded yet
if not st.session_state.data_loaded:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ê³  ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    st.stop()

# --- Main App Logic continues only if data is loaded ---
raw_trade_df = st.session_state.raw_trade_df
st.sidebar.success(f"**{', '.join(st.session_state.selected_categories)}** ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
st.sidebar.markdown("---")

# 2. Set Date Range & Keywords
st.sidebar.subheader("2. ë¶„ì„ ê¸°ê°„ ë° í‚¤ì›Œë“œ ì„¤ì •")
file_start_date = raw_trade_df['Date'].min()
file_end_date = raw_trade_df['Date'].max()
start_date = pd.to_datetime(st.sidebar.date_input('ì‹œì‘ì¼', file_start_date, min_value=file_start_date, max_value=file_end_date))
end_date = pd.to_datetime(st.sidebar.date_input('ì¢…ë£Œì¼', file_end_date, min_value=start_date, max_value=file_end_date))
default_keywords = ", ".join(st.session_state.selected_categories)
keyword_input = st.sidebar.text_input("íŠ¸ë Œë“œ/ë‰´ìŠ¤ ë¶„ì„ í‚¤ì›Œë“œ", default_keywords)
search_keywords = [k.strip() for k in keyword_input.split(',') if k.strip()]

# 3. Fetch External Data
st.sidebar.subheader("3. ì™¸ë¶€ ë°ì´í„° ì—°ë™")
with st.sidebar.expander("ğŸ”‘ API í‚¤ ì…ë ¥"):
    kamis_api_key = st.text_input("KAMIS API Key", type="password")
    kamis_api_id = st.text_input("KAMIS API ID", type="password")
    naver_client_id = st.text_input("Naver API Client ID", type="password")
    naver_client_secret = st.text_input("Naver API Client Secret", type="password")

if st.sidebar.button("ğŸ”— ëª¨ë“  ì™¸ë¶€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
    # Fetch KAMIS Data
    with st.spinner("KAMIS ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
        # For simplicity, we fetch for the first selected category if it's in KAMIS_FULL_DATA
        kamis_item_name = next((cat for cat in st.session_state.selected_categories if cat in KAMIS_FULL_DATA), None)
        if kamis_item_name and kamis_api_key and kamis_api_id:
            item_info = KAMIS_FULL_DATA[kamis_item_name]
            item_info['item_code'] = item_info['item_code']
            item_info['kind_code'] = list(item_info['kinds'].values())[0] # Default to first kind
            item_info['rank_code'] = '01'
            st.session_state.wholesale_data = fetch_kamis_data(bq_client, item_info, start_date, end_date, {'key': kamis_api_key, 'id': kamis_api_id})
        elif not kamis_item_name:
            st.sidebar.info("ì„ íƒëœ í’ˆëª©ì— ëŒ€í•œ KAMIS ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.sidebar.warning("KAMIS API í‚¤ì™€ IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # Fetch Naver Trend Data
    with st.spinner("ë„¤ì´ë²„ íŠ¸ë Œë“œ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
        if search_keywords and naver_client_id and naver_client_secret:
            st.session_state.search_data = fetch_naver_trends_data(bq_client, search_keywords, start_date, end_date, {'id': naver_client_id, 'secret': naver_client_secret})
        elif not search_keywords:
            st.sidebar.warning("íŠ¸ë Œë“œ ë¶„ì„ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            st.sidebar.warning("Naver API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            
    # Fetch News Sentiment Data
    with st.spinner("ë‰´ìŠ¤ ê°ì„± ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
        if search_keywords:
            # For simplicity, fetch for the first keyword
            st.session_state.news_data = fetch_and_analyze_news_lightweight(bq_client, nlp_client, search_keywords[0])
        else:
            st.sidebar.warning("ë‰´ìŠ¤ ë¶„ì„ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# --- Main Display Tabs ---
raw_wholesale_df = st.session_state.get('wholesale_data', pd.DataFrame())
raw_search_df = st.session_state.get('search_data', pd.DataFrame())
raw_news_df = st.session_state.get('news_data', pd.DataFrame())

tab1, tab2, tab3, tab4, tab5 = st.tabs(["1ï¸âƒ£ ì›ë³¸ ë°ì´í„°", "2ï¸âƒ£ ë°ì´í„° í‘œì¤€í™”", "3ï¸âƒ£ ë‰´ìŠ¤ ê°ì„± ë¶„ì„", "4ï¸âƒ£ ìƒê´€ê´€ê³„ ë¶„ì„", "ğŸ“ˆ ì‹œê³„ì—´ ì˜ˆì¸¡"])

with tab1:
    st.subheader("A. ìˆ˜ì¶œì… ë°ì´í„° (ì„ íƒ ê¸°ê°„)")
    st.dataframe(raw_trade_df[(raw_trade_df['Date'] >= start_date) & (raw_trade_df['Date'] <= end_date)])
    st.subheader("B. ì™¸ë¶€ ê°€ê²© ë°ì´í„°"); st.dataframe(raw_wholesale_df)
    st.subheader("C. íŠ¸ë Œë“œ ë°ì´í„°"); st.dataframe(raw_search_df)
    st.subheader("D. ë‰´ìŠ¤ ë°ì´í„°"); st.dataframe(raw_news_df)
    
with tab2:
    st.header("ë°ì´í„° í‘œì¤€í™”: ì£¼ë³„(Weekly) ë°ì´í„°ë¡œ ë³€í™˜")
    # (ì´í•˜ íƒ­ ë‚´ìš©ì€ ì´ì „ê³¼ ë™ì¼)
    trade_df_in_range = raw_trade_df[(raw_trade_df['Date'] >= start_date) & (raw_trade_df['Date'] <= end_date)]
    filtered_trade_df = trade_df_in_range[trade_df_in_range['Category'].isin(st.session_state.selected_categories)].copy()
    
    if not filtered_trade_df.empty:
        filtered_trade_df.set_index('Date', inplace=True)
        trade_weekly = filtered_trade_df.resample('W-Mon').agg(ìˆ˜ì…ì•¡_USD=('Value', 'sum'), ìˆ˜ì…ëŸ‰_KG=('Volume', 'sum')).copy()
        trade_weekly['ìˆ˜ì…ë‹¨ê°€_USD_KG'] = trade_weekly['ìˆ˜ì…ì•¡_USD'] / trade_weekly['ìˆ˜ì…ëŸ‰_KG']
        trade_weekly.index.name = 'ë‚ ì§œ'
        
        dfs_to_process = {'wholesale': raw_wholesale_df, 'search': raw_search_df, 'news': raw_news_df}
        weekly_dfs = {'trade': trade_weekly}

        for name, df in dfs_to_process.items():
            if not df.empty and 'ë‚ ì§œ' in df.columns:
                df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
                df_in_range = df[(df['ë‚ ì§œ'] >= start_date) & (df['ë‚ ì§œ'] <= end_date)]
                if not df_in_range.empty:
                    df_weekly = df_in_range.set_index('ë‚ ì§œ').resample('W-Mon').mean(numeric_only=True)
                    df_weekly.index.name = 'ë‚ ì§œ'
                    weekly_dfs[name] = df_weekly

        st.session_state['weekly_dfs'] = weekly_dfs
        st.write("### ì£¼ë³„ ì§‘ê³„ ë°ì´í„° ìƒ˜í”Œ")
        for name, df in weekly_dfs.items():
            st.write(f"##### {name.capitalize()} Data (Weekly)")
            st.dataframe(df.head())
    else:
        st.warning("ì„ íƒëœ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ìˆ˜ì¶œì… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

with tab3:
    st.header("ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ê²°ê³¼")
    if not raw_news_df.empty:
        news_weekly = st.session_state.get('weekly_dfs', {}).get('news', pd.DataFrame())
        if not news_weekly.empty:
            fig = px.line(news_weekly, y='Sentiment', title="ì£¼ë³„ í‰ê·  ë‰´ìŠ¤ ê°ì„± ì ìˆ˜", labels={'Sentiment': 'ê°ì„± ì ìˆ˜'})
            fig.add_hline(y=0, line_dash="dash", line_color="red")
            st.plotly_chart(fig, use_container_width=True)
        st.markdown("---")
        st.subheader("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ (ìµœì‹ ìˆœ)")
        st.dataframe(raw_news_df.sort_values(by='ë‚ ì§œ', ascending=False))
    else:
        st.info("ì‚¬ì´ë“œë°”ì—ì„œ ì™¸ë¶€ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì£¼ì„¸ìš”.")

with tab4:
    st.header("ìƒê´€ê´€ê³„ ë¶„ì„")
    if 'weekly_dfs' in st.session_state:
        weekly_dfs = st.session_state['weekly_dfs']
        dfs_to_concat = [df for df in weekly_dfs.values() if not df.empty]
        if len(dfs_to_concat) > 1:
            final_df = reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), dfs_to_concat)
            final_df = final_df.interpolate(method='linear', limit_direction='both').dropna(how='all', axis=1).dropna()
            st.session_state['final_df'] = final_df
            
            st.subheader("í†µí•© ë°ì´í„° ì‹œê°í™”")
            df_long = final_df.reset_index().melt(id_vars='ë‚ ì§œ', var_name='ë°ì´í„° ì¢…ë¥˜', value_name='ê°’')
            fig = px.line(df_long, x='ë‚ ì§œ', y='ê°’', color='ë°ì´í„° ì¢…ë¥˜', title="í†µí•© ë°ì´í„° ì‹œê³„ì—´ ì¶”ì´")
            st.plotly_chart(fig, use_container_width=True)

            if len(final_df.columns) > 1:
                st.subheader("ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ")
                corr_matrix = final_df.corr(numeric_only=True)
                fig_heatmap = px.imshow(corr_matrix, text_auto=True, aspect="auto", color_continuous_scale='RdBu_r', range_color=[-1, 1])
                st.plotly_chart(fig_heatmap, use_container_width=True)
        else:
            st.warning("ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•´ ë‘˜ ì´ìƒì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        st.warning("2ë‹¨ê³„ì—ì„œ ë°ì´í„°ê°€ ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

with tab5:
    st.header("ì‹œê³„ì—´ ë¶„í•´ ë° ì˜ˆì¸¡ (by Prophet)")
    if 'final_df' in st.session_state and not st.session_state['final_df'].empty:
        final_df = st.session_state['final_df']
        forecast_col = st.selectbox("ì˜ˆì¸¡ ëŒ€ìƒ ë³€ìˆ˜ ì„ íƒ", final_df.columns)
        if st.button("ğŸ“ˆ ì„ íƒí•œ ë³€ìˆ˜ë¡œ ì˜ˆì¸¡ ì‹¤í–‰í•˜ê¸°"):
            ts_data = final_df[[forecast_col]].dropna()
            if len(ts_data) < 24:
                st.warning(f"ìµœì†Œ 24ì£¼ ì´ìƒì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬: {len(ts_data)}ì£¼")
            else:
                # (ì´í•˜ ì˜ˆì¸¡ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼)
                with st.spinner(f"'{forecast_col}' ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì¤‘..."):
                    st.subheader(f"'{forecast_col}' ì‹œê³„ì—´ ë¶„í•´")
                    period = 52 if len(ts_data) >= 104 else max(4, int(len(ts_data) / 2))
                    decomposition = seasonal_decompose(ts_data[forecast_col], model='additive', period=period)
                    fig_decompose = go.Figure()
                    fig_decompose.add_trace(go.Scatter(x=decomposition.observed.index, y=decomposition.observed, name='Observed'))
                    fig_decompose.add_trace(go.Scatter(x=decomposition.trend.index, y=decomposition.trend, name='Trend'))
                    fig_decompose.add_trace(go.Scatter(x=decomposition.seasonal.index, y=decomposition.seasonal, name='Seasonal'))
                    st.session_state['fig_decompose'] = fig_decompose

                    st.subheader(f"'{forecast_col}' ë¯¸ë˜ 12ì£¼ ì˜ˆì¸¡")
                    prophet_df = ts_data.reset_index().rename(columns={'ë‚ ì§œ': 'ds', forecast_col: 'y'})
                    m = Prophet()
                    m.fit(prophet_df)
                    future = m.make_future_dataframe(periods=12, freq='W')
                    forecast = m.predict(future)
                    fig_forecast = plot_plotly(m, forecast)
                    st.session_state['fig_forecast'] = fig_forecast
                    st.session_state['forecast_data'] = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(12)
        
        if 'fig_decompose' in st.session_state:
            st.plotly_chart(st.session_state['fig_decompose'], use_container_width=True)
        if 'fig_forecast' in st.session_state:
            st.plotly_chart(st.session_state['fig_forecast'], use_container_width=True)
            st.dataframe(st.session_state['forecast_data'])
    else:
        st.info("4ë²ˆ íƒ­ì—ì„œ ë°ì´í„°ê°€ í†µí•©ë˜ì–´ì•¼ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
