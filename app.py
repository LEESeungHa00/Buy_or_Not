import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import requests
from datetime import datetime, timedelta, timezone
from functools import reduce
import json
import urllib.request
from google.oauth2 import service_account
from google.cloud import bigquery
import pandas_gbq
import feedparser
from urllib.parse import quote
from statsmodels.tsa.seasonal import seasonal_decompose
from prophet import Prophet
from prophet.plot import plot_plotly, plot_components_plotly
from kamis_data import KAMIS_FULL_DATA

# Transformers / HuggingFace
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from huggingface_hub import login as hf_login

# (NEW) optional: robust article text fetch
try:
    from newspaper import Article, Config as NewsConfig
    _HAS_NEWSPAPER = True
except Exception:
    _HAS_NEWSPAPER = False

# ----------------------------
#  Configuration / Constants
# ----------------------------
st.set_page_config(layout="wide")
st.title("ğŸ“Š ë°ì´í„° íƒìƒ‰ ë° í†µí•© ë¶„ì„ ëŒ€ì‹œë³´ë“œ (ë‹¤ì¤‘ AI ê°ì„± & Naver ì¤‘ì‹¬)")

BQ_DATASET = "data_explorer"
BQ_TABLE_NAVER = "naver_trends_cache"
BQ_TABLE_NEWS = "news_sentiment_finbert"
BQ_TABLE_TRADE = "tds_data"

#KAMIS_FULL_DATA = {
    'ìŒ€': {'cat_code': '100', 'item_code': '111', 'kinds': {'20kg': '01', 'ë°±ë¯¸': '02'}},
    'ê°ì': {'cat_code': '100', 'item_code': '152', 'kinds': {'ìˆ˜ë¯¸(ë…¸ì§€)': '01', 'ìˆ˜ë¯¸(ì‹œì„¤)': '04'}},
    'ë°°ì¶”': {'cat_code': '200', 'item_code': '211', 'kinds': {'ë´„': '01', 'ì—¬ë¦„': '02', 'ê°€ì„': '03'}},
    'ì–‘íŒŒ': {'cat_code': '200', 'item_code': '245', 'kinds': {'ì–‘íŒŒ': '00', 'í–‡ì–‘íŒŒ': '02'}},
    'ì‚¬ê³¼': {'cat_code': '400', 'item_code': '411', 'kinds': {'í›„ì§€': '05', 'ì•„ì˜¤ë¦¬': '06'}},
    'ë°”ë‚˜ë‚˜': {'cat_code': '400', 'item_code': '418', 'kinds': {'ìˆ˜ì…': '02'}},
    'ì•„ë³´ì¹´ë„': {'cat_code': '400', 'item_code': '430', 'kinds': {'ìˆ˜ì…': '00'}},
    'ê³ ë“±ì–´': {'cat_code': '600', 'item_code': '611', 'kinds': {'ìƒì„ ': '01', 'ëƒ‰ë™': '02'}},
}

# ----------------------------
#  Helpers: BigQuery + Naver + KAMIS
# ----------------------------
@st.cache_resource
def get_bq_connection():
    try:
        creds_dict = st.secrets["gcp_service_account"]
        creds = service_account.Credentials.from_service_account_info(creds_dict)
        client = bigquery.Client(credentials=creds, project=creds.project_id)
        return client
    except Exception as e:
        st.error(f"Google BigQuery ì—°ê²° ì‹¤íŒ¨: secrets.tomlì„ í™•ì¸í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}")
        return None


def call_naver_api(url, body, naver_keys):
    try:
        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", naver_keys['id'])
        request.add_header("X-Naver-Client-Secret", naver_keys['secret'])
        request.add_header("Content-Type", "application/json")
        response = urllib.request.urlopen(request, data=body.encode("utf-8"))
        if response.getcode() == 200:
            return json.loads(response.read().decode('utf-8'))
        return None
    except Exception as e:
        st.error(f"Naver API ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def fetch_kamis_data(_client, item_info, start_date, end_date, kamis_keys):
    start_str, end_str = start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')
    url = (f"http://www.kamis.or.kr/service/price/xml.do?action=periodWholesaleProductList"
           f"&p_product_cls_code=01&p_startday={start_str}&p_endday={end_str}"
           f"&p_item_category_code={item_info['cat_code']}&p_item_code={item_info['item_code']}&p_kind_code={item_info['kind_code']}"
           f"&p_product_rank_code={item_info['rank_code']}&p_convert_kg_yn=Y"
           f"&p_cert_key={kamis_keys['key']}&p_cert_id={kamis_keys['id']}&p_returntype=json")
    try:
        response = requests.get(url, timeout=20, headers={"User-Agent": "Mozilla/5.0 KAMISClient/1.0"})
        if response.status_code == 200 and "data" in response.json() and "item" in response.json()["data"]:
            price_data = response.json()["data"]["item"]
            if not price_data:
                return pd.DataFrame()
            df_new = pd.DataFrame(price_data)[['regday', 'price']].rename(columns={'regday': 'ë‚ ì§œ', 'price': 'ë„ë§¤ê°€ê²©_ì›'})
            def format_kamis_date(date_str):
                processed_str = str(date_str).replace('/', '-')
                if processed_str.count('-') == 1:
                    # no year provided in some responses; assume start_date year
                    return f"{start_date.year}-{processed_str}"
                return processed_str
            df_new['ë‚ ì§œ'] = pd.to_datetime(df_new['ë‚ ì§œ'].apply(format_kamis_date), errors='coerce')
            df_new['ë„ë§¤ê°€ê²©_ì›'] = pd.to_numeric(df_new['ë„ë§¤ê°€ê²©_ì›'].astype(str).str.replace(',', ''), errors='coerce')
            return df_new
    except Exception as e:
        st.sidebar.error(f"KAMIS API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
    return pd.DataFrame()

# ----------------------------
#  HuggingFace: ë¡œê·¸ì¸ + ì•ˆì „í•œ ëª¨ë¸ ë¡œë“œ
# ----------------------------
hf_token = None
try:
    hf_token = st.secrets["huggingface"]["token"]
except Exception:
    hf_token = None

if hf_token:
    try:
        hf_login(token=hf_token)
        st.sidebar.success("HuggingFace token ì ìš©ë¨.")
    except Exception as e:
        st.sidebar.warning(f"HuggingFace ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")
else:
    st.sidebar.info("HuggingFace tokenì´ secretsì— ì—†ìŠµë‹ˆë‹¤. private ëª¨ë¸ ì‚¬ìš© ì‹œ í•„ìš”í•©ë‹ˆë‹¤.")

DEFAULT_MODEL_IDS = {
    "finbert": "snunlp/KR-FinBERT-SC",
    "elite": "nlptown/bert-base-multilingual-uncased-sentiment",
    "product": "cardiffnlp/twitter-xlm-roberta-base-sentiment"
}

st.sidebar.subheader("ëª¨ë¸ ì„¤ì • (ì›í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì‚¬ìš©)")
finbert_id = st.sidebar.text_input("ê¸ˆìœµ íŠ¹í™” ëª¨ë¸ ID", DEFAULT_MODEL_IDS["finbert"])
elite_id = st.sidebar.text_input("ë²”ìš©(ì—˜ë¦¬íŠ¸) ëª¨ë¸ ID", DEFAULT_MODEL_IDS["elite"])
product_id = st.sidebar.text_input("ìƒí’ˆí‰ ëª¨ë¸ ID", DEFAULT_MODEL_IDS["product"])
USER_MODEL_IDS = {"finbert": finbert_id.strip() or DEFAULT_MODEL_IDS["finbert"],
                  "elite": elite_id.strip() or DEFAULT_MODEL_IDS["elite"],
                  "product": product_id.strip() or DEFAULT_MODEL_IDS["product"]}

@st.cache_resource
def load_models(user_model_ids, token):
    models = {}
    load_report = {}
    for key, mid in user_model_ids.items():
        models[key] = None
        load_report[key] = {"model_id": mid, "loaded": False, "error": None}

        try:
            # 1) ì‹œë„: AutoTokenizer + AutoModelForSequenceClassification
            if token:
                tok = AutoTokenizer.from_pretrained(mid, use_auth_token=token)
                mdl = AutoModelForSequenceClassification.from_pretrained(mid, use_auth_token=token)
            else:
                tok = AutoTokenizer.from_pretrained(mid)
                mdl = AutoModelForSequenceClassification.from_pretrained(mid)

            pipe = pipeline("sentiment-analysis", model=mdl, tokenizer=tok)
            models[key] = pipe
            load_report[key]["loaded"] = True
        except Exception as e1:
            try:
                if token:
                    pipe = pipeline("sentiment-analysis", model=mid, tokenizer=mid, use_auth_token=token)
                else:
                    pipe = pipeline("sentiment-analysis", model=mid, tokenizer=mid)
                models[key] = pipe
                load_report[key]["loaded"] = True
            except Exception as e2:
                load_report[key]["error"] = f"Primary error: {e1}; Fallback error: {e2}"
                models[key] = None
    return models, load_report

models, model_load_report = load_models(USER_MODEL_IDS, hf_token)

st.sidebar.markdown("### ëª¨ë¸ ë¡œë“œ ìƒíƒœ")
for k, r in model_load_report.items():
    if r["loaded"]:
        st.sidebar.success(f"{k} OK â€” {r['model_id']}")
    else:
        st.sidebar.error(f"{k} ì‹¤íŒ¨ â€” {r['model_id']}")
        if r["error"]:
            st.sidebar.caption(str(r["error"])[:240])

# ----------------------------
#  í‘œì¤€í™”ëœ label -> score ë³€í™˜ í•¨ìˆ˜
# ----------------------------
def _label_score_to_signed(pred):
    if pred is None:
        return 0.0, "neutral"
    lbl = str(pred.get("label", "")).lower()
    score = float(pred.get("score", 0.0))
    if "star" in lbl:
        try:
            n = int(lbl.split()[0])
            signed = (n - 3) / 2.0
            label = "positive" if signed > 0 else ("negative" if signed < 0 else "neutral")
            return float(signed), label
        except Exception:
            pass
    if any(x in lbl for x in ["neg", "negative", "bad", "ë¶€ì •"]):
        return -score, "negative"
    if any(x in lbl for x in ["pos", "positive", "good", "ê¸ì •"]):
        return +score, "positive"
    if "neu" in lbl or "neutral" in lbl:
        return 0.0, "neutral"
    return (score if score <= 1 else 0.0), ("positive" if score >= 0.5 else "neutral")


def analyze_sentiment_multi(texts, models_dict):
    results = []
    if not texts:
        return results
    preds = {}
    for key in ["finbert", "elite", "product"]:
        pipe = models_dict.get(key)
        if pipe is None:
            preds[key] = [None]*len(texts)
        else:
            try:
                preds[key] = pipe(texts)
            except Exception:
                tmp = []
                for t in texts:
                    try:
                        tmp.append(pipe(t)[0])
                    except Exception:
                        tmp.append(None)
                preds[key] = tmp
    for i, t in enumerate(texts):
        fin_s, fin_l = _label_score_to_signed(preds["finbert"][i] if preds.get("finbert") else None)
        el_s, el_l = _label_score_to_signed(preds["elite"][i] if preds.get("elite") else None)
        pr_s, pr_l = _label_score_to_signed(preds["product"][i] if preds.get("product") else None)
        # ì¢…í•© ê¸°ì¤€ì€ FinBERT ìš°ì„ 
        results.append({
            "FinBERT_Sentiment": fin_s, "FinBERT_Label": fin_l,
            "Elite_Sentiment": el_s, "Elite_Label": el_l,
            "Product_Sentiment": pr_s, "Product_Label": pr_l,
            "Sentiment": fin_s, "Label": fin_l
        })
    return results

# ----------------------------
#  ë‰´ìŠ¤ ìˆ˜ì§‘/ë¶„ì„ (ê°•í™”ëœ ì•ˆì •ì„±)
# ----------------------------

def _fetch_article_text(url):
    if not _HAS_NEWSPAPER or not url:
        return ""
    try:
        cfg = NewsConfig()
        cfg.browser_user_agent = "Mozilla/5.0 (compatible; NewsBot/1.0; +https://example.com/bot)"
        cfg.request_timeout = 10
        art = Article(url, language='ko', config=cfg)
        art.download()
        art.parse()
        return art.text or ""
    except Exception:
        return ""


def get_news_with_multi_model_analysis(_bq_client, models_dict, keyword, days_limit=7):
    project_id = _bq_client.project
    full_table_id = f"{project_id}.{BQ_DATASET}.{BQ_TABLE_NEWS}"

    try:
        time_limit = datetime.now(timezone.utc) - timedelta(days=days_limit)
        query = f"""
        SELECT * FROM `{full_table_id}`
        WHERE Keyword = @keyword AND InsertedAt >= @time_limit
        ORDER BY ë‚ ì§œ DESC
        """
        job_config = bigquery.QueryJobConfig(query_parameters=[
            bigquery.ScalarQueryParameter("keyword", "STRING", keyword),
            bigquery.ScalarQueryParameter("time_limit", "TIMESTAMP", time_limit)
        ])
        df_cache = _bq_client.query(query, job_config=job_config).to_dataframe()
    except Exception:
        df_cache = pd.DataFrame()

    if not df_cache.empty:
        st.sidebar.success(f"âœ”ï¸ '{keyword}' ìµœì‹  ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹œì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return df_cache

    all_news = []
    rss_url = f"https://news.google.com/rss/search?q={quote(keyword)}&hl=ko&gl=KR&ceid=KR:ko"
    feed = feedparser.parse(rss_url)

    for entry in feed.entries[:60]:
        title = entry.get('title', '').strip()
        link = entry.get('link', '').strip()
        if not title:
            continue
        try:
            pub_date = pd.to_datetime(entry.get('published')).date()
        except Exception:
            pub_date = datetime.utcnow().date()

        body = _fetch_article_text(link)
        text_for_model = (title + " " + body).strip() if body else title
        all_news.append({"ë‚ ì§œ": pub_date, "Title": title, "RawUrl": link, "ModelInput": text_for_model})

    if not all_news:
        st.error(f"'{keyword}'ì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    df_new = pd.DataFrame(all_news).drop_duplicates(subset=["Title"])
    with st.spinner(f"ë‹¤ì¤‘ ëª¨ë¸ë¡œ '{keyword}' ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì¤‘..."):
        multi = analyze_sentiment_multi(df_new['ModelInput'].tolist(), models_dict)

    multi_df = pd.DataFrame(multi)
    df_new = pd.concat([df_new.drop(columns=['ModelInput']), multi_df], axis=1)
    df_new['Keyword'] = keyword
    df_new['InsertedAt'] = datetime.now(timezone.utc)

    try:
        df_to_gbq = df_new[[
            "ë‚ ì§œ", "Title", "Keyword", "Sentiment", "Label", "InsertedAt",
            "FinBERT_Sentiment", "FinBERT_Label",
            "Elite_Sentiment", "Elite_Label",
            "Product_Sentiment", "Product_Label",
            "RawUrl"
        ]]
        pandas_gbq.to_gbq(df_to_gbq, f"{project_id}.{BQ_DATASET}.{BQ_TABLE_NEWS}", project_id=project_id,
                          if_exists="append", credentials=_bq_client._credentials)
    except Exception as e:
        st.sidebar.warning(f"BigQuery ì €ì¥ ì‹¤íŒ¨(ê³„ì† ì§„í–‰): {e}")
        df_to_gbq = df_new

    return df_to_gbq

# ----------------------------
#  BigQuery ë°ì´í„° ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸)
# ----------------------------
@st.cache_data(ttl=3600)
def get_categories_from_bq(_client):
    project_id = _client.project
    table_id = f"{project_id}.{BQ_DATASET}.{BQ_TABLE_TRADE}"
    try:
        query = f"SELECT DISTINCT Category FROM `{table_id}` WHERE Category IS NOT NULL ORDER BY Category"
        df = _client.query(query).to_dataframe()
        return sorted(df['Category'].astype(str).unique())
    except Exception:
        return []


def get_trade_data_from_bq(client, categories):
    if not categories:
        return pd.DataFrame()
    project_id = client.project
    table_id = f"{project_id}.{BQ_DATASET}.{BQ_TABLE_TRADE}"
    try:
        query_params = [bigquery.ArrayQueryParameter("categories", "STRING", categories)]
        sql = f"SELECT * FROM `{table_id}` WHERE Category IN UNNEST(@categories)"
        job_config = bigquery.QueryJobConfig(query_parameters=query_params)
        df = client.query(sql, job_config=job_config).to_dataframe()
        for col in df.columns:
            if 'price' in col.lower() or 'value' in col.lower() or 'volume' in col.lower():
                df[col] = pd.to_numeric(df[col], errors='coerce')
        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
        return df
    except Exception as e:
        st.error(f"BigQueryì—ì„œ TDS ë°ì´í„° ì½ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

@st.cache_data(ttl=3600)
def fetch_naver_trends_data(_client, keywords, start_date, end_date, naver_keys):
    project_id = _client.project
    table_id = f"{project_id}.{BQ_DATASET}.{BQ_TABLE_NAVER}"
    try:
        sql = f"SELECT * FROM `{table_id}` ORDER BY ë‚ ì§œ"
        df_cache = _client.query(sql).to_dataframe()
        if not df_cache.empty:
            df_cache['ë‚ ì§œ'] = pd.to_datetime(df_cache['ë‚ ì§œ'])
    except Exception:
        df_cache = pd.DataFrame(columns=['ë‚ ì§œ'])

    fetch_start_date = start_date
    if not df_cache.empty:
        last_cached_date = df_cache['ë‚ ì§œ'].max()
        if start_date > last_cached_date:
            fetch_start_date = start_date
        elif end_date > last_cached_date:
            fetch_start_date = last_cached_date + timedelta(days=1)
        else:
            fetch_start_date = end_date + timedelta(days=1)

    new_data_list = []
    if fetch_start_date <= end_date:
        current_start = fetch_start_date
        while current_start <= end_date:
            current_end = min(current_start + timedelta(days=89), end_date)
            NAVER_SHOPPING_CAT_MAP = {
                'ì•„ë³´ì¹´ë„': "50000007", 'ë°”ë‚˜ë‚˜': "50000007", 'ì‚¬ê³¼': "50000007",
                'ì»¤í”¼': "50000004", 'ìŒ€': "50000006", 'ê³ ë“±ì–´': "50000009"
            }
            all_data_chunk = []
            for keyword in keywords:
                keyword_dfs = []
                # SEARCH
                body_search = json.dumps({
                    "startDate": current_start.strftime('%Y-%m-%d'),
                    "endDate": current_end.strftime('%Y-%m-%d'),
                    "timeUnit": "date",
                    "keywordGroups": [{"groupName": keyword, "keywords": [keyword]}]
                })
                search_res = call_naver_api("https://openapi.naver.com/v1/datalab/search", body_search, naver_keys)
                if search_res and search_res.get('results') and search_res['results'][0]['data']:
                    df_search = pd.DataFrame(search_res['results'][0]['data'])
                    if not df_search.empty:
                        keyword_dfs.append(df_search.rename(columns={'period': 'ë‚ ì§œ', 'ratio': f'NaverSearch_{keyword}'}))

                # SHOPPING (use keyword as-is for map)
                norm_keyword = keyword
                if norm_keyword in NAVER_SHOPPING_CAT_MAP:
                    cat_id = NAVER_SHOPPING_CAT_MAP[norm_keyword]
                    body_shop = json.dumps({
                        "startDate": current_start.strftime('%Y-%m-%d'),
                        "endDate": current_end.strftime('%Y-%m-%d'),
                        "timeUnit": "date",
                        "category": [{"name": keyword, "param": [cat_id]}],
                        "keyword": [{"name": keyword, "param": [keyword]}]
                    })
                    shop_res = call_naver_api("https://openapi.naver.com/v1/datalab/shopping/categories", body_shop, naver_keys)
                    if shop_res and shop_res.get('results') and shop_res['results'][0]['data']:
                        df_shop = pd.DataFrame(shop_res['results'][0]['data'])
                        if not df_shop.empty:
                            keyword_dfs.append(df_shop.rename(columns={'period': 'ë‚ ì§œ', 'ratio': f'NaverShop_{keyword}'}))

                if keyword_dfs:
                    for df in keyword_dfs:
                        df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
                    merged = reduce(lambda l, r: pd.merge(l, r, on='ë‚ ì§œ', how='outer'), keyword_dfs)
                    all_data_chunk.append(merged)

            if all_data_chunk:
                new_data_list.append(reduce(lambda l, r: pd.merge(l, r, on='ë‚ ì§œ', how='outer'), all_data_chunk))

            current_start = current_end + timedelta(days=1)
    else:
        st.sidebar.success("âœ”ï¸ ë„¤ì´ë²„ íŠ¸ë Œë“œ: ëª¨ë“  ë°ì´í„°ê°€ ìºì‹œì— ìˆìŠµë‹ˆë‹¤.")

    if new_data_list:
        non_empty = [df for df in new_data_list if not df.empty]
        if non_empty:
            df_new = pd.concat(non_empty, ignore_index=True)
            df_new['ë‚ ì§œ'] = pd.to_datetime(df_new['ë‚ ì§œ'])
            # merge with cache safely
            if not df_cache.empty:
                df_combined = pd.concat([df_cache, df_new], ignore_index=True)
                df_combined = df_combined.sort_values('ë‚ ì§œ').drop_duplicates(subset=['ë‚ ì§œ'], keep='last').reset_index(drop=True)
            else:
                df_combined = df_new.sort_values('ë‚ ì§œ').reset_index(drop=True)

            try:
                pandas_gbq.to_gbq(df_combined, table_id, project_id=project_id, if_exists="replace", credentials=_client._credentials)
            except Exception:
                # if replace fails, skip saving
                pass
            df_final = df_combined
        else:
            df_final = df_cache
    else:
        df_final = df_cache

    if df_final.empty:
        return pd.DataFrame()
    return df_final[(df_final['ë‚ ì§œ'] >= start_date) & (df_final['ë‚ ì§œ'] <= end_date)].reset_index(drop=True)

# ----------------------------
#  Session state ì´ˆê¸°í™” (í•„ìš”í•œ í‚¤ë“¤)
# ----------------------------
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'selected_categories' not in st.session_state:
    st.session_state.selected_categories = []
if 'raw_trade_df' not in st.session_state:
    st.session_state.raw_trade_df = pd.DataFrame()
if 'wholesale_data' not in st.session_state:
    st.session_state.wholesale_data = pd.DataFrame()
if 'search_data' not in st.session_state:
    st.session_state.search_data = pd.DataFrame()
if 'news_data' not in st.session_state:
    st.session_state.news_data = pd.DataFrame()
if 'weekly_dfs' not in st.session_state:
    st.session_state.weekly_dfs = {}
if 'final_df' not in st.session_state:
    st.session_state.final_df = pd.DataFrame()
if 'fig_decompose' not in st.session_state:
    st.session_state.fig_decompose = None
if 'fig_forecast' not in st.session_state:
    st.session_state.fig_forecast = None
if 'forecast_data' not in st.session_state:
    st.session_state.forecast_data = pd.DataFrame()

# BQ client
bq_client = get_bq_connection()
if bq_client is None:
    st.error("GCP ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì•±ì„ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”.")
    st.stop()

# Ensure news table exists (best-effort)
def _safe_update_news_table_schema(client):
    try:
        project_id = client.project
        table_id = f"{project_id}.{BQ_DATASET}.{BQ_TABLE_NEWS}"
        table = client.get_table(table_id)
        existing = {s.name for s in table.schema}
        add_fields = []
        wanted = [
            ("FinBERT_Sentiment", "FLOAT"),
            ("FinBERT_Label", "STRING"),
            ("Elite_Sentiment", "FLOAT"),
            ("Elite_Label", "STRING"),
            ("Product_Sentiment", "FLOAT"),
            ("Product_Label", "STRING"),
            ("RawUrl", "STRING"),
        ]
        for name, t in wanted:
            if name not in existing:
                add_fields.append(bigquery.SchemaField(name, t))
        if add_fields:
            new_schema = list(table.schema) + add_fields
            table.schema = new_schema
            client.update_table(table, ["schema"])
    except Exception:
        pass


def ensure_news_table_exists(client):
    project_id = client.project
    full_table_id = f"{project_id}.{BQ_DATASET}.{BQ_TABLE_NEWS}"
    try:
        client.get_table(full_table_id)
        _safe_update_news_table_schema(client)
    except Exception:
        st.write(f"ë‰´ìŠ¤ ë¶„ì„ í…Œì´ë¸” '{full_table_id}'ì„ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        schema = [
            bigquery.SchemaField("ë‚ ì§œ", "DATE"),
            bigquery.SchemaField("Title", "STRING"),
            bigquery.SchemaField("Keyword", "STRING"),
            bigquery.SchemaField("Sentiment", "FLOAT"),
            bigquery.SchemaField("Label", "STRING"),
            bigquery.SchemaField("InsertedAt", "TIMESTAMP"),
            bigquery.SchemaField("FinBERT_Sentiment", "FLOAT"),
            bigquery.SchemaField("FinBERT_Label", "STRING"),
            bigquery.SchemaField("Elite_Sentiment", "FLOAT"),
            bigquery.SchemaField("Elite_Label", "STRING"),
            bigquery.SchemaField("Product_Sentiment", "FLOAT"),
            bigquery.SchemaField("Product_Label", "STRING"),
            bigquery.SchemaField("RawUrl", "STRING"),
        ]
        table = bigquery.Table(full_table_id, schema=schema)
        try:
            client.create_table(table)
            st.success(f"í…Œì´ë¸” '{BQ_TABLE_NEWS}' ìƒì„± ì™„ë£Œ.")
        except Exception as e:
            st.warning(f"í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨ (ë¬´ì‹œ ê°€ëŠ¥): {e}")

ensure_news_table_exists(bq_client)

# ----------------------------
#  Sidebar: ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ & API í‚¤
# ----------------------------
st.sidebar.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
st.sidebar.subheader("1. ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ")
data_src = st.sidebar.radio("ë°ì´í„° ì†ŒìŠ¤", ["BigQuery", "CSV ì—…ë¡œë“œ"])

if data_src == 'CSV ì—…ë¡œë“œ':
    uploaded_file = st.sidebar.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ (BigQueryì™€ ë™ì¼ í—¤ë”)", type=['csv'])
    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)
            if 'Date' in df.columns:
                df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
            st.session_state.raw_trade_df = df
            st.session_state.data_loaded = True
            st.session_state.selected_categories = sorted(df['Category'].astype(str).unique().tolist()) if 'Category' in df.columns else []
        except Exception as e:
            st.sidebar.error(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
else:
    categories = get_categories_from_bq(bq_client)
    if not categories:
        st.sidebar.warning("BigQueryì— ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        selected_categories = st.sidebar.multiselect(
            "ë¶„ì„í•  í’ˆëª© ì„ íƒ", categories, default=st.session_state.get('selected_categories', [])
        )
        if st.sidebar.button("ğŸš€ ì„ íƒ í’ˆëª© ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°", key="load_trade"):
            if not selected_categories:
                st.sidebar.warning("ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            else:
                df = get_trade_data_from_bq(bq_client, selected_categories)
                if df is not None and not df.empty:
                    st.session_state.raw_trade_df = df
                    st.session_state.data_loaded = True
                    st.session_state.selected_categories = selected_categories
                    st.rerun()
                else:
                    st.sidebar.error("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if not st.session_state.data_loaded:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ë°ì´í„°ë¥¼ ì„ íƒÂ·ë¶ˆëŸ¬ì˜¤ì„¸ìš”.")
    st.stop()

raw_trade_df = st.session_state.raw_trade_df
st.sidebar.success(f"**{', '.join(st.session_state.selected_categories)}** ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
st.sidebar.markdown("---")

st.sidebar.subheader("2. ë¶„ì„ ê¸°ê°„ ë° í‚¤ì›Œë“œ ì„¤ì •")
file_start_date = pd.to_datetime(raw_trade_df['Date'].min())
file_end_date = pd.to_datetime(raw_trade_df['Date'].max())
start_date = pd.to_datetime(st.sidebar.date_input('ì‹œì‘ì¼', file_start_date, min_value=file_start_date, max_value=file_end_date))
end_date = pd.to_datetime(st.sidebar.date_input('ì¢…ë£Œì¼', file_end_date, min_value=start_date, max_value=file_end_date))

news_keyword_input = st.sidebar.text_input("ë‰´ìŠ¤ ê°ì„± ë¶„ì„ í‚¤ì›Œë“œ (í•˜ë‚˜ë§Œ ì…ë ¥)", st.session_state.selected_categories[0] if st.session_state.selected_categories else "")
search_keywords_input = st.sidebar.text_input("ë„¤ì´ë²„ íŠ¸ë Œë“œ ë¶„ì„ í‚¤ì›Œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„)", ",".join(st.session_state.selected_categories) if st.session_state.selected_categories else "")
search_keywords = [k.strip() for k in search_keywords_input.split(',') if k.strip()]

st.sidebar.subheader("3. ì™¸ë¶€ ë°ì´í„° ì—°ë™")
with st.sidebar.expander("ğŸ”‘ API í‚¤ ì…ë ¥"):
    kamis_api_key = st.text_input("KAMIS API Key", type="password")
    kamis_api_id = st.text_input("KAMIS API ID", type="password")
    naver_client_id = st.text_input("Naver API Client ID", type="password")
    naver_client_secret = st.text_input("Naver API Client Secret", type="password")

st.sidebar.markdown("##### KAMIS ë†ì‚°ë¬¼ ê°€ê²©")
kamis_item_name = st.sidebar.selectbox("í’ˆëª© ì„ íƒ", list(KAMIS_FULL_DATA.keys()))
if kamis_item_name:
    kamis_kind_name = st.sidebar.selectbox("í’ˆì¢… ì„ íƒ", list(KAMIS_FULL_DATA[kamis_item_name]['kinds'].keys()))
    if st.sidebar.button("ğŸŒ¾ KAMIS ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
        if kamis_api_key and kamis_api_id:
            item_info = KAMIS_FULL_DATA[kamis_item_name].copy()
            item_info['kind_code'] = item_info['kinds'][kamis_kind_name]
            item_info['rank_code'] = '01'
            st.session_state.wholesale_data = fetch_kamis_data(
                bq_client, item_info, start_date, end_date, {'key': kamis_api_key, 'id': kamis_api_id}
            )
        else:
            st.sidebar.error("KAMIS API Keyì™€ IDë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")

st.sidebar.markdown("##### ë„¤ì´ë²„ íŠ¸ë Œë“œ ë°ì´í„° (ê²€ìƒ‰/ì‡¼í•‘)")
if st.sidebar.button("ğŸ“ˆ ë„¤ì´ë²„ íŠ¸ë Œë“œ ê°€ì ¸ì˜¤ê¸°"):
    if search_keywords and naver_client_id and naver_client_secret:
        st.session_state.search_data = fetch_naver_trends_data(
            bq_client, search_keywords, start_date, end_date,
            {'id': naver_client_id, 'secret': naver_client_secret}
        )
    elif not search_keywords:
        st.sidebar.warning("íŠ¸ë Œë“œ ë¶„ì„ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        st.sidebar.error("Naver API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

st.sidebar.markdown("##### ë‰´ìŠ¤ ê°ì„± ë¶„ì„ (ë‹¤ì¤‘ ëª¨ë¸)")
if st.sidebar.button("ğŸ“° ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹¤í–‰"):
    if news_keyword_input:
        st.session_state.news_data = get_news_with_multi_model_analysis(bq_client, models, news_keyword_input)
    else:
        st.sidebar.warning("ë‰´ìŠ¤ ë¶„ì„ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# ----------------------------
#  ë©”ì¸: ë¹ ë¥¸ ì‘ì—… (ëª¨ë“  ì™¸ë¶€ ë°ì´í„°ì…‹ í•´ì œ í¬í•¨)
# ----------------------------
st.markdown("### âš¡ ë¹ ë¥¸ ì‘ì—…")
colA, colB, colC, colD = st.columns(4)

with colA:
    if st.button("ğŸ§¹ ë„¤ì´ë²„ ë°ì´í„° í•´ì œ(ì´ˆê¸°í™”)"):
        st.session_state.search_data = pd.DataFrame()
        st.success("ë„¤ì´ë²„ ë°ì´í„°ì…‹ì„ í•´ì œí–ˆìŠµë‹ˆë‹¤.")

with colB:
    if st.button("ğŸ§¹ KAMIS ë°ì´í„° í•´ì œ"):
        st.session_state.wholesale_data = pd.DataFrame()
        st.success("KAMIS(ë„ë§¤ê°€ê²©) ë°ì´í„°ë¥¼ í•´ì œí–ˆìŠµë‹ˆë‹¤.")

with colC:
    if st.button("ğŸ§¹ ë‰´ìŠ¤ ë°ì´í„° í•´ì œ"):
        st.session_state.news_data = pd.DataFrame()
        st.success("ë‰´ìŠ¤ ë°ì´í„°ì…‹ì„ í•´ì œí–ˆìŠµë‹ˆë‹¤.")

with colD:
    if st.button("ğŸ§¹ ëª¨ë“  ì™¸ë¶€ ë°ì´í„° ì´ˆê¸°í™” (ë„¤ì´ë²„/KAMIS/ë‰´ìŠ¤)"):
        st.session_state.search_data = pd.DataFrame()
        st.session_state.wholesale_data = pd.DataFrame()
        st.session_state.news_data = pd.DataFrame()
        st.success("ëª¨ë“  ì™¸ë¶€ ë°ì´í„°ì…‹ì„ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")

# ë°ì´í„° (ì„¸ì…˜)
raw_wholesale_df = st.session_state.get('wholesale_data', pd.DataFrame())
raw_search_df = st.session_state.get('search_data', pd.DataFrame())
raw_news_df = st.session_state.get('news_data', pd.DataFrame())

# ----------------------------
#  Tabs: ì›ë³¸, í‘œì¤€í™”, ë‰´ìŠ¤(ë‹¤ì¤‘ëª¨ë¸), ìƒê´€, ì˜ˆì¸¡
# ----------------------------
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "1ï¸âƒ£ ì›ë³¸ ë°ì´í„°",
    "2ï¸âƒ£ ë°ì´í„° í‘œì¤€í™”",
    "3ï¸âƒ£ ë‰´ìŠ¤ ê°ì„± ë¶„ì„(ë‹¤ì¤‘ ëª¨ë¸)",
    "4ï¸âƒ£ ìƒê´€ê´€ê³„ ë¶„ì„",
    "ğŸ“ˆ ì‹œê³„ì—´ ì˜ˆì¸¡"
])

with tab1:
    st.subheader("A. ìˆ˜ì¶œì… ë°ì´í„° (ì„ íƒ ê¸°ê°„)")
    st.dataframe(raw_trade_df[(raw_trade_df['Date'] >= start_date) & (raw_trade_df['Date'] <= end_date)])
    st.subheader("B. ì™¸ë¶€ ê°€ê²© ë°ì´í„°")
    st.dataframe(raw_wholesale_df)
    st.subheader("C. ë„¤ì´ë²„ íŠ¸ë Œë“œ ë°ì´í„° (ê²€ìƒ‰/ì‡¼í•‘)")
    st.dataframe(raw_search_df)
    st.subheader("D. ë‰´ìŠ¤ ë°ì´í„°")
    st.dataframe(raw_news_df)

with tab2:
    st.header("ë°ì´í„° í‘œì¤€í™”: ì£¼ë³„(Weekly) ë°ì´í„°ë¡œ ë³€í™˜")
    trade_df_in_range = raw_trade_df[(raw_trade_df['Date'] >= start_date) & (raw_trade_df['Date'] <= end_date)]
    filtered_trade_df = trade_df_in_range[trade_df_in_range['Category'].isin(st.session_state.selected_categories)].copy()

    if not filtered_trade_df.empty:
        filtered_trade_df.set_index('Date', inplace=True)
        trade_weekly = filtered_trade_df.resample('W-Mon').agg(
            ìˆ˜ì…ì•¡_USD=('Value', 'sum'),
            ìˆ˜ì…ëŸ‰_KG=('Volume', 'sum')
        ).copy()
        trade_weekly['ìˆ˜ì…ë‹¨ê°€_USD_KG'] = trade_weekly['ìˆ˜ì…ì•¡_USD'] / trade_weekly['ìˆ˜ì…ëŸ‰_KG']
        trade_weekly.index.name = 'ë‚ ì§œ'

        weekly_dfs = {'trade': trade_weekly}

        # ì™¸ë¶€ ê°€ê²© ë°ì´í„° (KAMIS)
        if not raw_wholesale_df.empty and 'ë‚ ì§œ' in raw_wholesale_df.columns:
            raw_wholesale_df['ë‚ ì§œ'] = pd.to_datetime(raw_wholesale_df['ë‚ ì§œ'])
            wholesale_weekly = raw_wholesale_df.set_index('ë‚ ì§œ').resample('W-Mon').mean(numeric_only=True)
            wholesale_weekly.index.name = 'ë‚ ì§œ'
            weekly_dfs['wholesale'] = wholesale_weekly

        # ë„¤ì´ë²„ íŠ¸ë Œë“œ ë°ì´í„° (ê²€ìƒ‰/ì‡¼í•‘)
        if not raw_search_df.empty and 'ë‚ ì§œ' in raw_search_df.columns:
            raw_search_df['ë‚ ì§œ'] = pd.to_datetime(raw_search_df['ë‚ ì§œ'])
            search_weekly = raw_search_df.set_index('ë‚ ì§œ').resample('W-Mon').mean(numeric_only=True).fillna(0)
            search_weekly.index.name = 'ë‚ ì§œ'
            weekly_dfs['search'] = search_weekly

        # ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ë°ì´í„° (ë‹¤ì¤‘ ëª¨ë¸)
        if not raw_news_df.empty and 'ë‚ ì§œ' in raw_news_df.columns:
            raw_news_df['ë‚ ì§œ'] = pd.to_datetime(raw_news_df['ë‚ ì§œ'])
            sentiment_cols = [col for col in raw_news_df.columns if 'Sentiment' in col]
            if sentiment_cols:
                news_weekly = raw_news_df.set_index('ë‚ ì§œ')[sentiment_cols].resample('W-Mon').mean(numeric_only=True)
                # ì•ˆì „í•œ ì»¬ëŸ¼ rename
                news_weekly = news_weekly.rename(columns=lambda x: 'News_' + x if not x.startswith('News_') else x)
                news_weekly.index.name = 'ë‚ ì§œ'
                weekly_dfs['news'] = news_weekly

        st.session_state['weekly_dfs'] = weekly_dfs
        st.write("### ì£¼ë³„ ì§‘ê³„ ë°ì´í„° ìƒ˜í”Œ")
        for name, df in weekly_dfs.items():
            st.write(f"##### {name.capitalize()} Data (Weekly)")
            st.dataframe(df.head())
            st.write(f"**ì»¬ëŸ¼:** {df.columns.tolist()}")

    else:
        st.warning("ì„ íƒëœ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ìˆ˜ì¶œì… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

with tab3:
    st.header("ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ê²°ê³¼ (ì„¸ ëª¨ë¸ êµì°¨ ë¹„êµ)")

    raw_news_df = st.session_state.get('news_data', pd.DataFrame())

    def _simple_tokenize_ko(text):
        import re
        toks = re.findall(r"[ê°€-í£A-Za-z0-9]+", str(text))
        stop = set(["í•œêµ­", "ì •ë¶€", "ì‹œì¥", "ê²½ì œ", "ê°€ê²©", "ê´€ë ¨", "ë“±", "ë°", "ë‰´ìŠ¤", "ê¸°ì‚¬", "ìµœê·¼", "ì „ë§"])
        return [t for t in toks if len(t) > 1 and t not in stop]

    if not raw_news_df.empty:
        dfn = raw_news_df.copy()
        dfn['ë‚ ì§œ'] = pd.to_datetime(dfn['ë‚ ì§œ'])

        for col in ["FinBERT_Sentiment", "Elite_Sentiment", "Product_Sentiment"]:
            if col not in dfn.columns:
                dfn[col] = np.nan

        dfn["AvgSentiment_3Models"] = dfn[["FinBERT_Sentiment", "Elite_Sentiment", "Product_Sentiment"]].mean(axis=1, skipna=True)

        N_days = 14
        cutoff = dfn['ë‚ ì§œ'].max() - pd.Timedelta(days=N_days-1)
        df_recent = dfn[dfn['ë‚ ì§œ'] >= cutoff]

        pos_mask = dfn['AvgSentiment_3Models'] > 0.15
        neg_mask = dfn['AvgSentiment_3Models'] < -0.15
        pos_words = pd.Series(sum([_simple_tokenize_ko(t) for t in dfn.loc[pos_mask, 'Title'].dropna().tolist()], [])).value_counts().head(5)
        neg_words = pd.Series(sum([_simple_tokenize_ko(t) for t in dfn.loc[neg_mask, 'Title'].dropna().tolist()], [])).value_counts().head(5)

        avg_recent = round(df_recent['AvgSentiment_3Models'].mean(), 3) if not df_recent.empty else np.nan
        trend_line = "ê¸ì •ì " if avg_recent > 0.05 else ("ë¶€ì •ì " if avg_recent < -0.05 else "ì¤‘ë¦½ì ")
        pos_kw = ", ".join(pos_words.index.tolist())
        neg_kw = ", ".join(neg_words.index.tolist())

        with st.container():
            st.markdown("#### ğŸ¤– AI ë‰´ìŠ¤ íŠ¸ë Œë“œ ìš”ì•½")
            st.info(
                f"ìµœê·¼ {N_days}ì¼ ê¸°ì¤€ í‰ê·  ì¢…í•© ê°ì„±ì€ **{avg_recent if not np.isnan(avg_recent) else 'N/A'}**ë¡œ **{trend_line}**ì…ë‹ˆë‹¤. "
                f"ê¸ì • í‚¤ì›Œë“œ: {pos_kw if pos_kw else 'ì—†ìŒ'} / ë¶€ì • í‚¤ì›Œë“œ: {neg_kw if neg_kw else 'ì—†ìŒ'}"
            )

        news_weekly = dfn.dropna(subset=['ë‚ ì§œ']).set_index('ë‚ ì§œ').resample('W-Mon').mean(numeric_only=True)
        news_weekly.index.name = 'ë‚ ì§œ'

        fig_multi = px.line(
            news_weekly.reset_index(),
            x='ë‚ ì§œ',
            y=['FinBERT_Sentiment', 'Elite_Sentiment', 'Product_Sentiment', 'AvgSentiment_3Models'],
            title="ì£¼ë³„ í‰ê·  ë‰´ìŠ¤ ê°ì„± ì ìˆ˜ (FinBERT / Elite / Product / í‰ê· )"
        )
        fig_multi.add_hline(y=0, line_dash="dash")
        st.plotly_chart(fig_multi, use_container_width=True)

        if 'Keyword' in dfn.columns and dfn['Keyword'].nunique() > 1:
            fig_kw = px.line(dfn.sort_values('ë‚ ì§œ'), x='ë‚ ì§œ', y='FinBERT_Sentiment', color='Keyword', title="í‚¤ì›Œë“œë³„ FinBERT ê°ì„± ì¶”ì´")
            st.plotly_chart(fig_kw, use_container_width=True)

        st.markdown("---")
        st.subheader("ìˆ˜ì§‘ ë‰´ìŠ¤ ì›ë³¸ ë° ê°ì„± ì ìˆ˜ (ìµœì‹ ìˆœ)")
        show_cols = ['ë‚ ì§œ', 'Title', 'Keyword', 'FinBERT_Sentiment', 'Elite_Sentiment', 'Product_Sentiment',
                     'FinBERT_Label', 'Elite_Label', 'Product_Label', 'RawUrl']
        st.dataframe(dfn.sort_values(by='ë‚ ì§œ', ascending=False)[[c for c in show_cols if c in dfn.columns]])
    else:
        st.info("ì‚¬ì´ë“œë°” ë˜ëŠ” ìƒë‹¨ ë¹ ë¥¸ ì‘ì—…ì—ì„œ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

with tab4:
    st.header("ìƒê´€ê´€ê³„ ë¶„ì„")
    if 'weekly_dfs' in st.session_state and st.session_state['weekly_dfs']:
        weekly_dfs = st.session_state['weekly_dfs']
        dfs_to_concat = [df for df in weekly_dfs.values() if not df.empty]
        if len(dfs_to_concat) > 0:
            # merge on index 'ë‚ ì§œ' -> ensure index names
            for k, df in weekly_dfs.items():
                if df.index.name != 'ë‚ ì§œ':
                    df.index.name = 'ë‚ ì§œ'

            final_df = reduce(lambda left, right: pd.merge(left.reset_index(), right.reset_index(), on='ë‚ ì§œ', how='outer'), dfs_to_concat)
            final_df['ë‚ ì§œ'] = pd.to_datetime(final_df['ë‚ ì§œ'])
            final_df = final_df.set_index('ë‚ ì§œ').sort_index()

            # interpolate missing values
            final_df = final_df.interpolate(method='linear', limit_direction='both')

            final_df_valid = final_df.dropna(how='all', axis=1)

            if final_df_valid.empty:
                st.warning("ë°ì´í„° ë³‘í•©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‚ ì§œ ë²”ìœ„ê°€ ê²¹ì¹˜ì§€ ì•Šê±°ë‚˜, ëª¨ë“  ë°ì´í„°ê°€ ê²°ì¸¡ì¹˜ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‚ ì§œ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                st.session_state['final_df'] = pd.DataFrame()
                st.session_state['scaled_final_df'] = pd.DataFrame()
            else:
                st.session_state['final_df'] = final_df_valid

                numeric_cols = final_df_valid.select_dtypes(include=np.number).columns
                scaled_final_df = final_df_valid.copy()
                for col in numeric_cols:
                    min_val = scaled_final_df[col].min()
                    max_val = scaled_final_df[col].max()
                    if pd.isna(min_val) or pd.isna(max_val):
                        scaled_final_df[col] = scaled_final_df[col].fillna(0.0)
                        continue
                    if max_val - min_val > 0:
                        scaled_final_df[col] = (scaled_final_df[col] - min_val) / (max_val - min_val)
                    else:
                        scaled_final_df[col] = 0.0
                st.session_state['scaled_final_df'] = scaled_final_df

            if 'scaled_final_df' in st.session_state and not st.session_state['scaled_final_df'].empty:
                scaled_final_df = st.session_state['scaled_final_df']

                st.subheader("í†µí•© ë°ì´í„° ì‹œê°í™” (ìŠ¤ì¼€ì¼ë§ ì ìš©)")
                df_long = scaled_final_df.reset_index().melt(id_vars='ë‚ ì§œ', var_name='ë°ì´í„° ì¢…ë¥˜', value_name='ê°’')
                fig = px.line(df_long, x='ë‚ ì§œ', y='ê°’', color='ë°ì´í„° ì¢…ë¥˜', title="í†µí•© ë°ì´í„° ì‹œê³„ì—´ ì¶”ì´ (Min-Max Scaling)")
                st.plotly_chart(fig, use_container_width=True)

                if len(scaled_final_df.columns) > 1:
                    st.subheader("ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ")
                    corr_matrix = scaled_final_df.corr(numeric_only=True)
                    fig_heatmap = px.imshow(corr_matrix, text_auto=True, aspect="auto", color_continuous_scale='RdBu_r', range_color=[-1, 1])
                    st.plotly_chart(fig_heatmap, use_container_width=True)

                    st.subheader("êµì°¨ ìƒê´€ê´€ê³„ ë¶„ì„ (ìµœì  ì‹œì°¨)")

                    driver_vars = [col for col in scaled_final_df.columns if 'Sentiment' in col or 'Naver' in col or 'News_' in col or 'search' in col.lower()]
                    outcome_vars = [col for col in scaled_final_df.columns if 'ìˆ˜ì…' in col or 'ë„ë§¤ê°€ê²©' in col or 'price' in col.lower()]

                    if not driver_vars or not outcome_vars:
                        st.info("êµì°¨ ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•œ ë³€ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ê°ì„±/ê²€ìƒ‰ëŸ‰(ë“œë¼ì´ë²„)ê³¼ ìˆ˜ì…/ê°€ê²©(ê²°ê³¼) ë°ì´í„°ë¥¼ ëª¨ë‘ ë¶ˆëŸ¬ì™”ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                    else:
                        best_correlations = []
                        max_lag = 5
                        for driver in driver_vars:
                            for outcome in outcome_vars:
                                if driver == outcome:
                                    continue
                                max_corr_val = None
                                best_lag = 0
                                for lag in range(-max_lag, max_lag + 1):
                                    if lag == 0:
                                        continue
                                    try:
                                        if lag > 0:
                                            corr = scaled_final_df[driver].corr(scaled_final_df[outcome].shift(lag))
                                        else:
                                            corr = scaled_final_df[driver].shift(abs(lag)).corr(scaled_final_df[outcome])
                                    except Exception:
                                        corr = np.nan
                                    if pd.notna(corr):
                                        if max_corr_val is None or abs(corr) > abs(max_corr_val):
                                            max_corr_val = corr
                                            best_lag = lag
                                if max_corr_val is not None:
                                    best_correlations.append({
                                        'driver': driver,
                                        'outcome': outcome,
                                        'lag': best_lag,
                                        'correlation': max_corr_val
                                    })

                        if best_correlations:
                            best_corr_df = pd.DataFrame(best_correlations)
                            top_3_corrs = best_corr_df.iloc[best_corr_df['correlation'].abs().sort_values(ascending=False).index].head(3)

                            st.markdown("##### ğŸ“ˆ **ê°€ì¥ ë†’ì€ êµì°¨ ìƒê´€ê´€ê³„ ìƒìœ„ 3**")

                            for _, row in top_3_corrs.iterrows():
                                driver = row['driver']
                                outcome = row['outcome']
                                lag = row['lag']
                                corr = row['correlation']
                                direction = "ê¸ì •ì " if corr > 0 else "ë¶€ì •ì "
                                sentence = f"**{driver}**ì˜ ë³€í™”ëŠ” **{abs(lag)}ì£¼ í›„** **{outcome}**ì™€ **{direction}** ê´€ê³„ë¥¼ ê°€ì§‘ë‹ˆë‹¤ (ìƒê´€ê³„ìˆ˜: {corr:.2f})."
                                st.markdown(f"â€¢ {sentence}")
                        else:
                            st.info("êµì°¨ ìƒê´€ê´€ê³„ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")

                st.subheader("ì‚°ì ë„ í–‰ë ¬ (Scatter Matrix)")
                try:
                    fig_matrix = px.scatter_matrix(scaled_final_df.reset_index(),
                                                   dimensions=scaled_final_df.columns,
                                                   title="í†µí•© ë°ì´í„° ì‚°ì ë„ í–‰ë ¬ (Min-Max Scaling)")
                    st.plotly_chart(fig_matrix, use_container_width=True)
                except Exception as e:
                    st.warning(f"ì‚°ì ë„ í–‰ë ¬ ìƒì„± ì‹¤íŒ¨: {e}")

            else:
                st.warning("ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•´ ë‘˜ ì´ìƒì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            st.warning("ë°ì´í„°í”„ë ˆì„ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì™¸ë¶€ ë°ì´í„°(ë„¤ì´ë²„/ë‰´ìŠ¤/KAMIS)ë¥¼ ë¶ˆëŸ¬ì™€ì£¼ì„¸ìš”.")
    else:
        st.warning("2ë‹¨ê³„ì—ì„œ ë°ì´í„°ê°€ ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

with tab5:
    st.header("ì‹œê³„ì—´ ë¶„í•´ ë° ì˜ˆì¸¡ (by Prophet)")
    if 'final_df' in st.session_state and not st.session_state['final_df'].empty:
        final_df = st.session_state['final_df'].reset_index()
        forecast_col = st.selectbox("ì˜ˆì¸¡ ëŒ€ìƒ ë³€ìˆ˜ ì„ íƒ", [c for c in final_df.columns if c != 'ë‚ ì§œ'])

        regressors_cols = [col for col in final_df.columns if col != 'ë‚ ì§œ' and col != forecast_col]
        selected_regressors = st.multiselect("ì˜ˆì¸¡ì— ì‚¬ìš©í•  ì™¸ë¶€ ë³€ìˆ˜ ì„ íƒ", regressors_cols, default=[])

        if 'fig_decompose' not in st.session_state:
            st.session_state.fig_decompose = None
        if 'fig_forecast' not in st.session_state:
            st.session_state.fig_forecast = None

        if st.button("ğŸ“ˆ ì„ íƒí•œ ë³€ìˆ˜ë¡œ ì˜ˆì¸¡ ì‹¤í–‰í•˜ê¸°"):
            required_cols = ['ë‚ ì§œ', forecast_col] + selected_regressors
            if not all(col in final_df.columns for col in required_cols):
                st.error("ì„ íƒí•œ ì˜ˆì¸¡ ë³€ìˆ˜ ë˜ëŠ” ì™¸ë¶€ ë³€ìˆ˜ê°€ ë°ì´í„°ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëª¨ë“  ì™¸ë¶€ ë°ì´í„°(ë„¤ì´ë²„/ë‰´ìŠ¤)ë¥¼ ë¨¼ì € ë¶ˆëŸ¬ì™”ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                st.stop()

            ts_data_raw = final_df[required_cols].copy()

            prophet_df = ts_data_raw.rename(columns={'ë‚ ì§œ': 'ds', forecast_col: 'y'})
            prophet_df['ds'] = pd.to_datetime(prophet_df['ds'])
            prophet_df = prophet_df.interpolate(method='linear', limit_direction='both')
            prophet_df = prophet_df.dropna()

            if len(prophet_df) < 24:
                st.warning(f"ìµœì†Œ 24ì£¼ ì´ìƒì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬: {len(prophet_df)}ì£¼")
            else:
                with st.spinner(f"'{forecast_col}' ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì¤‘..."):
                    period = 52 if len(prophet_df) >= 104 else max(4, int(len(prophet_df) / 2))
                    try:
                        decomposition = seasonal_decompose(prophet_df.set_index('ds')['y'], model='additive', period=period)
                        fig_decompose = go.Figure()
                        fig_decompose.add_trace(go.Scatter(x=decomposition.observed.index, y=decomposition.observed, name='Observed'))
                        fig_decompose.add_trace(go.Scatter(x=decomposition.trend.index, y=decomposition.trend, name='Trend'))
                        fig_decompose.add_trace(go.Scatter(x=decomposition.seasonal.index, y=decomposition.seasonal, name='Seasonal'))
                        st.session_state.fig_decompose = fig_decompose
                    except Exception as e:
                        st.warning(f"ì‹œê³„ì—´ ë¶„í•´ ì‹¤íŒ¨: {e}")

                    for reg in selected_regressors:
                        min_val = prophet_df[reg].min()
                        max_val = prophet_df[reg].max()
                        if pd.isna(min_val) or pd.isna(max_val) or max_val - min_val == 0:
                            prophet_df[reg] = 0.0
                        else:
                            prophet_df[reg] = (prophet_df[reg] - min_val) / (max_val - min_val)

                    m = Prophet()
                    for reg in selected_regressors:
                        m.add_regressor(reg)

                    try:
                        m.fit(prophet_df)
                    except Exception as e:
                        st.error(f"Prophet í•™ìŠµ ì‹¤íŒ¨: {e}")
                        st.stop()

                    future = m.make_future_dataframe(periods=12, freq='W')
                    for reg in selected_regressors:
                        future[reg] = prophet_df.tail(1).iloc[0][reg]

                    forecast = m.predict(future)

                    fig_forecast = plot_plotly(m, forecast)
                    st.session_state.fig_forecast = fig_forecast
                    st.session_state['forecast_data'] = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(12)

                    st.subheader("ì˜ˆì¸¡ ìš”ì¸ë³„ ê¸°ì—¬ë„")
                    try:
                        fig_components = plot_components_plotly(m, forecast)
                        st.plotly_chart(fig_components, use_container_width=True)
                    except Exception:
                        st.info("êµ¬ì„± ìš”ì†Œ í”Œë¡¯ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if st.session_state.fig_decompose is not None:
            st.plotly_chart(st.session_state.fig_decompose, use_container_width=True)
        if st.session_state.fig_forecast is not None:
            st.plotly_chart(st.session_state.fig_forecast, use_container_width=True)
            st.dataframe(st.session_state['forecast_data'])
    else:
        st.info("4ë²ˆ íƒ­ì—ì„œ ë°ì´í„°ê°€ í†µí•©ë˜ì–´ì•¼ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                                        
