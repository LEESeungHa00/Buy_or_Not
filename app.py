import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import requests
from datetime import datetime, timedelta
from functools import reduce
from pytrends.request import TrendReq
import json
import urllib.request
import yfinance as yf
import gspread
from google.oauth2.service_account import Credentials
import time

# --- Google Sheets Connection ---
@st.cache_resource
def get_gspread_client():
    """Google Sheets í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        creds = Credentials.from_service_account_info(
            st.secrets["gcp_service_account"],
            scopes=[
                "https://www.googleapis.com/auth/spreadsheets",
                "https://www.googleapis.com/auth/drive",
            ],
        )
        return gspread.authorize(creds)
    except Exception as e:
        st.error(f"Google Sheets ì¸ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. st.secrets ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”: {e}")
        return None

@st.cache_data(ttl=600)
def read_gsheet(_gs_client, sheet_name):
    """ì§€ì •ëœ ì‹œíŠ¸ì—ì„œ ëª¨ë“  ë°ì´í„°ë¥¼ ì½ì–´ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. (ëŒ€ìš©ëŸ‰ ë°ì´í„° ìµœì í™”)"""
    try:
        spreadsheet = _gs_client.open_by_key(st.secrets["google_sheet_key"])
        worksheet = spreadsheet.worksheet(sheet_name)
        with st.spinner(f"'{sheet_name}' ì‹œíŠ¸ì—ì„œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì½ëŠ” ì¤‘..."):
            all_values = worksheet.get_all_values()
        if not all_values or len(all_values) < 2:
            return pd.DataFrame()
        header = all_values[0]
        data = all_values[1:]
        df = pd.DataFrame(data, columns=header)
        # ë°ì´í„° íƒ€ì… ë³€í™˜ (ìˆ«ìí˜•, ë‚ ì§œí˜• ë“±)
        for col in df.columns:
            if 'ê°€ê²©' in col or 'ëŸ‰' in col or 'ì•¡' in col or 'Price' in col or 'Value' in col or 'Volume' in col:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            # --- [FIX START] 'ë‚ ì§œ' ì»¬ëŸ¼ë„ ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ë„ë¡ ìˆ˜ì • ---
            elif 'ì¼ì' in col or 'Date' in col or 'ë‚ ì§œ' in col:
                df[col] = pd.to_datetime(df[col], errors='coerce')
            # --- [FIX END] ---
        return df
    except gspread.exceptions.WorksheetNotFound:
        return pd.DataFrame()
    except Exception as e:
        st.warning(f"'{sheet_name}' ì‹œíŠ¸ë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()

def update_gsheet(client, sheet_name, df_to_add):
    """ì§€ì •ëœ ì‹œíŠ¸ì— ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. í—¤ë”ëŠ” í•„ìš” ì‹œ í•œ ë²ˆë§Œ ê¸°ë¡í•©ë‹ˆë‹¤."""
    if df_to_add.empty: return
    try:
        spreadsheet = client.open_by_key(st.secrets["google_sheet_key"])
        try:
            worksheet = spreadsheet.worksheet(sheet_name)
            if not worksheet.get_all_records(numericise_ignore=['all']): # Use a lightweight check
                 worksheet.update([df_to_add.columns.values.tolist()])
        except gspread.exceptions.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=1, cols=len(df_to_add.columns))
            worksheet.update([df_to_add.columns.values.tolist()])

        worksheet.append_rows(df_to_add.astype(str).values.tolist(), value_input_option='USER_ENTERED')
    except Exception as e:
        st.error(f"'{sheet_name}' ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# --- Data Fetching & Processing Functions ---
def process_new_trade_data(gs_client, uploaded_file):
    """ìƒˆë¡œ ì—…ë¡œë“œëœ íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  Google Sheetì— ì €ì¥í•©ë‹ˆë‹¤."""
    sheet_name = "TDS"
    file_name = uploaded_file.name
    try:
        df = pd.read_csv(uploaded_file) if file_name.endswith('.csv') else pd.read_excel(uploaded_file)
        if 'Date' not in df.columns or 'Category' not in df.columns:
            st.error("ì—…ë¡œë“œëœ íŒŒì¼ì— 'Date'ì™€ 'Category' ì»¬ëŸ¼ì´ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤."); return None

        df['Date'] = pd.to_datetime(df['Date'])
        df_to_save = df.copy()
        df_to_save['file_name'] = file_name
        
        with st.spinner(f"'{file_name}'ì„ Google Sheetì— ì €ì¥ ì¤‘..."):
            update_gsheet(gs_client, sheet_name, df_to_save)
        st.sidebar.info(f"'{file_name}'ì„ Google Sheetì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        return df
    except Exception as e:
        st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"); return None

def fetch_yfinance_data(gs_client, tickers, start_date, end_date):
    sheet_name = "yfinance"
    all_data = []
    df_sheet = read_gsheet(gs_client, sheet_name)

    for name, ticker in tickers.items():
        if not df_sheet.empty and 'Ticker' in df_sheet.columns:
            cached_data = df_sheet[(df_sheet['Ticker'] == ticker) & (df_sheet['ì¡°ì‚¬ì¼ì'] >= start_date) & (df_sheet['ì¡°ì‚¬ì¼ì'] <= end_date)]
            if len(cached_data) >= (end_date - start_date).days * 0.8:
                 st.sidebar.info(f"'{name}' ë°ì´í„°ë¥¼ Google Sheetì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                 all_data.append(cached_data.rename(columns={'Price': f'{name} ì„ ë¬¼ê°€ê²©(USD)'})[['ì¡°ì‚¬ì¼ì', f'{name} ì„ ë¬¼ê°€ê²©(USD)']])
                 continue

        with st.spinner(f"'{name}' ë°ì´í„°ë¥¼ Yahoo Finance APIì—ì„œ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
            data = yf.download(ticker, start=start_date, end=end_date, progress=False)
            if not data.empty:
                df = data[['Close']].copy().reset_index().rename(columns={'Date': 'ì¡°ì‚¬ì¼ì', 'Close': 'Price'})
                df['Ticker'] = ticker
                update_gsheet(gs_client, sheet_name, df)
                all_data.append(df.rename(columns={'Price': f'{name} ì„ ë¬¼ê°€ê²©(USD)'})[['ì¡°ì‚¬ì¼ì', f'{name} ì„ ë¬¼ê°€ê²©(USD)']])
            else: st.sidebar.warning(f"'{name}'ì— ëŒ€í•œ API ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    if not all_data: return pd.DataFrame()
    return reduce(lambda left, right: pd.merge(left, right, on='ì¡°ì‚¬ì¼ì', how='outer'), all_data)

def fetch_trends_data(gs_client, keywords, start_date, end_date, naver_keys):
    sheet_name = "ë„¤ì´ë²„ë°ì´í„°ë©"
    all_data = []
    df_sheet = read_gsheet(gs_client, sheet_name)
    
    for keyword in keywords:
        google_col = f'Google_{keyword}'; naver_col = f'Naver_{keyword}'
        if not df_sheet.empty and 'ë‚ ì§œ' in df_sheet.columns and (google_col in df_sheet.columns or naver_col in df_sheet.columns):
            cached_data = df_sheet[(df_sheet['ë‚ ì§œ'] >= start_date) & (df_sheet['ë‚ ì§œ'] <= end_date)]
            if not cached_data.empty:
                 st.sidebar.info(f"'{keyword}' ê²€ìƒ‰ëŸ‰ ë°ì´í„°ë¥¼ Google Sheetì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                 cols_to_use = ['ë‚ ì§œ']
                 if google_col in cached_data.columns: cols_to_use.append(google_col)
                 if naver_col in cached_data.columns: cols_to_use.append(naver_col)
                 all_data.append(cached_data[cols_to_use])
                 continue

        with st.spinner(f"'{keyword}' ê²€ìƒ‰ëŸ‰ ë°ì´í„°ë¥¼ APIì—ì„œ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
            keyword_dfs = [] # --- [FIX] keywordë³„ë¡œ ë°ì´í„°ë¥¼ ëª¨ì„ ë¦¬ìŠ¤íŠ¸ ---

            pytrends = TrendReq(hl='ko-KR', tz=540)
            timeframe = f"{start_date.strftime('%Y-%m-%d')} {end_date.strftime('%Y-%m-%d')}"
            pytrends.build_payload([keyword], cat=0, timeframe=timeframe, geo='KR', gprop='')
            google_df = pytrends.interest_over_time()
            if not google_df.empty and keyword in google_df.columns:
                google_df_renamed = google_df.reset_index().rename(columns={'date': 'ë‚ ì§œ', keyword: google_col})
                keyword_dfs.append(google_df_renamed[['ë‚ ì§œ', google_col]])

            if naver_keys['id'] and naver_keys['secret']:
                url = "https://openapi.naver.com/v1/datalab/search"
                body = json.dumps({"startDate": start_date.strftime('%Y-%m-%d'), "endDate": end_date.strftime('%Y-%m-%d'), "timeUnit": "date", "keywordGroups": [{"groupName": keyword, "keywords": [keyword]}]})
                request = urllib.request.Request(url)
                request.add_header("X-Naver-Client-Id", naver_keys['id']); request.add_header("X-Naver-Client-Secret", naver_keys['secret']); request.add_header("Content-Type", "application/json")
                response = urllib.request.urlopen(request, data=body.encode("utf-8"))
                if response.getcode() == 200:
                    naver_raw = json.loads(response.read().decode('utf-8'))
                    naver_df = pd.DataFrame(naver_raw['results'][0]['data']).rename(columns={'period': 'ë‚ ì§œ', 'ratio': naver_col})
                    naver_df['ë‚ ì§œ'] = pd.to_datetime(naver_df['ë‚ ì§œ'])
                    keyword_dfs.append(naver_df)
            
            # --- [FIX] ë°ì´í„°ë¥¼ ëª¨ë‘ ëª¨ì€ í›„ í•œë²ˆì— í†µí•©í•˜ê³  ì €ì¥ ---
            if keyword_dfs:
                merged_keyword_df = reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), keyword_dfs)
                all_data.append(merged_keyword_df)
                update_gsheet(gs_client, sheet_name, merged_keyword_df)

    if not all_data: return pd.DataFrame()
    return reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), all_data)


def fetch_kamis_data(gs_client, item_info, start_date, end_date, kamis_keys):
    all_data = []
    date_range = pd.date_range(start=start_date, end=end_date)
    if len(date_range) > 180: st.sidebar.warning(f"KAMIS ì¡°íšŒ ê¸°ê°„ì´ {len(date_range)}ì¼ë¡œ ê¹ë‹ˆë‹¤. ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    progress_bar = st.sidebar.progress(0, text="KAMIS ë°ì´í„° ì¡°íšŒ ì¤‘...")
    
    for i, date in enumerate(date_range):
        date_str = date.strftime('%Y-%m-%d')
        url = (f"http://www.kamis.or.kr/service/price/xml.do?p_product_cls_code=02&p_item_category_code={item_info['cat_code']}"
               f"&p_item_code={item_info['item_code']}&p_regday={date_str}&p_convert_kg_yn=Y"
               f"&p_cert_key={kamis_keys['key']}&p_cert_id={kamis_keys['id']}&p_returntype=json")
        try:
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                items = data.get("data", {}).get("item", [])
                if items:
                    price_str = items[0].get('dpr1', '0').replace(',', '')
                    if price_str.isdigit() and int(price_str) > 0:
                        all_data.append({'ì¡°ì‚¬ì¼ì': date, 'ë„ë§¤ê°€ê²©(ì›)': int(price_str)})
        except Exception: continue
        finally: progress_bar.progress((i + 1) / len(date_range), text=f"KAMIS ë°ì´í„° ì¡°íšŒ ì¤‘... {date_str}")
    
    progress_bar.empty()
    if not all_data: st.sidebar.warning("í•´ë‹¹ ê¸°ê°„ì— ëŒ€í•œ KAMIS ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return pd.DataFrame()
    df = pd.DataFrame(all_data)
    return df

# --- Constants ---
COFFEE_TICKERS_YFINANCE = {"ë¯¸êµ­ ì»¤í”¼ C": "KC=F", "ëŸ°ë˜ ë¡œë¶€ìŠ¤íƒ€": "RC=F"}
KAMIS_CATEGORIES = {"ì±„ì†Œë¥˜": "100", "ê³¼ì¼ë¥˜": "200", "ì¶•ì‚°ë¬¼": "300", "ìˆ˜ì‚°ë¬¼": "400"}
KAMIS_ITEMS = {"ì±„ì†Œë¥˜": {"ë°°ì¶”": "111", "ë¬´": "112", "ì–‘íŒŒ": "114", "ë§ˆëŠ˜": "141"}, "ê³¼ì¼ë¥˜": {"ì‚¬ê³¼": "211", "ë°”ë‚˜ë‚˜": "214", "ì•„ë³´ì¹´ë„": "215"}, "ì¶•ì‚°ë¬¼": {"ì†Œê³ ê¸°": "311", "ë¼ì§€ê³ ê¸°": "312"}, "ìˆ˜ì‚°ë¬¼": {"ê³ ë“±ì–´": "411", "ì˜¤ì§•ì–´": "413"}}

# --- Streamlit App ---
st.set_page_config(layout="wide")
st.title("ğŸ“Š ë°ì´í„° íƒìƒ‰ ë° í†µí•© ëŒ€ì‹œë³´ë“œ (Google Sheets ì—°ë™)")

gs_client = get_gspread_client()
if gs_client is None: st.stop()

st.sidebar.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
st.sidebar.subheader("1. ë¶„ì„ ë°ì´í„°ì…‹ ì„ íƒ")

if 'tds_data' not in st.session_state:
    st.session_state.tds_data = read_gsheet(gs_client, "TDS")

if not st.session_state.tds_data.empty and 'file_name' in st.session_state.tds_data.columns:
    existing_files = sorted(st.session_state.tds_data['file_name'].unique())
    selection_options = ["ìƒˆ íŒŒì¼ ì—…ë¡œë“œ"] + existing_files
    selected_option = st.sidebar.selectbox("ë°ì´í„°ì…‹ ì„ íƒ ë˜ëŠ” ìƒˆ íŒŒì¼ ì—…ë¡œë“œ", selection_options)

    if selected_option == "ìƒˆ íŒŒì¼ ì—…ë¡œë“œ":
        uploaded_file = st.sidebar.file_uploader("ì—…ë¡œë“œí•  ì‹ ê·œ íŒŒì¼", type=['csv', 'xlsx'])
        if uploaded_file:
            if uploaded_file.name in existing_files:
                st.sidebar.warning(f"'{uploaded_file.name}'ì€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ìœ„ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")
                raw_trade_df = None
            else:
                new_data = process_new_trade_data(gs_client, uploaded_file)
                if new_data is not None:
                    st.session_state.tds_data = pd.concat([st.session_state.tds_data, new_data])
                raw_trade_df = new_data
    else:
        raw_trade_df = st.session_state.tds_data[st.session_state.tds_data['file_name'] == selected_option].copy()
        raw_trade_df = raw_trade_df.drop(columns=['file_name'])
else:
    st.sidebar.info("ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    uploaded_file = st.sidebar.file_uploader("ìˆ˜ì¶œì… ë°ì´í„° íŒŒì¼ ì—…ë¡œë“œ", type=['csv', 'xlsx'])
    if uploaded_file:
        raw_trade_df = process_new_trade_data(gs_client, uploaded_file)
        if raw_trade_df is not None:
            st.session_state.tds_data = raw_trade_df # Initialize state
    else:
        raw_trade_df = None

if raw_trade_df is None:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ê±°ë‚˜ ìƒˆ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."); st.stop()

# --- Subsequent UI and Logic ---
file_start_date, file_end_date = raw_trade_df['Date'].min(), raw_trade_df['Date'].max()
category_options = sorted(raw_trade_df['Category'].unique())
selected_categories = st.sidebar.multiselect("2. ë¶„ì„í•  í’ˆëª© ì¹´í…Œê³ ë¦¬ ì„ íƒ", category_options, default=category_options[0] if category_options else None)
keyword_input = st.sidebar.text_input("3. ê²€ìƒ‰ì–´ ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„)", ", ".join(selected_categories) if selected_categories else None)
search_keywords = [k.strip() for k in keyword_input.split(',') if k.strip()]
st.sidebar.subheader("4. ë¶„ì„ ê¸°ê°„ ì„¤ì •")
start_date_input = st.sidebar.date_input('ì‹œì‘ì¼', file_start_date, min_value=file_start_date, max_value=file_end_date)
end_date_input = st.sidebar.date_input('ì¢…ë£Œì¼', file_end_date, min_value=start_date_input, max_value=file_end_date)
start_date = pd.to_datetime(start_date_input)
end_date = pd.to_datetime(end_date_input)

# --- External Data Loading Section ---
st.sidebar.subheader("ğŸ”— ì™¸ë¶€ ê°€ê²© ë°ì´í„°")
is_coffee_selected = any('ì»¤í”¼' in str(cat) for cat in selected_categories)

if is_coffee_selected:
    st.sidebar.info("Yahoo Financeì—ì„œ ì„ ë¬¼ê°€ê²©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    if st.sidebar.button("ì„ ë¬¼ê°€ê²© ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
        df = fetch_yfinance_data(gs_client, COFFEE_TICKERS_YFINANCE, start_date, end_date)
        st.session_state['wholesale_data'] = df
else:
    st.sidebar.info("KAMISì—ì„œ ë†ì‚°ë¬¼ ë„ë§¤ê°€ê²© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    kamis_api_key = st.sidebar.text_input("KAMIS API Key", type="password")
    kamis_api_id = st.sidebar.text_input("KAMIS API ID", type="password")
    cat_name = st.sidebar.selectbox("í’ˆëª© ë¶„ë¥˜ ì„ íƒ", list(KAMIS_CATEGORIES.keys()))
    if cat_name:
        item_name = st.sidebar.selectbox("ì„¸ë¶€ í’ˆëª© ì„ íƒ", list(KAMIS_ITEMS[cat_name].keys()))
        if st.sidebar.button("KAMIS ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
            if kamis_api_key and kamis_api_id:
                item_info = {'item_code': KAMIS_ITEMS[cat_name][item_name], 'cat_code': KAMIS_CATEGORIES[cat_name]}
                df = fetch_kamis_data(gs_client, item_info, start_date, end_date, {'key': kamis_api_key, 'id': kamis_api_id})
                st.session_state['wholesale_data'] = df
            else: st.sidebar.error("KAMIS API Keyì™€ IDë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")

raw_wholesale_df = st.session_state.get('wholesale_data', pd.DataFrame())

# --- Search Data Loading Section ---
st.sidebar.subheader("ğŸ“° ê²€ìƒ‰ëŸ‰ ë°ì´í„°")
naver_client_id = st.sidebar.text_input("Naver API Client ID", type="password")
naver_client_secret = st.sidebar.text_input("Naver API Client Secret", type="password")
if st.sidebar.button("ê²€ìƒ‰ëŸ‰ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
    if not search_keywords: st.sidebar.warning("ê²€ìƒ‰ì–´ë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        df = fetch_trends_data(gs_client, search_keywords, start_date, end_date, {'id': naver_client_id, 'secret': naver_client_secret})
        st.session_state['search_data'] = df

raw_search_df = st.session_state.get('search_data', pd.DataFrame())

# --- Main Display Area ---
tab1, tab2, tab3 = st.tabs(["1ï¸âƒ£ ì›ë³¸ ë°ì´í„° í™•ì¸", "2ï¸âƒ£ ë°ì´í„° í‘œì¤€í™”", "3ï¸âƒ£ ìµœì¢… í†µí•© ë°ì´í„°"])

with tab1:
    st.subheader("A. ìˆ˜ì¶œì… ë°ì´í„°"); st.dataframe(raw_trade_df.head())
    st.subheader("B. ì™¸ë¶€ ê°€ê²© ë°ì´í„°"); st.dataframe(raw_wholesale_df.head())
    st.subheader("C. ê²€ìƒ‰ëŸ‰ ë°ì´í„°"); st.dataframe(raw_search_df.head())

with tab2:
    if not selected_categories: st.warning("ë¶„ì„í•  ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
    else:
        st.subheader("2-1. ë¶„ì„ ëŒ€ìƒ í’ˆëª© í•„í„°ë§")
        trade_df_in_range = raw_trade_df[(raw_trade_df['Date'] >= start_date) & (raw_trade_df['Date'] <= end_date)]
        filtered_trade_df = trade_df_in_range[trade_df_in_range['Category'].isin(selected_categories)].copy()
        st.write(f"ì„ íƒëœ ì¹´í…Œê³ ë¦¬: **{', '.join(selected_categories)}**"); st.dataframe(filtered_trade_df.head())
        
        st.subheader("2-2. ì£¼(Week) ë‹¨ìœ„ ë°ì´í„°ë¡œ ì§‘ê³„")
        if not filtered_trade_df.empty:
            filtered_trade_df.set_index('Date', inplace=True)
            trade_weekly = filtered_trade_df.resample('W-Mon').agg({'Value': 'sum', 'Volume': 'sum'})
            trade_weekly['Unit Price'] = trade_weekly['Value'] / trade_weekly['Volume']
            trade_weekly.columns = ['ìˆ˜ì…ì•¡(USD)', 'ìˆ˜ì…ëŸ‰(KG)', 'ìˆ˜ì…ë‹¨ê°€(USD/KG)']
            
            wholesale_weekly = pd.DataFrame()
            if not raw_wholesale_df.empty:
                date_col = 'ì¡°ì‚¬ì¼ì' if 'ì¡°ì‚¬ì¼ì' in raw_wholesale_df.columns else 'ë‚ ì§œ'
                raw_wholesale_df[date_col] = pd.to_datetime(raw_wholesale_df[date_col], errors='coerce')
                wholesale_df_processed = raw_wholesale_df.set_index(date_col)
                price_cols = [col for col in wholesale_df_processed.columns if 'ê°€ê²©' in col]
                agg_dict = {col: 'mean' for col in price_cols}
                if agg_dict:
                    wholesale_weekly = wholesale_df_processed.resample('W-Mon').agg(agg_dict)
                    if 'ë„ë§¤ê°€ê²©(ì›)' in wholesale_weekly.columns:
                        wholesale_weekly['ë„ë§¤ê°€ê²©(USD)'] = wholesale_weekly['ë„ë§¤ê°€ê²©(ì›)'] / 1350 # í™˜ìœ¨ ì ìš©
                        wholesale_weekly.drop(columns=['ë„ë§¤ê°€ê²©(ì›)'], inplace=True)
            
            search_weekly = pd.DataFrame()
            if not raw_search_df.empty:
                raw_search_df['ë‚ ì§œ'] = pd.to_datetime(raw_search_df['ë‚ ì§œ'], errors='coerce')
                search_df_processed = raw_search_df.set_index('ë‚ ì§œ')
                numeric_cols = search_df_processed.select_dtypes(include=np.number).columns
                search_weekly = search_df_processed.resample('W-Mon').agg({col: 'mean' for col in numeric_cols})
            
            st.write("â–¼ ì¼ë³„(Daily) vs ì£¼ë³„(Weekly) ìˆ˜ì…ëŸ‰ ë¹„êµ")
            col1, col2 = st.columns(2)
            with col1: st.line_chart(filtered_trade_df['Volume'])
            with col2: st.line_chart(trade_weekly['ìˆ˜ì…ëŸ‰(KG)'])
        else: st.warning("ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

with tab3:
    if 'trade_weekly' in locals() and not trade_weekly.empty:
        dfs_to_concat = [trade_weekly]
        if 'wholesale_weekly' in locals() and not wholesale_weekly.empty: dfs_to_concat.append(wholesale_weekly)
        if 'search_weekly' in locals() and not search_weekly.empty: dfs_to_concat.append(search_weekly)
        
        final_df = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='outer'), dfs_to_concat)
        final_df = final_df.interpolate(method='linear', limit_direction='forward').dropna(how='all')
        
        st.dataframe(final_df)
        st.subheader("ìµœì¢… í†µí•© ë°ì´í„° ì‹œê°í™”")
        if not final_df.empty:
            fig = px.line(final_df, labels={'value': 'ê°’', 'index': 'ë‚ ì§œ', 'variable': 'ë°ì´í„° ì¢…ë¥˜'}, title="ìµœì¢… í†µí•© ë°ì´í„° ì‹œê³„ì—´ ì¶”ì´")
            st.plotly_chart(fig, use_container_width=True)
    else: st.warning("í†µí•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

