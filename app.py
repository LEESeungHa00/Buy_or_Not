import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
import requests
from datetime import datetime, timedelta, timezone
from functools import reduce
import json
import urllib.request
from google.oauth2 import service_account
from google.cloud import bigquery
import pandas_gbq
import feedparser
from urllib.parse import quote
from prophet import Prophet
from prophet.plot import plot_plotly, plot_components_plotly
from kamis_data import KAMIS_FULL_DATA

# Advanced Analysis Libraries
from scipy.stats import pearsonr, spearmanr, kendalltau
from statsmodels.tsa.stattools import adfuller
from statsmodels.stats.multitest import multipletests
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Transformers / HuggingFace
from transformers import pipeline
from huggingface_hub import login as hf_login

# Optional: robust article text fetch
try:
    from newspaper import Article, Config as NewsConfig
    _HAS_NEWSPAPER = True
except ImportError:
    _HAS_NEWSPAPER = False

# ----------------------------
#  Configuration / Constants
# ----------------------------
st.set_page_config(layout="wide")
st.title("ğŸ“Š í†µí•© ë°ì´í„° ê¸°ë°˜ íƒìƒ‰ ë° ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ")

BQ_DATASET = "data_explorer"
BQ_TABLE_NAVER = "naver_trends_cache"
BQ_TABLE_NEWS = "news_sentiment_finbert"
BQ_TABLE_TRADE = "tds_data"

# ----------------------------
#  Helper Functions (Analysis & Interpretation)
# ----------------------------
@st.cache_data
def calculate_advanced_correlation(df, method='pearson', p_adjust_method='fdr_bh'):
    """Calculate correlation matrix and corresponding p-values."""
    df_numeric = df.select_dtypes(include=np.number).dropna(how='all', axis=1)
    cols = df_numeric.columns
    corr_matrix = pd.DataFrame(np.ones((len(cols), len(cols))), index=cols, columns=cols)
    pval_matrix = pd.DataFrame(np.ones((len(cols), len(cols))), index=cols, columns=cols)
    
    pvalues_list = []
    
    for i in range(len(cols)):
        for j in range(i + 1, len(cols)):
            col1_data = df_numeric[cols[i]].dropna()
            col2_data = df_numeric[cols[j]].dropna()
            
            common_index = col1_data.index.intersection(col2_data.index)
            if len(common_index) < 3: continue

            col1_data = col1_data.loc[common_index]
            col2_data = col2_data.loc[common_index]

            if method == 'pearson':
                corr, pval = pearsonr(col1_data, col2_data)
            elif method == 'spearman':
                corr, pval = spearmanr(col1_data, col2_data)
            else: # kendall
                corr, pval = kendalltau(col1_data, col2_data)
                
            corr_matrix.iloc[i, j] = corr_matrix.iloc[j, i] = corr
            pval_matrix.iloc[i, j] = pval_matrix.iloc[j, i] = pval
            pvalues_list.append(pval)

    # Adjust p-values for multiple comparisons
    if pvalues_list:
        _, pvals_corrected, _, _ = multipletests(pvalues_list, alpha=0.05, method=p_adjust_method)
        
        corrected_pval_matrix = pd.DataFrame(np.ones((len(cols), len(cols))), index=cols, columns=cols)
        k = 0
        for i in range(len(cols)):
            for j in range(i + 1, len(cols)):
                corrected_pval_matrix.iloc[i, j] = corrected_pval_matrix.iloc[j, i] = pvals_corrected[k]
                k += 1
    else:
        corrected_pval_matrix = pval_matrix

    return corr_matrix, corrected_pval_matrix


@st.cache_data
def find_best_lagged_correlation(df, driver_vars, outcome_vars, max_lag=12):
    """Find the best time lag for correlation between driver and outcome variables."""
    best_correlations = []
    for driver in driver_vars:
        for outcome in outcome_vars:
            if driver == outcome: continue
            
            max_corr_val = 0
            best_lag = 0
            
            for lag in range(-max_lag, max_lag + 1):
                try:
                    shifted_driver = df[driver].shift(lag)
                    corr = shifted_driver.corr(df[outcome])
                except Exception:
                    corr = np.nan

                if pd.notna(corr) and abs(corr) > abs(max_corr_val):
                    max_corr_val = corr
                    best_lag = lag
            
            if best_lag != 0:
                best_correlations.append({
                    'Driver (X)': driver,
                    'Outcome (Y)': outcome,
                    'Best Lag (Weeks)': best_lag,
                    'Correlation': max_corr_val
                })
    
    if not best_correlations:
        return pd.DataFrame()
        
    df_lags = pd.DataFrame(best_correlations)
    df_lags['Abs Correlation'] = df_lags['Correlation'].abs()
    return df_lags.sort_values('Abs Correlation', ascending=False).drop(columns=['Abs Correlation'])

def interpret_correlation(corr_matrix, pval_matrix, threshold=0.05):
    """Generates a human-readable interpretation of the correlation results."""
    strong_corrs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            corr_val = corr_matrix.iloc[i, j]
            pval = pval_matrix.iloc[i, j]
            
            if pval < threshold and abs(corr_val) >= 0.5:
                direction = "ê°•í•œ ì–‘ì˜" if corr_val > 0 else "ê°•í•œ ìŒì˜"
                interpretation = (
                    f"**'{corr_matrix.columns[i]}'**ì™€(ê³¼) **'{corr_matrix.columns[j]}'** ì‚¬ì´ì—ëŠ” "
                    f"í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ **{direction} ìƒê´€ê´€ê³„**ê°€ ìˆìŠµë‹ˆë‹¤ (ìƒê´€ê³„ìˆ˜: {corr_val:.2f})."
                )
                strong_corrs.append({'text': interpretation, 'value': abs(corr_val)})
    
    if not strong_corrs:
        return "í˜„ì¬ ì„¤ì •ëœ ìœ ì˜ìˆ˜ì¤€ê³¼ ìƒê´€ê³„ìˆ˜ ê¸°ì¤€(0.5 ì´ìƒ)ì—ì„œ í†µê³„ì ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” ê°•í•œ ê´€ê³„ëŠ” ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³€ìˆ˜ ì„ íƒì´ë‚˜ ê¸°ê°„ì„ ë³€ê²½í•˜ì—¬ ë‹¤ì‹œ ë¶„ì„í•´ ë³´ì„¸ìš”."
        
    strong_corrs = sorted(strong_corrs, key=lambda x: x['value'], reverse=True)
    
    summary = "### ì£¼ìš” ë°œê²¬:\n\n" + "\n".join([f"- {corr['text']}" for corr in strong_corrs[:5]])
    summary += "\n\n* **ì–‘ì˜ ìƒê´€ê´€ê³„ (+):** í•œ ë³€ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ë‹¤ë¥¸ ë³€ìˆ˜ë„ í•¨ê»˜ ì¦ê°€í•˜ëŠ” ê²½í–¥ì„ ë³´ì…ë‹ˆë‹¤."
    summary += "\n* **ìŒì˜ ìƒê´€ê´€ê³„ (-):** í•œ ë³€ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ë‹¤ë¥¸ ë³€ìˆ˜ëŠ” ì˜¤íˆë ¤ ê°ì†Œí•˜ëŠ” ê²½í–¥ì„ ë³´ì…ë‹ˆë‹¤."
    return summary

def interpret_adf_test(adf_result):
    """Generates an interpretation of the ADF test result."""
    p_value = adf_result[1]
    if p_value < 0.05:
        return (
            f"**ê²°ë¡ : ëª¨ë¸ì´ ì•ˆì •ì ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.** (p-value: {p_value:.3f})\n\n"
            "ADF í…ŒìŠ¤íŠ¸ ê²°ê³¼, ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨)ë“¤ì´ íŠ¹ì • íŒ¨í„´ ì—†ì´ ë¬´ì‘ìœ„ì ì¸ 'ë°±ìƒ‰ì†ŒìŒ'ì— ê°€ê¹ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. "
            "ì´ëŠ” Prophet ëª¨ë¸ì´ ë°ì´í„°ì˜ ì£¼ìš” ì¶”ì„¸ì™€ ê³„ì ˆì„± íŒ¨í„´ì„ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµí–ˆë‹¤ëŠ” ê¸ì •ì ì¸ ì‹ í˜¸ì…ë‹ˆë‹¤."
        )
    else:
        return (
            f"**ê²°ë¡ : ëª¨ë¸ ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.** (p-value: {p_value:.3f})\n\n"
            "ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨)ì— ì•„ì§ ì„¤ëª…ë˜ì§€ ì•Šì€ íŠ¹ì • íŒ¨í„´ì´ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. "
            "ì´ëŠ” ëª¨ë¸ì´ ë°ì´í„°ì˜ ëª¨ë“  ì •ë³´ë¥¼ ì™„ì „íˆ í•™ìŠµí•˜ì§€ ëª»í–ˆì„ ê°€ëŠ¥ì„±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. "
            "ì˜ˆì¸¡ë ¥ì„ ë†’ì´ê¸° ìœ„í•´ ë‹¤ë¥¸ ì™¸ë¶€ ë³€ìˆ˜(Regressor)ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ Prophet íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )

def interpret_xgboost_results(r2, rmse, feature_imp_df, y_mean):
    """Generates an interpretation of XGBoost model results."""
    r2_perc = r2 * 100
    rmse_perc = (rmse / y_mean) * 100 if y_mean != 0 else 0

    top_features = feature_imp_df.sort_values('Value', ascending=False).head(3)['Feature'].tolist()

    interpretation = f"""
    ### XGBoost ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½:

    - **RÂ² Score (ì„¤ëª…ë ¥): {r2:.3f}**
      - **ì˜ë¯¸:** ìš°ë¦¬ ëª¨ë¸ì´ ì˜ˆì¸¡ ëŒ€ìƒ ë³€ìˆ˜ì˜ ì „ì²´ ë³€ë™ì„± ì¤‘ **ì•½ {r2_perc:.1f}%**ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤.
      - **íŒë‹¨:** 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤. ì´ ìˆ˜ì¹˜ê°€ ë†’ë‹¤ëŠ” ê²ƒì€ ëª¨ë¸ì´ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì˜ í¬ì°©í•˜ê³  ìˆë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.

    - **RMSE (í‰ê·  ì˜ˆì¸¡ ì˜¤ì°¨): {rmse:.3f}**
      - **ì˜ë¯¸:** ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì€ ì‹¤ì œê°’ê³¼ í‰ê· ì ìœ¼ë¡œ **ì•½ {rmse:.3f}** ë§Œí¼ì˜ ì˜¤ì°¨ë¥¼ ë³´ì…ë‹ˆë‹¤. (ì´ëŠ” ì˜ˆì¸¡ ëŒ€ìƒ í‰ê· ê°’ì˜ ì•½ {rmse_perc:.1f}% ìˆ˜ì¤€ì…ë‹ˆë‹¤.)
      - **íŒë‹¨:** 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤. ì´ ìˆ˜ì¹˜ê°€ ë‚®ë‹¤ëŠ” ê²ƒì€ ì˜ˆì¸¡ì´ ë” ì •ë°€í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

    ### ì˜ˆì¸¡ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹œ ë³€ìˆ˜ TOP 3:
    1. **{top_features[0]}**
    2. **{top_features[1]}**
    3. **{top_features[2]}**

    **ì¢…í•© ì˜ê²¬:**
    ì´ ë³€ìˆ˜ë“¤ì´ ë¯¸ë˜ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ê°€ì¥ ì¤‘ìš”í•œ ì—­í• ì„ í–ˆìŠµë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµ ìˆ˜ë¦½ ì‹œ ì´ í•µì‹¬ ì§€í‘œë“¤ì˜ ë³€í™”ë¥¼ ì£¼ì˜ ê¹Šê²Œ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
    """
    return interpretation

# ----------------------------
#  Helpers: BigQuery + Naver + KAMIS
# ----------------------------
@st.cache_resource
def get_bq_connection():
    try:
        creds_dict = st.secrets["gcp_service_account"]
        creds = service_account.Credentials.from_service_account_info(creds_dict)
        client = bigquery.Client(credentials=creds, project=creds.project_id)
        return client
    except Exception as e:
        st.error(f"Google BigQuery ì—°ê²° ì‹¤íŒ¨: secrets.tomlì„ í™•ì¸í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}")
        return None

def upload_df_to_bq(client, df, table_id):
    """Uploads a DataFrame to a specified BigQuery table."""
    try:
        pandas_gbq.to_gbq(
            df,
            f"{BQ_DATASET}.{table_id}",
            project_id=client.project,
            if_exists="append",
            credentials=client._credentials
        )
        return True, None
    except Exception as e:
        return False, str(e)

def call_naver_api(url, body, naver_keys):
    try:
        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", naver_keys['id'])
        request.add_header("X-Naver-Client-Secret", naver_keys['secret'])
        request.add_header("Content-Type", "application/json")
        response = urllib.request.urlopen(request, data=body.encode("utf-8"))
        if response.getcode() == 200:
            return json.loads(response.read().decode('utf-8'))
        return None
    except Exception as e:
        st.error(f"Naver API ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def fetch_kamis_data(_client, item_info, start_date, end_date, kamis_keys):
    start_str, end_str = start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')
    url = (f"http://www.kamis.or.kr/service/price/xml.do?action=periodWholesaleProductList"
           f"&p_product_cls_code=01&p_startday={start_str}&p_endday={end_str}"
           f"&p_item_category_code={item_info['cat_code']}&p_item_code={item_info['item_code']}&p_kind_code={item_info['kind_code']}"
           f"&p_product_rank_code={item_info['rank_code']}&p_convert_kg_yn=Y"
           f"&p_cert_key={kamis_keys['key']}&p_cert_id={kamis_keys['id']}&p_returntype=json")
    try:
        response = requests.get(url, timeout=20, headers={"User-Agent": "Mozilla/5.0 KAMISClient/1.0"})
        if response.status_code == 200 and "data" in response.json() and "item" in response.json()["data"]:
            price_data = response.json()["data"]["item"]
            if not price_data:
                return pd.DataFrame()
            df_new = pd.DataFrame(price_data)[['regday', 'price']].rename(columns={'regday': 'ë‚ ì§œ', 'price': 'ë„ë§¤ê°€ê²©_ì›'})
            def format_kamis_date(date_str):
                processed_str = str(date_str).replace('/', '-')
                if processed_str.count('-') == 1:
                    return f"{start_date.year}-{processed_str}"
                return processed_str
            df_new['ë‚ ì§œ'] = pd.to_datetime(df_new['ë‚ ì§œ'].apply(format_kamis_date), errors='coerce')
            df_new['ë„ë§¤ê°€ê²©_ì›'] = pd.to_numeric(df_new['ë„ë§¤ê°€ê²©_ì›'].astype(str).str.replace(',', ''), errors='coerce')
            return df_new
    except Exception as e:
        st.sidebar.error(f"KAMIS API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
    return pd.DataFrame()
# ----------------------------
#  HuggingFace: ë¡œê·¸ì¸ + ì•ˆì „í•œ ëª¨ë¸ ë¡œë“œ
# ----------------------------
hf_token = st.secrets.get("huggingface", {}).get("token")

if hf_token:
    try:
        hf_login(token=hf_token)
        st.sidebar.success("HuggingFace token ì ìš©ë¨.")
    except Exception as e:
        st.sidebar.warning(f"HuggingFace ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")
else:
    st.sidebar.info("HuggingFace tokenì´ secretsì— ì—†ìŠµë‹ˆë‹¤.")

DEFAULT_MODEL_IDS = {
    "finbert": "snunlp/KR-FinBERT-SC",
    "elite": "nlptown/bert-base-multilingual-uncased-sentiment",
    "product": "cardiffnlp/twitter-xlm-roberta-base-sentiment"
}
@st.cache_resource
def load_models(model_ids, token):
    models, report = {}, {}
    for key, mid in model_ids.items():
        report[key] = {"model_id": mid, "loaded": False, "error": None}
        try:
            models[key] = pipeline("sentiment-analysis", model=mid, tokenizer=mid, token=token)
            report[key]["loaded"] = True
        except Exception as e:
            report[key]["error"] = str(e)
            models[key] = None
    return models, report
models, model_load_report = load_models(DEFAULT_MODEL_IDS, hf_token)

# ----------------------------
#  ê°ì„±ë¶„ì„ ì ìˆ˜ ë³€í™˜ í•¨ìˆ˜
# ----------------------------
def _label_score_to_signed(pred):
    if not pred: return 0.0, "neutral"
    lbl, score = str(pred.get("label", "")).lower(), float(pred.get("score", 0.0))
    if "star" in lbl:
        try:
            n = int(lbl.split()[0])
            return (n - 3) / 2.0, "positive" if n > 3 else "negative" if n < 3 else "neutral"
        except (ValueError, IndexError):
            return 0.0, "neutral"
    if any(x in lbl for x in ["neg", "negative", "ë¶€ì •"]): return -score, "negative"
    if any(x in lbl for x in ["pos", "positive", "ê¸ì •"]): return score, "positive"
    return 0.0, "neutral"

def analyze_sentiment_multi(texts, models_dict):
    results = []
    if not texts: return results
    
    preds = {key: (model(texts, truncation=True, max_length=512) if model else [None]*len(texts)) for key, model in models_dict.items()}
        
    for i, _ in enumerate(texts):
        fin_s, fin_l = _label_score_to_signed(preds.get("finbert", [])[i] if preds.get("finbert") else None)
        el_s, el_l = _label_score_to_signed(preds.get("elite", [])[i] if preds.get("elite") else None)
        pr_s, pr_l = _label_score_to_signed(preds.get("product", [])[i] if preds.get("product") else None)
        results.append({
            "FinBERT_Sentiment": fin_s, "FinBERT_Label": fin_l,
            "Elite_Sentiment": el_s, "Elite_Label": el_l,
            "Product_Sentiment": pr_s, "Product_Label": pr_l,
            "Sentiment": fin_s, "Label": fin_l # Default to FinBERT
        })
    return results
# ----------------------------
#  ë‰´ìŠ¤ ìˆ˜ì§‘/ë¶„ì„
# ----------------------------
def _fetch_article_text(url):
    if not _HAS_NEWSPAPER or not url: return ""
    try:
        cfg = NewsConfig()
        cfg.browser_user_agent = "Mozilla/5.0 (compatible; NewsBot/1.0)"
        cfg.request_timeout = 10
        art = Article(url, language='ko', config=cfg)
        art.download()
        art.parse()
        return art.text or ""
    except Exception:
        return ""

@st.cache_data(ttl=3600)
def get_news_with_multi_model_analysis(_bq_client, models_dict, keyword, days_limit=7):
    # This function is cached, so it won't reflect real-time news unless the keyword/days_limit changes.
    # For a real-time app, consider removing @st.cache_data or using a more complex caching strategy.
    
    project_id = _bq_client.project
    full_table_id = f"{project_id}.{BQ_DATASET}.{BQ_TABLE_NEWS}"

    try:
        time_limit = datetime.now(timezone.utc) - timedelta(days=days_limit)
        query = "SELECT * FROM `{}` WHERE Keyword = @keyword AND InsertedAt >= @time_limit ORDER BY ë‚ ì§œ DESC".format(full_table_id)
        job_config = bigquery.QueryJobConfig(query_parameters=[
            bigquery.ScalarQueryParameter("keyword", "STRING", keyword),
            bigquery.ScalarQueryParameter("time_limit", "TIMESTAMP", time_limit)
        ])
        df_cache = _bq_client.query(query, job_config=job_config).to_dataframe()
    except Exception:
        df_cache = pd.DataFrame()

    if not df_cache.empty:
        st.sidebar.success(f"âœ”ï¸ '{keyword}' ìµœì‹  ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹œì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return df_cache

    all_news = []
    rss_url = f"https://news.google.com/rss/search?q={quote(keyword)}&hl=ko&gl=KR&ceid=KR:ko"
    feed = feedparser.parse(rss_url)

    for entry in feed.entries[:60]: # Limit to 60 articles
        title = entry.get('title', '').strip()
        link = entry.get('link', '').strip()
        if not title: continue
        pub_date = pd.to_datetime(entry.get('published')).date() if 'published' in entry else datetime.utcnow().date()
        body = _fetch_article_text(link)
        text_for_model = (title + ". " + body).strip() if body else title
        all_news.append({"ë‚ ì§œ": pub_date, "Title": title, "RawUrl": link, "ModelInput": text_for_model})

    if not all_news:
        st.error(f"'{keyword}'ì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    df_new = pd.DataFrame(all_news).drop_duplicates(subset=["Title"])
    with st.spinner(f"ë‹¤ì¤‘ ëª¨ë¸ë¡œ '{keyword}' ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì¤‘ ({len(df_new)}ê±´)..."):
        multi = analyze_sentiment_multi(df_new['ModelInput'].tolist(), models_dict)

    multi_df = pd.DataFrame(multi)
    df_new = pd.concat([df_new.reset_index(drop=True), multi_df], axis=1)
    df_new['Keyword'] = keyword
    df_new['InsertedAt'] = datetime.now(timezone.utc)

    try:
        df_to_gbq = df_new.drop(columns=['ModelInput'])
        upload_df_to_bq(_bq_client, df_to_gbq, BQ_TABLE_NEWS)
    except Exception as e:
        st.sidebar.warning(f"BigQuery ë‰´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    return df_new.drop(columns=['ModelInput'])

# ----------------------------
#  BigQuery ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ë“¤
# ----------------------------
@st.cache_data(ttl=3600)
def get_categories_from_bq(_client):
    try:
        query = f"SELECT DISTINCT Category FROM `{_client.project}.{BQ_DATASET}.{BQ_TABLE_TRADE}` ORDER BY Category"
        return sorted(_client.query(query).to_dataframe()['Category'].astype(str).unique())
    except Exception: return []

def get_trade_data_from_bq(client, categories):
    if not categories: return pd.DataFrame()
    try:
        sql = f"SELECT * FROM `{client.project}.{BQ_DATASET}.{BQ_TABLE_TRADE}` WHERE Category IN UNNEST(@categories)"
        job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ArrayQueryParameter("categories", "STRING", categories)])
        df = client.query(sql, job_config=job_config).to_dataframe()
        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
        for col in [c for c in df.columns if 'price' in c.lower() or 'value' in c.lower() or 'volume' in c.lower()]:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        return df
    except Exception as e:
        st.error(f"BigQuery TDS ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

@st.cache_data(ttl=3600)
def fetch_naver_trends_data(_client, keywords, start_date, end_date, naver_keys):
    # Placeholder function for brevity. The actual implementation can be complex.
    st.sidebar.warning("Naver Trends API ì—°ë™ì€ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤.")
    return pd.DataFrame()

# Initialize BQ Client and session state
bq_client = get_bq_connection()
if bq_client is None: st.stop()

if 'final_df' not in st.session_state: st.session_state.final_df = pd.DataFrame()

# ----------------------------
#  Sidebar: UI & Data Loading
# ----------------------------
st.sidebar.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
categories = get_categories_from_bq(bq_client)
selected_categories = st.sidebar.multiselect("ë¶„ì„í•  í’ˆëª© ì„ íƒ", categories, default=categories[:1] if categories else [])
start_date = pd.to_datetime(st.sidebar.date_input('ì‹œì‘ì¼', datetime(2022, 1, 1)))
end_date = pd.to_datetime(st.sidebar.date_input('ì¢…ë£Œì¼', datetime.now()))
news_keyword_input = st.sidebar.text_input("ë‰´ìŠ¤ ë¶„ì„ í‚¤ì›Œë“œ", selected_categories[0] if selected_categories else "")

with st.sidebar.expander("ğŸ”‘ API í‚¤ ì…ë ¥ (ì„ íƒ)"):
    kamis_api_key = st.text_input("KAMIS API Key", type="password")
    kamis_api_id = st.text_input("KAMIS API ID", type="password")
    naver_client_id = st.text_input("Naver API Client ID", type="password")
    naver_client_secret = st.text_input("Naver API Client Secret", type="password")

if st.sidebar.button("ğŸš€ ëª¨ë“  ë°ì´í„° í†µí•© ë° ë¶„ì„ ì‹¤í–‰"):
    if not selected_categories:
        st.error("ë¶„ì„í•  í’ˆëª©ì„ 1ê°œ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
        st.stop()
        
    with st.spinner("ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  í†µí•©í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        trade_df = get_trade_data_from_bq(bq_client, selected_categories)
        trade_df_in_range = trade_df[(trade_df['Date'] >= start_date) & (trade_df['Date'] <= end_date)]
        
        if trade_df_in_range.empty:
            st.error("ì„ íƒëœ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()
        
        # Group by Date and aggregate, handling multiple categories
        trade_agg = trade_df_in_range.groupby('Date').agg(
            Value=('Value', 'sum'),
            Volume=('Volume', 'sum')
        ).copy()

        trade_agg.set_index(pd.to_datetime(trade_agg.index), inplace=True)
        trade_weekly = trade_agg.resample('W-Mon').agg(
            ìˆ˜ì…ì•¡_USD=('Value', 'sum'),
            ìˆ˜ì…ëŸ‰_KG=('Volume', 'sum')
        ).copy()
        trade_weekly['ìˆ˜ì…ë‹¨ê°€_USD_KG'] = trade_weekly['ìˆ˜ì…ì•¡_USD'] / trade_weekly['ìˆ˜ì…ëŸ‰_KG']
        trade_weekly.index.name = 'ë‚ ì§œ'
        
        all_weekly_dfs = {'trade': trade_weekly}

        if news_keyword_input:
            news_df = get_news_with_multi_model_analysis(bq_client, models, news_keyword_input)
            if not news_df.empty:
                news_df['ë‚ ì§œ'] = pd.to_datetime(news_df['ë‚ ì§œ'])
                sentiment_cols = [col for col in news_df.columns if 'Sentiment' in col]
                news_weekly = news_df.set_index('ë‚ ì§œ')[sentiment_cols].resample('W-Mon').mean()
                all_weekly_dfs['news'] = news_weekly.rename(columns=lambda x: 'News_' + x.replace("News_", ""))

        # Combine all dataframes
        final_df = reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), all_weekly_dfs.values())
        
        final_df = final_df.interpolate(method='time').fillna(method='bfill').fillna(method='ffill')
        st.session_state.final_df = final_df.dropna(how='all', axis=1).replace([np.inf, -np.inf], np.nan).dropna()
        st.success("ë°ì´í„° í†µí•© ì™„ë£Œ!")

# --- CSV Upload Section ---
st.sidebar.markdown("---")
st.sidebar.subheader("í’ˆëª© ë°ì´í„° ì—…ë¡œë“œ (CSV)")
uploaded_file = st.sidebar.file_uploader(
    "ë¶„ì„í•  í’ˆëª©ì˜ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.",
    type=['csv'],
    help="í•„ìˆ˜ ì»¬ëŸ¼: Date, Value, Volume. í—¤ë” ì´ë¦„ì€ ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
)
new_category_name = st.sidebar.text_input(
    "ì—…ë¡œë“œí•  ë°ì´í„°ì˜ í’ˆëª©ëª…(Category)ì„ ì…ë ¥í•˜ì„¸ìš”.",
    help="ì˜ˆ: ì•„ë³´ì¹´ë„, ë°”ë‚˜ë‚˜ ë“±"
)

if st.sidebar.button("BigQueryì— ì—…ë¡œë“œ"):
    if uploaded_file is not None and new_category_name.strip():
        with st.spinner("íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  BigQueryì— ì—…ë¡œë“œí•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                df_upload = pd.read_csv(uploaded_file)
                df_upload.columns = [c.lower() for c in df_upload.columns]

                required_cols = {'date', 'value', 'volume'}
                if not required_cols.issubset(df_upload.columns):
                    st.sidebar.error(f"íŒŒì¼ì— í•„ìˆ˜ ì»¬ëŸ¼('Date', 'Value', 'Volume')ì´ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    df_upload = df_upload.rename(columns={'date': 'Date', 'value': 'Value', 'volume': 'Volume'})
                    df_upload['Date'] = pd.to_datetime(df_upload['Date'], errors='coerce')
                    df_upload['Value'] = pd.to_numeric(df_upload['Value'], errors='coerce')
                    df_upload['Volume'] = pd.to_numeric(df_upload['Volume'], errors='coerce')
                    df_upload['Category'] = new_category_name.strip()
                    
                    df_to_bq = df_upload[['Date', 'Category', 'Value', 'Volume']].dropna()
                    
                    success, error_msg = upload_df_to_bq(bq_client, df_to_bq, BQ_TABLE_TRADE)
                    
                    if success:
                        st.sidebar.success(f"'{new_category_name}' ë°ì´í„° {len(df_to_bq)}ê±´ì„ BigQueryì— ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œí–ˆìŠµë‹ˆë‹¤!")
                        st.cache_data.clear() # Clear cache to refresh category list
                    else:
                        st.sidebar.error(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {error_msg}")

            except Exception as e:
                st.sidebar.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
        st.sidebar.warning("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  í’ˆëª©ëª…ì„ ì •í™•íˆ ì…ë ¥í•´ì£¼ì„¸ìš”.")


# ----------------------------
#  Main Dashboard Tabs
# ----------------------------
if not st.session_state.final_df.empty:
    final_df = st.session_state.final_df
    
    tab1, tab2, tab3 = st.tabs([
        "ğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„",
        "ğŸ“ˆ ì‹œê³„ì—´ ì˜ˆì¸¡",
        "ğŸ“„ í†µí•© ë°ì´í„°"
    ])

    with tab1:
        st.header("ìƒê´€ê´€ê³„ ë¶„ì„")
        
        col1, col2 = st.columns(2)
        with col1:
            corr_method = st.selectbox("ìƒê´€ê´€ê³„ ë¶„ì„ ë°©ë²•", ('pearson', 'spearman'), help="í”¼ì–´ìŠ¨: ì„ í˜• ê´€ê³„, ìŠ¤í”¼ì–´ë§Œ: ìˆœìœ„ ê¸°ë°˜ ë¹„ì„ í˜• ê´€ê³„")
        with col2:
            pval_threshold = st.slider("ìœ ì˜ìˆ˜ì¤€ (P-value) í•„í„°", 0.0, 1.0, 0.05, help="ì´ ê°’ë³´ë‹¤ í° p-valueë¥¼ ê°€ì§„ ìƒê´€ê´€ê³„ëŠ” ë¬´ì‹œí•©ë‹ˆë‹¤.")

        corr_matrix, pval_matrix = calculate_advanced_correlation(final_df, method=corr_method)
        
        st.subheader(f"'{corr_method.capitalize()}' ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ")
        fig_heatmap = go.Figure(data=go.Heatmap(
            z=corr_matrix.values,
            x=corr_matrix.columns,
            y=corr_matrix.columns,
            colorscale='RdBu_r', zmin=-1, zmax=1,
            text=corr_matrix.round(2).astype(str),
            texttemplate="%{text}"
        ))
        st.plotly_chart(fig_heatmap, use_container_width=True)

        with st.expander("ğŸ” íˆíŠ¸ë§µ ê²°ê³¼ í•´ì„í•˜ê¸°"):
            interpretation = interpret_correlation(corr_matrix, pval_matrix, threshold=pval_threshold)
            st.markdown(interpretation)

        st.markdown("---")
        st.subheader("ì‹œì°¨ êµì°¨ìƒê´€ ë¶„ì„")
        st.write("í•œ ë³€ìˆ˜ì˜ ë³€í™”ê°€ ë¯¸ë˜ì˜ ë‹¤ë¥¸ ë³€ìˆ˜ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤.")
        
        driver_cols = st.multiselect("ì„ í–‰ ë³€ìˆ˜ (Driver) ì„ íƒ", final_df.columns, default=[c for c in final_df if 'News' in c or 'Naver' in c])
        outcome_cols = st.multiselect("í›„í–‰ ë³€ìˆ˜ (Outcome) ì„ íƒ", final_df.columns, default=[c for c in final_df if 'ìˆ˜ì…' in c or 'ê°€ê²©' in c])

        if driver_cols and outcome_cols:
            lag_df = find_best_lagged_correlation(final_df, driver_cols, outcome_cols)
            st.dataframe(lag_df.head(10))

            if not lag_df.empty:
                top_lag = lag_df.iloc[0]
                st.info(f"ê°€ì¥ ê°•í•œ ì‹œì°¨ ê´€ê³„: **{top_lag['Driver (X)']}**ì˜ ë³€í™”ëŠ” **{top_lag['Best Lag (Weeks)']}ì£¼ í›„** **{top_lag['Outcome (Y)']}**ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤ (ìƒê´€ê³„ìˆ˜: {top_lag['Correlation']:.3f}).")
                with st.expander("ğŸ” ì‹œì°¨ ë¶„ì„ ê²°ê³¼ í•´ì„í•˜ê¸°"):
                    st.markdown(f"""
                    - **Driver (X):** ì›ì¸ì´ ë˜ëŠ” ì„ í–‰ ë³€ìˆ˜
                    - **Outcome (Y):** ì˜í–¥ì„ ë°›ëŠ” í›„í–‰ ë³€ìˆ˜
                    - **Best Lag (Weeks):** 'Driver'ê°€ ë³€í•œ ë’¤ 'Outcome'ì´ ë°˜ì‘í•˜ê¸°ê¹Œì§€ ê±¸ë¦¬ëŠ” í‰ê·  ì‹œê°„(ì£¼)
                    - **Correlation:** ë‘ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ ê°•ë„

                    ê°€ì¥ ìœ„ì— ìˆëŠ” **'{top_lag['Driver (X)']}'** ì§€í‘œëŠ” ë¯¸ë˜ì˜ **'{top_lag['Outcome (Y)']}'** ë³€í™”ë¥¼ ì•½ **{abs(top_lag['Best Lag (Weeks)'])}ì£¼** ë¨¼ì € ì•Œë ¤ì£¼ëŠ” ì„ í–‰ ì§€í‘œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                    """)
        st.markdown("---")
        st.subheader("ì‚°ì ë„ í–‰ë ¬")
        if len(final_df.columns) > 10:
            st.warning("ë³€ìˆ˜ê°€ 10ê°œ ì´ìƒì´ë©´ ì‚°ì ë„ í–‰ë ¬ ë Œë”ë§ì´ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            selected_dims = st.multiselect("ì‚°ì ë„ì— í‘œì‹œí•  ë³€ìˆ˜ ì„ íƒ", final_df.columns, default=list(final_df.columns[:5]))
        else:
            selected_dims = list(final_df.columns)
        
        if selected_dims:
            fig_scatter = px.scatter_matrix(final_df[selected_dims])
            st.plotly_chart(fig_scatter, use_container_width=True)

    with tab2:
        st.header("ì‹œê³„ì—´ ì˜ˆì¸¡ (Prophet & XGBoost)")
        
        prophet_df = final_df.reset_index().rename(columns={'ë‚ ì§œ': 'ds'})
        
        col1, col2 = st.columns(2)
        forecast_col = col1.selectbox("ì˜ˆì¸¡ ëŒ€ìƒ ë³€ìˆ˜ (y)", final_df.columns)
        forecast_periods = col2.number_input("ì˜ˆì¸¡ ê¸°ê°„ (ì£¼)", min_value=4, max_value=52, value=12)
        
        prophet_df = prophet_df.rename(columns={forecast_col: 'y'})
        regressors = [c for c in prophet_df.columns if c not in ['ds', 'y']]
        selected_regressors = st.multiselect("ì™¸ë¶€ ì˜ˆì¸¡ ë³€ìˆ˜ (Regressors)", regressors, default=regressors)
        
        st.subheader("Prophet ëª¨ë¸ íŒŒë¼ë¯¸í„° íŠœë‹")
        p_col1, p_col2, p_col3 = st.columns(3)
        changepoint_prior_scale = p_col1.slider("Trend ìœ ì—°ì„±", 0.01, 0.5, 0.05)
        seasonality_prior_scale = p_col2.slider("ê³„ì ˆì„± ê°•ë„", 0.01, 10.0, 1.0)
        seasonality_mode = p_col3.selectbox("ê³„ì ˆì„± ëª¨ë“œ", ('additive', 'multiplicative'))

        if st.button("ğŸš€ ì˜ˆì¸¡ ì‹¤í–‰", key="run_forecast"):
            with st.spinner("Prophet ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ì¤‘..."):
                m = Prophet(changepoint_prior_scale=changepoint_prior_scale, seasonality_prior_scale=seasonality_prior_scale, seasonality_mode=seasonality_mode)
                for reg in selected_regressors:
                    m.add_regressor(reg)
                
                m.fit(prophet_df[['ds', 'y'] + selected_regressors])
                future = m.make_future_dataframe(periods=forecast_periods, freq='W')
                
                future_regressors = prophet_df[['ds'] + selected_regressors].set_index('ds')
                last_values = future_regressors.iloc[-1]
                future_regressors = future_regressors.reindex(future['ds']).fillna(method='ffill').fillna(last_values)
                future_with_regs = pd.concat([future.set_index('ds'), future_regressors.drop(columns=['ds'], errors='ignore')], axis=1).reset_index()

                forecast = m.predict(future_with_regs)

            st.subheader("Prophet ì˜ˆì¸¡ ê²°ê³¼")
            fig_forecast = plot_plotly(m, forecast)
            st.plotly_chart(fig_forecast, use_container_width=True)
            with st.expander("ğŸ” Prophet ì˜ˆì¸¡ ê·¸ë˜í”„ í•´ì„í•˜ê¸°"):
                st.markdown("""
                - **ê²€ì€ ì :** ì‹¤ì œ ë°ì´í„°
                - **ì§„í•œ íŒŒë€ì„ :** ëª¨ë¸ ì˜ˆì¸¡ê°’
                - **ì—°í•œ íŒŒë€ ì˜ì—­:** ë¶ˆí™•ì‹¤ì„± êµ¬ê°„ (80% ì‹ ë¢°êµ¬ê°„)
                """)
            
            st.subheader("Prophet ìš”ì¸ ë¶„í•´")
            fig_components = plot_components_plotly(m, forecast)
            st.plotly_chart(fig_components, use_container_width=True)
            with st.expander("ğŸ” ìš”ì¸ ë¶„í•´ ê·¸ë˜í”„ í•´ì„í•˜ê¸°"):
                st.markdown("""
                - **trend:** ì¥ê¸°ì ì¸ ì¶”ì„¸
                - **yearly:** ì—°ê°„ ê³„ì ˆì„± íŒ¨í„´
                - **weekly:** ì£¼ê°„ ê³„ì ˆì„± íŒ¨í„´
                - **(ì™¸ë¶€ ë³€ìˆ˜):** ê° ì™¸ë¶€ ë³€ìˆ˜ê°€ ì˜ˆì¸¡ì— ë¯¸ì¹œ ì˜í–¥
                """)

            st.markdown("---")
            st.subheader("ëª¨ë¸ ì§„ë‹¨: ì”ì°¨ ë¶„ì„")
            
            df_pred = forecast.set_index('ds')[['yhat']].join(prophet_df.set_index('ds')[['y']]).dropna()
            residuals = df_pred['y'] - df_pred['yhat']
            
            diag_col1, diag_col2 = st.columns(2)
            with diag_col1:
                st.markdown("**ì”ì°¨ ì •ìƒì„± ê²€ì • (ADF Test)**")
                adf_result = adfuller(residuals)
                st.write(f"p-value: {adf_result[1]:.4f}")
                with st.expander("ğŸ” ADF í…ŒìŠ¤íŠ¸ ê²°ê³¼ í•´ì„"):
                    st.markdown(interpret_adf_test(adf_result))

            with diag_col2:
                st.markdown("**ì”ì°¨ ë¶„í¬**")
                fig_dist = ff.create_distplot([residuals], ['residuals'], bin_size=.2, show_rug=False)
                st.plotly_chart(fig_dist, use_container_width=True)

            st.markdown("---")
            st.subheader("ê³ ê¸‰ ì˜ˆì¸¡: XGBoost Meta-Forecasting")
            with st.spinner("XGBoost Meta-Model í•™ìŠµ ì¤‘..."):
                ml_df = forecast[['ds', 'trend', 'yearly', 'weekly']].set_index('ds').join(prophet_df.set_index('ds')).dropna()
                X = ml_df[['trend', 'yearly', 'weekly'] + selected_regressors]
                y = ml_df['y']
                
                train_size = int(len(X) * 0.85)
                X_train, X_test, y_train, y_test = X.iloc[:train_size], X.iloc[train_size:], y.iloc[:train_size], y.iloc[train_size:]
                
                xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=1000, learning_rate=0.01, early_stopping_rounds=50)
                xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
                
                y_pred_xgb = xgb_model.predict(X_test)
                r2, rmse = r2_score(y_test, y_pred_xgb), np.sqrt(mean_squared_error(y_test, y_pred_xgb))
                
                st.metric("XGBoost Test RÂ² Score", f"{r2:.3f}")
                st.metric("XGBoost Test RMSE", f"{rmse:.3f}")
                
                fig_xgb = go.Figure()
                fig_xgb.add_trace(go.Scatter(x=y_train.index, y=y_train, name='Train'))
                fig_xgb.add_trace(go.Scatter(x=y_test.index, y=y_test, name='Test (Actual)'))
                fig_xgb.add_trace(go.Scatter(x=y_test.index, y=y_pred_xgb, name='XGBoost Prediction'))
                st.plotly_chart(fig_xgb, use_container_width=True)

                feature_imp = pd.DataFrame(sorted(zip(xgb_model.feature_importances_, X.columns)), columns=['Value','Feature'])
                fig_imp = px.bar(feature_imp, x="Value", y="Feature", orientation='h', title="Feature Importance")
                st.plotly_chart(fig_imp, use_container_width=True)

                with st.expander("ğŸ” XGBoost ì¢…í•© ê²°ê³¼ í•´ì„"):
                    st.markdown(interpret_xgboost_results(r2, rmse, feature_imp, y_test.mean()))

    with tab3:
        st.header("í†µí•© ë°ì´í„° (ì£¼ë³„)")
        st.dataframe(final_df)
        st.download_button("CSVë¡œ ë‹¤ìš´ë¡œë“œ", final_df.to_csv(index=False).encode('utf-8-sig'), "integrated_weekly_data.csv")

else:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ë°ì´í„°ë¥¼ ì„ íƒí•˜ê³  'ëª¨ë“  ë°ì´í„° í†µí•© ë° ë¶„ì„ ì‹¤í–‰' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

