import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import requests
from datetime import datetime, timedelta
from functools import reduce
import json
import urllib.request
import yfinance as yf
from google.oauth2 import service_account
from google.cloud import bigquery
import pandas_gbq
from newspaper import Article, Config
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from statsmodels.tsa.seasonal import seasonal_decompose
from prophet import Prophet
from prophet.plot import plot_plotly
import feedparser
from urllib.parse import quote
import torch
from captum.attr import LayerIntegratedGradients, TokenReferenceBase

# --- BigQuery Connection ---
@st.cache_resource
def get_bq_connection():
    """BigQueryì— ì§ì ‘ ì—°ê²°í•˜ê³  í´ë¼ì´ì–¸íŠ¸ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        creds_dict = st.secrets["gcp_service_account"]
        creds = service_account.Credentials.from_service_account_info(creds_dict)
        client = bigquery.Client(credentials=creds, project=creds.project_id)
        return client
    except Exception as e:
        st.error(f"Google BigQuery ì—°ê²° ì‹¤íŒ¨: secrets.toml ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}")
        return None

# --- Sentiment Models ---
@st.cache_resource
def load_sentiment_assets():
    with st.spinner("í•œ/ì˜ ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì¤‘..."):
        return {
            'ko': {
                'model': AutoModelForSequenceClassification.from_pretrained("snunlp/KR-FinBERT-SC"),
                'tokenizer': AutoTokenizer.from_pretrained("snunlp/KR-FinBERT-SC", use_fast=True)
            },
            'en': {
                'model': AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert"),
                'tokenizer': AutoTokenizer.from_pretrained("ProsusAI/finbert", use_fast=True)
            }
        }

# --- Helper functions ---
def _softmax(x):
    e = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return e / e.sum(axis=-1, keepdims=True)

def predict_batch(texts, model, tokenizer, device=DEVICE, max_len=128):
    model.eval()
    enc = tokenizer(texts, return_tensors='pt', padding=True, truncation=True,
                    max_length=max_len, return_offsets_mapping=False)
    if device != "cpu":
        enc = {k: v.to(device) for k, v in enc.items()}
        model.to(device)
    with torch.no_grad():
        logits = model(**enc).logits
    probs = _softmax(logits.cpu().numpy())
    id2label = model.config.id2label
    labels = [id2label[i].lower() for i in range(logits.shape[-1])]
    return probs, labels

def compute_margin_and_pred(probs, labels):
    order = np.argsort(probs)[::-1]
    pred_idx = int(order[0])
    pred_prob = float(probs[pred_idx])
    second = float(probs[order[1]]) if len(order) > 1 else 0.0
    margin = pred_prob - second
    return pred_idx, pred_prob, margin

class ForwardWrapper(torch.nn.Module):
    def __init__(self, model): super().__init__(); self.model = model
    def forward(self, input_ids, attention_mask):
        return self.model(input_ids=input_ids, attention_mask=attention_mask).logits

def token_attributions_ig(text, model, tokenizer, target_idx,
                          max_len=128, n_steps=IG_N_STEPS):
    model.eval()
    enc = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length',
                    max_length=max_len, return_offsets_mapping=True)
    input_ids, attention_mask = enc['input_ids'], enc['attention_mask']
    offsets = enc['offset_mapping'][0].tolist()
    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0
    baselines = torch.full_like(input_ids, pad_id)
    fw = ForwardWrapper(model)
    lig = LayerIntegratedGradients(fw, model.get_input_embeddings())
    attributions, _ = lig.attribute(inputs=input_ids,
                                    baselines=baselines,
                                    additional_forward_args=(attention_mask,),
                                    target=target_idx,
                                    n_steps=n_steps)
    token_attr = attributions.sum(dim=-1).squeeze(0).cpu().numpy()
    token_attr = token_attr * attention_mask.squeeze(0).cpu().numpy()
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    keep_idx = [i for i, t in enumerate(tokens)
                if t not in (tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token)]
    words = []
    for i in keep_idx:
        s, e = offsets[i]
        word = text[s:e]
        words.append((word, float(token_attr[i])))
    if MECAB_AVAILABLE:
        merged = {}
        for w, sc in words:
            try: morphs = mecab.nouns(w) or [w]
            except: morphs = [w]
            for m in morphs: merged[m] = merged.get(m, 0.0) + sc
        words = list(merged.items())
    pos = sorted([x for x in words if x[1] > 0], key=lambda x: -x[1])
    neg = sorted([x for x in words if x[1] < 0], key=lambda x: x[1])
    return pos, neg

def classify_with_conditional_ig(texts, model, tokenizer,
                                batch_size=BATCH_SIZE, topk=TOPK_WORDS):
    results = []
    for i in range(0, len(texts), batch_size):
        chunk = texts[i:i+batch_size]
        probs_batch, labels = predict_batch(chunk, model, tokenizer)
        for j, text in enumerate(chunk):
            probs = probs_batch[j]
            pred_idx, pred_prob, margin = compute_margin_and_pred(probs, labels)
            pred_label = labels[pred_idx]
            run_ig = (pred_prob < IG_PROB_THRESH) or (margin < IG_MARGIN_THRESH)
            pos_words, neg_words = [], []
            if run_ig:
                try:
                    pos_words, neg_words = token_attributions_ig(text, model, tokenizer, pred_idx)
                except Exception as e:
                    logging.warning(f"IG ì‹¤íŒ¨: {e}")
            top_pos = [w for w,_ in pos_words[:topk]]
            top_neg = [w for w,_ in neg_words[:topk]]
            results.append({
                "text": text,
                "label": pred_label,
                "prob": float(pred_prob),
                "top_pos_words": top_pos,
                "top_neg_words": top_neg,
                "probs": {labels[k]: float(probs[k]) for k in range(len(labels))},
                "margin": float(margin)
            })
    return results

# --- News pipeline ---
def fetch_robust_news_data(client, keywords, models):
    config = Config()
    config.browser_user_agent = 'Mozilla/5.0'
    all_news, today = [], datetime.now()
    start_date = today - timedelta(days=14)
    for kw in keywords:
        st.write(f"â–¶ '{kw}' í‚¤ì›Œë“œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        for lang, rss_url in {
            'ko': f"https://news.google.com/rss/search?q={quote(kw)}&hl=ko&gl=KR&ceid=KR:ko",
            'en': f"https://news.google.com/rss/search?q={quote(kw)}&hl=en-US&gl=US&ceid=US:en-US"
        }.items():
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:30]:
                title = entry.get('title', '')
                if not title: continue
                try: pub_date = pd.to_datetime(entry.get('published'))
                except: pub_date = None
                if pub_date and start_date <= pub_date <= today:
                    all_news.append({"Date": pub_date.date(), "Title": title, "Keyword": kw, "Language": lang})
    if not all_news:
        return pd.DataFrame()
    df = pd.DataFrame(all_news).drop_duplicates(subset=["Title","Keyword","Language"])
    rows = []
    for lang in df["Language"].unique():
        subset = df[df["Language"]==lang].reset_index(drop=True)
        titles = subset["Title"].tolist()
        model, tok = models[lang]["model"], models[lang]["tokenizer"]
        results = classify_with_conditional_ig(titles, model, tok)
        for i,r in enumerate(results):
            posp = r["probs"].get("positive", r["probs"].get("pos",0))
            negp = r["probs"].get("negative", r["probs"].get("neg",0))
            score = posp - negp
            rows.append({
                "ë‚ ì§œ": subset.loc[i,"Date"],
                "Title": r["text"],
                "Label": r["label"],
                "Prob": r["prob"],
                "Top_Positive_Keywords": ", ".join(r["top_pos_words"]),
                "Top_Negative_Keywords": ", ".join(r["top_neg_words"]),
                "Keyword": subset.loc[i,"Keyword"],
                "Language": lang,
                "Sentiment": score,
                "InsertedAt": datetime.utcnow()
            })
    final_df = pd.DataFrame(rows)
    if client is not None and not final_df.empty:
        ensure_bq_table_schema(client, BQ_DATASET, BQ_TABLE_NEWS)
        pandas_gbq.to_gbq(final_df, f"{client.project}.{BQ_DATASET}.{BQ_TABLE_NEWS}",
                          project_id=client.project, if_exists="append",
                          credentials=client._credentials)
    return final_df

# --- Data Fetching & Processing Functions ---

@st.cache_data(ttl=3600)
def get_categories_from_bq(_client):
    """BigQueryì—ì„œ ë¶„ì„ ê°€ëŠ¥í•œ í’ˆëª© ì¹´í…Œê³ ë¦¬ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    project_id = _client.project
    table_id = f"{project_id}.data_explorer.tds_data"
    try:
        with st.spinner("BigQueryì—ì„œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
            query = f"SELECT DISTINCT Category FROM `{table_id}` WHERE Category IS NOT NULL ORDER BY Category"
            df = _client.query(query).to_dataframe()
        return sorted(df['Category'].astype(str).unique())
    except Exception as e:
        st.error(f"BigQuery í…Œì´ë¸”({table_id})ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
        return []

def get_trade_data_from_bq(client, categories):
    """ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ìˆ˜ì¶œì… ë°ì´í„°ë¥¼ BigQueryì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    if not categories: return pd.DataFrame()
    project_id = client.project
    table_id = f"{project_id}.data_explorer.tds_data"
    try:
        query_params = [bigquery.ArrayQueryParameter("categories", "STRING", categories)]
        sql = f"SELECT * FROM `{table_id}` WHERE Category IN UNNEST(@categories)"
        job_config = bigquery.QueryJobConfig(query_parameters=query_params)
        with st.spinner(f"BigQueryì—ì„œ {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬ ë°ì´í„° ë¡œë“œ ì¤‘..."):
            df = client.query(sql, job_config=job_config).to_dataframe()
        
        # ë°ì´í„° íƒ€ì… ì •ë¦¬
        for col in df.columns:
            if 'price' in col.lower() or 'value' in col.lower() or 'volume' in col.lower():
                df[col] = pd.to_numeric(df[col], errors='coerce')
        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
        
        return df
    except Exception as e:
        st.error(f"BigQueryì—ì„œ TDS ë°ì´í„° ì½ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

def deduplicate_and_write_to_bq(client, df_new, table_name, subset_cols=None):
    """ë°ì´í„°ë¥¼ BigQueryì— ì¤‘ë³µ ì œê±°í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."""
    project_id = client.project
    table_id = f"{project_id}.data_explorer.{table_name}"
    try:
        try:
            sql = f"SELECT * FROM `{table_id}`"
            df_existing = client.query(sql).to_dataframe()
        except Exception: 
            df_existing = pd.DataFrame()

        df_combined = pd.concat([df_existing, df_new], ignore_index=True)
        if subset_cols:
            df_deduplicated = df_combined.drop_duplicates(subset=subset_cols, keep='last')
        else:
            df_deduplicated = df_combined.drop_duplicates(keep='last')
        
        with st.spinner(f"'{table_name}' í…Œì´ë¸”ì— ë°ì´í„° ì €ì¥ ì¤‘..."):
            pandas_gbq.to_gbq(df_deduplicated, table_id, project_id=project_id, if_exists="replace", credentials=client._credentials)
        st.sidebar.success(f"'{table_name}'ì— ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
    except Exception as e: 
        st.error(f"BigQuery ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

def call_naver_api(url, body, naver_keys):
    """ë„¤ì´ë²„ APIë¥¼ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", naver_keys['id'])
        request.add_header("X-Naver-Client-Secret", naver_keys['secret'])
        request.add_header("Content-Type", "application/json")
        response = urllib.request.urlopen(request, data=body.encode("utf-8"))
        if response.getcode() == 200:
            return json.loads(response.read().decode('utf-8'))
        return None
    except urllib.error.HTTPError as e:
        error_body = e.read().decode('utf-8')
        st.error(f"Naver API ì˜¤ë¥˜ ë°œìƒ: {e.code} - {e.reason}")
        st.error(f"ì„œë²„ ì‘ë‹µ: {error_body}")
        return None
    except Exception as e:
        st.error(f"API í˜¸ì¶œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


@st.cache_data(ttl=3600)
def fetch_naver_trends_data(keywords, start_date, end_date, naver_keys):
    """
    ë„¤ì´ë²„ ë°ì´í„°ë©(ê²€ìƒ‰ì–´ íŠ¸ë Œë“œ, ì‡¼í•‘ì¸ì‚¬ì´íŠ¸) ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    í‚¤ì›Œë“œì— ë”°ë¼ ìë™ìœ¼ë¡œ ì‡¼í•‘ì¸ì‚¬ì´íŠ¸ ì¹´í…Œê³ ë¦¬ IDë¥¼ ë§¤ì¹­í•©ë‹ˆë‹¤.
    """
    if (end_date - start_date).days > 90:
        st.sidebar.error("ë„¤ì´ë²„ íŠ¸ë Œë“œ ì¡°íšŒ ê¸°ê°„ì€ ìµœëŒ€ 3ê°œì›”(90ì¼)ì…ë‹ˆë‹¤.")
        return pd.DataFrame()

    NAVER_SHOPPING_CAT_MAP = {
        'ì•„ë³´ì¹´ë„': "50000007", 'ë°”ë‚˜ë‚˜': "50000007", 'ì‚¬ê³¼': "50000007", 'ìˆ˜ì…ê³¼ì¼': "50000007",
        'ì»¤í”¼': "50000004", 'ì»¤í”¼ ìƒë‘': "50000004",
        'ìŒ€': "50000006", 'ê³ ë“±ì–´': "50000009"
    }
    all_data = []

    for keyword in keywords:
        keyword_dfs = []
        with st.spinner(f"'{keyword}' ë„¤ì´ë²„ íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
            if naver_keys['id'] and naver_keys['secret']:
                # 1. ë„¤ì´ë²„ ê²€ìƒ‰ì–´ íŠ¸ë Œë“œ API í˜¸ì¶œ (ì´ ë¶€ë¶„ì€ êµ¬ì¡°ê°€ ì›ë˜ ë§ì•˜ìŒ)
                body_search = json.dumps({
                    "startDate": start_date.strftime('%Y-%m-%d'), 
                    "endDate": end_date.strftime('%Y-%m-%d'), 
                    "timeUnit": "date", 
                    "keywordGroups": [{"groupName": keyword, "keywords": [keyword]}]
                })
                search_res = call_naver_api("https://openapi.naver.com/v1/datalab/search", body_search, naver_keys)
                if search_res and search_res.get('results'):
                    df_search = pd.DataFrame(search_res['results'][0]['data'])
                    keyword_dfs.append(df_search.rename(columns={'period': 'ë‚ ì§œ', 'ratio': f'NaverSearch_{keyword}'}))

                # 2. ë„¤ì´ë²„ ì‡¼í•‘ì¸ì‚¬ì´íŠ¸ API í˜¸ì¶œ
                lower_keyword = keyword.lower().replace(' ', '')
                if lower_keyword in NAVER_SHOPPING_CAT_MAP:
                    category_id = NAVER_SHOPPING_CAT_MAP[lower_keyword]
                    
                    # [ìˆ˜ì •] API ë¬¸ì„œì— ë§ëŠ” ë°°ì—´/ê°ì²´ í˜•íƒœë¡œ êµ¬ì¡° ë³€ê²½
                    body_shop = json.dumps({
                        "startDate": start_date.strftime('%Y-%m-%d'),
                        "endDate": end_date.strftime('%Y-%m-%d'),
                        "timeUnit": "date",
                        "category": [{"name": keyword, "param": [category_id]}],
                        "keyword": [{"name": keyword, "param": [keyword]}]
                    })
                    
                    shop_res = call_naver_api("https://openapi.naver.com/v1/datalab/shopping/categories", body_shop, naver_keys)
                    if shop_res and shop_res.get('results'):
                        df_shop = pd.DataFrame(shop_res['results'][0]['data'])
                        keyword_dfs.append(df_shop.rename(columns={'period': 'ë‚ ì§œ', 'ratio': f'NaverShop_{keyword}'}))
                else:
                    st.sidebar.info(f"'{keyword}'ì— ëŒ€í•œ ì‡¼í•‘ì¸ì‚¬ì´íŠ¸ ì¹´í…Œê³ ë¦¬ ì •ë³´ê°€ ì—†ì–´ ê²€ìƒ‰ íŠ¸ë Œë“œë§Œ ì¡°íšŒí•©ë‹ˆë‹¤.")

            if keyword_dfs:
                for df in keyword_dfs:
                    df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
                merged_df = reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), keyword_dfs)
                all_data.append(merged_df)

    if not all_data: 
        return pd.DataFrame()
        
    return reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), all_data)

def fetch_kamis_data(client, item_info, start_date, end_date, kamis_keys):
    """KAMISì—ì„œ ê¸°ê°„ë³„ ë„ë§¤ ê°€ê²© ë°ì´í„°ë¥¼ í•œ ë²ˆì˜ API í˜¸ì¶œë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    start_str, end_str = start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')
    item_code, kind_code = item_info['item_code'], item_info['kind_code']

    with st.spinner("KAMISì—ì„œ ê¸°ê°„ ë°ì´í„° ì¡°íšŒ ì¤‘..."):
        url = (
            "http://www.kamis.or.kr/service/price/xml.do?action=periodWholesaleProductList"
            f"&p_product_cls_code=01&p_startday={start_str}&p_endday={end_str}"
            f"&p_item_category_code={item_info['cat_code']}&p_item_code={item_code}&p_kind_code={kind_code}"
            f"&p_product_rank_code={item_info['rank_code']}&p_convert_kg_yn=Y"
            f"&p_cert_key={kamis_keys['key']}&p_cert_id={kamis_keys['id']}&p_returntype=json"
        )
        try:
            response = requests.get(url, timeout=20)
            if response.status_code == 200:
                data = response.json()
                if data and "data" in data and data["data"] and "item" in data["data"]:
                    price_data = data["data"]["item"]
                    if not price_data:
                        st.sidebar.warning("í•´ë‹¹ ê¸°ê°„ì— ëŒ€í•œ KAMIS ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        return pd.DataFrame()
                    
                    df_new = pd.DataFrame(price_data).rename(columns={'regday': 'ë‚ ì§œ', 'price': 'ë„ë§¤ê°€ê²©_ì›'})
                    df_new = df_new[['ë‚ ì§œ', 'ë„ë§¤ê°€ê²©_ì›']]
                    
                    def format_date(date_str):
                        return f"{start_date.year}-{date_str}" if len(date_str) <= 5 else date_str
                    
                    df_new['ë‚ ì§œ'] = pd.to_datetime(df_new['ë‚ ì§œ'].apply(format_date))
                    df_new['ë„ë§¤ê°€ê²©_ì›'] = pd.to_numeric(df_new['ë„ë§¤ê°€ê²©_ì›'].str.replace(',', ''), errors='coerce')
                    return df_new
                else:
                    st.sidebar.warning("í•´ë‹¹ ê¸°ê°„ì— ëŒ€í•œ KAMIS ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.sidebar.error(f"KAMIS ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: Status {response.status_code}")

        except Exception as e:
            st.sidebar.error(f"KAMIS API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

    return pd.DataFrame()

def fetch_robust_news_data(client, keywords, models):
    """ì•ˆì •ì„±ì„ ë†’ì¸ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ í•¨ìˆ˜ (RSS ê¸°ë°˜)"""
    config = Config()
    config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'
    
    all_news_data, today = [], datetime.now()
    start_date = today - timedelta(days=14)

    for keyword in keywords:
        st.write(f"â–¶ '{keyword}' í‚¤ì›Œë“œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        urls = {
            'ko': f"https://news.google.com/rss/search?q={quote(keyword)}&hl=ko&gl=KR&ceid=KR:ko",
            'en': f"https://news.google.com/rss/search?q={quote(keyword)}&hl=en-US&gl=US&ceid=US:en-US"
        }
        for lang, rss_url in urls.items():
            try:
                feed = feedparser.parse(rss_url)
                if not feed.entries:
                    st.warning(f"'{keyword}'({lang}) RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    continue
                
                model, tokenizer = models[lang]['model'], models[lang]['tokenizer']
                for entry in feed.entries[:20]:
                    try:
                        article = Article(entry.link, config=config, language=lang)
                        article.download(); article.parse()

                        if not article.publish_date: continue
                        pub_date = article.publish_date.replace(tzinfo=None)

                        if start_date <= pub_date <= today:
                            title = article.title
                            pipe = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
                            analysis = pipe(title)[0]
                            label, score = analysis['label'], analysis['score']
                            sentiment_score = score if label.lower() in ['positive', '5 stars'] else -score
                            
                            all_news_data.append({'Date': pub_date.date(), 'Title': title, 'Sentiment': sentiment_score, 'Keyword': keyword, 'Language': lang})
                    except Exception as e:
                        st.warning(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {entry.link} - ì›ì¸: {str(e)[:100]}")
            except Exception as e:
                st.error(f"RSS í”¼ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {rss_url} - ì›ì¸: {e}")

    if not all_news_data:
        st.sidebar.error("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")
        return pd.DataFrame()

    final_df = pd.DataFrame(all_news_data)
    deduplicate_and_write_to_bq(client, final_df, "news_sentiment_cache", subset_cols=['Title', 'Keyword', 'Language'])
    final_df['Date'] = pd.to_datetime(final_df['Date'])
    return final_df.rename(columns={'Date': 'ë‚ ì§œ'})

# --- Constants & App ---
KAMIS_FULL_DATA = {
    'ìŒ€': {'cat_code': '100', 'item_code': '111', 'kinds': {'20kg': '01', 'ë°±ë¯¸': '02'}},
    'ê°ì': {'cat_code': '100', 'item_code': '152', 'kinds': {'ìˆ˜ë¯¸(ë…¸ì§€)': '01', 'ìˆ˜ë¯¸(ì‹œì„¤)': '04'}},
    'ë°°ì¶”': {'cat_code': '200', 'item_code': '211', 'kinds': {'ë´„': '01', 'ì—¬ë¦„': '02', 'ê°€ì„': '03'}},
    'ì–‘íŒŒ': {'cat_code': '200', 'item_code': '245', 'kinds': {'ì–‘íŒŒ': '00', 'í–‡ì–‘íŒŒ': '02'}},
    'ì‚¬ê³¼': {'cat_code': '400', 'item_code': '411', 'kinds': {'í›„ì§€': '05', 'ì•„ì˜¤ë¦¬': '06'}},
    'ë°”ë‚˜ë‚˜': {'cat_code': '400', 'item_code': '418', 'kinds': {'ìˆ˜ì…': '02'}},
    'ì•„ë³´ì¹´ë„': {'cat_code': '400', 'item_code': '430', 'kinds': {'ìˆ˜ì…': '00'}},
    'ê³ ë“±ì–´': {'cat_code': '600', 'item_code': '611', 'kinds': {'ìƒì„ ': '01', 'ëƒ‰ë™': '02'}},
}
st.set_page_config(layout="wide")
st.title("ğŸ“Š ë°ì´í„° íƒìƒ‰ ë° í†µí•© ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

bq_client = get_bq_connection()
sentiment_assets = load_sentiment_assets()
if bq_client is None: st.stop()

# --- Sidebar ---
st.sidebar.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
if 'data_loaded' not in st.session_state: st.session_state.data_loaded = False
if 'categories' not in st.session_state: st.session_state.categories = get_categories_from_bq(bq_client)

st.sidebar.subheader("1. ë¶„ì„ ëŒ€ìƒ ì„¤ì •")
if not st.session_state.categories:
    st.sidebar.warning("BigQueryì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ë°ì´í„°ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
else:
    selected_categories = st.sidebar.multiselect("ë¶„ì„í•  í’ˆëª© ì¹´í…Œê³ ë¦¬ ì„ íƒ", st.session_state.categories, default=st.session_state.get('selected_categories', []))
    if st.sidebar.button("ğŸš€ ì„ íƒ ì™„ë£Œ ë° ë¶„ì„ ì‹œì‘"):
        if not selected_categories:
            st.sidebar.warning("ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        else:
            st.session_state.raw_trade_df = get_trade_data_from_bq(bq_client, selected_categories)
            if st.session_state.raw_trade_df is not None and not st.session_state.raw_trade_df.empty:
                st.session_state.data_loaded = True
                st.session_state.selected_categories = selected_categories
            else:
                st.sidebar.error("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if not st.session_state.data_loaded:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ê³  'ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    st.stop()

# --- Main App Logic ---
raw_trade_df = st.session_state.raw_trade_df
selected_categories = st.session_state.selected_categories
st.sidebar.success(f"**{', '.join(selected_categories)}** ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
st.sidebar.markdown("---")

try:
    raw_trade_df.dropna(subset=['Date'], inplace=True)
    file_start_date, file_end_date = raw_trade_df['Date'].min(), raw_trade_df['Date'].max()
    
    st.sidebar.subheader("2. ë¶„ì„ ê¸°ê°„ ë° í‚¤ì›Œë“œ ì„¤ì •")
    start_date = pd.to_datetime(st.sidebar.date_input('ì‹œì‘ì¼', file_start_date, min_value=file_start_date, max_value=file_end_date))
    end_date = pd.to_datetime(st.sidebar.date_input('ì¢…ë£Œì¼', file_end_date, min_value=start_date, max_value=file_end_date))
    
    default_keywords = ", ".join(selected_categories)
    keyword_input = st.sidebar.text_input("ë‰´ìŠ¤/íŠ¸ë Œë“œ ë¶„ì„ í‚¤ì›Œë“œ", default_keywords)
    search_keywords = [k.strip() for k in keyword_input.split(',') if k.strip()]
except Exception as e:
    st.error(f"ë°ì´í„° ê¸°ê°„ ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}")
    st.stop()

st.sidebar.subheader("3. ì™¸ë¶€ ë°ì´í„° ì—°ë™")

st.sidebar.markdown("##### KAMIS ë†ì‚°ë¬¼ ê°€ê²©")
kamis_api_key = st.sidebar.text_input("KAMIS API Key", type="password", help="KAMIS ê³µê³µë°ì´í„° í¬í„¸ì—ì„œ ë°œê¸‰ë°›ì€ ì¸ì¦í‚¤")
kamis_api_id = st.sidebar.text_input("KAMIS API ID", type="password", help="KAMIS API ì‹ ì²­ ì‹œ ë“±ë¡í•œ ID")
item_name = st.sidebar.selectbox("í’ˆëª© ì„ íƒ", list(KAMIS_FULL_DATA.keys()))
if item_name:
    kind_name = st.sidebar.selectbox("í’ˆì¢… ì„ íƒ", list(KAMIS_FULL_DATA[item_name]['kinds'].keys()))
    if st.sidebar.button("KAMIS ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
        if kamis_api_key and kamis_api_id:
            item_info = {**KAMIS_FULL_DATA[item_name], 'item_name': item_name, 'kind_name': kind_name, 'rank_code': '01'}
            item_info['kind_code'] = KAMIS_FULL_DATA[item_name]['kinds'][kind_name]
            st.session_state.wholesale_data = fetch_kamis_data(bq_client, item_info, start_date, end_date, {'key': kamis_api_key, 'id': kamis_api_id})
        else:
            st.sidebar.error("KAMIS API Keyì™€ IDë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")

st.sidebar.markdown("##### ë„¤ì´ë²„ íŠ¸ë Œë“œ ë°ì´í„°")
naver_client_id = st.sidebar.text_input("Naver API Client ID", type="password")
naver_client_secret = st.sidebar.text_input("Naver API Client Secret", type="password")
if st.sidebar.button("ë„¤ì´ë²„ íŠ¸ë Œë“œ ê°€ì ¸ì˜¤ê¸°"):
    if not search_keywords: st.sidebar.warning("ë¶„ì„í•  í‚¤ì›Œë“œë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        st.session_state.search_data = fetch_naver_trends_data(search_keywords, start_date, end_date, {'id': naver_client_id, 'secret': naver_client_secret})

st.sidebar.markdown("##### ë‰´ìŠ¤ ê°ì„± ë¶„ì„")
if st.sidebar.button("ìµœì‹  ë‰´ìŠ¤ ë¶„ì„í•˜ê¸°"):
    if not search_keywords: st.sidebar.warning("ë¶„ì„í•  í‚¤ì›Œë“œë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        st.session_state.news_data = fetch_robust_news_data(bq_client, search_keywords, sentiment_assets)

# --- Main Display Tabs ---
raw_wholesale_df = st.session_state.get('wholesale_data', pd.DataFrame())
raw_search_df = st.session_state.get('search_data', pd.DataFrame())
raw_news_df = st.session_state.get('news_data', pd.DataFrame())

tab1, tab2, tab3, tab4, tab5 = st.tabs(["1ï¸âƒ£ ì›ë³¸ ë°ì´í„°", "2ï¸âƒ£ ë°ì´í„° í‘œì¤€í™”", "3ï¸âƒ£ ë‰´ìŠ¤ ê°ì„± ë¶„ì„", "4ï¸âƒ£ ìƒê´€ê´€ê³„ ë¶„ì„", "ğŸ“ˆ ì‹œê³„ì—´ ì˜ˆì¸¡"])

with tab1:
    st.subheader("A. ìˆ˜ì¶œì… ë°ì´í„° (ì„ íƒ ê¸°ê°„)"); st.dataframe(raw_trade_df[(raw_trade_df['Date'] >= start_date) & (raw_trade_df['Date'] <= end_date)])
    st.subheader("B. ì™¸ë¶€ ê°€ê²© ë°ì´í„°"); st.dataframe(raw_wholesale_df)
    st.subheader("C. íŠ¸ë Œë“œ ë°ì´í„°"); st.dataframe(raw_search_df)
    st.subheader("D. ë‰´ìŠ¤ ë°ì´í„°"); st.dataframe(raw_news_df)
    
with tab2:
    st.header("ë°ì´í„° í‘œì¤€í™”: ì£¼ë³„(Weekly) ë°ì´í„°ë¡œ ë³€í™˜")
    trade_df_in_range = raw_trade_df[(raw_trade_df['Date'] >= start_date) & (raw_trade_df['Date'] <= end_date)]
    filtered_trade_df = trade_df_in_range[trade_df_in_range['Category'].isin(selected_categories)].copy()
    
    if not filtered_trade_df.empty:
        filtered_trade_df.set_index('Date', inplace=True)
        trade_weekly = filtered_trade_df.resample('W-Mon').agg(ìˆ˜ì…ì•¡_USD=('Value', 'sum'), ìˆ˜ì…ëŸ‰_KG=('Volume', 'sum')).copy()
        trade_weekly['ìˆ˜ì…ë‹¨ê°€_USD_KG'] = trade_weekly['ìˆ˜ì…ì•¡_USD'] / trade_weekly['ìˆ˜ì…ëŸ‰_KG']
        trade_weekly.index.name = 'ë‚ ì§œ'
        
        dfs_to_process = {
            'wholesale': raw_wholesale_df,
            'search': raw_search_df,
            'news': raw_news_df
        }
        weekly_dfs = {'trade': trade_weekly}

        for name, df in dfs_to_process.items():
            if not df.empty and 'ë‚ ì§œ' in df.columns:
                df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
                df_in_range = df[(df['ë‚ ì§œ'] >= start_date) & (df['ë‚ ì§œ'] <= end_date)]
                if not df_in_range.empty:
                    df_weekly = df_in_range.set_index('ë‚ ì§œ').resample('W-Mon').mean(numeric_only=True)
                    df_weekly.index.name = 'ë‚ ì§œ'
                    weekly_dfs[name] = df_weekly

        st.session_state['weekly_dfs'] = weekly_dfs
        st.write("### ì£¼ë³„ ì§‘ê³„ ë°ì´í„° ìƒ˜í”Œ")
        for name, df in weekly_dfs.items():
            st.write(f"##### {name.capitalize()} Data (Weekly)")
            st.dataframe(df.head())
    else:
        st.warning("ì„ íƒëœ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ìˆ˜ì¶œì… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

with tab3:
    st.header("ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ê²°ê³¼")
    if not raw_news_df.empty:
        news_weekly = st.session_state.get('weekly_dfs', {}).get('news', pd.DataFrame())
        if not news_weekly.empty:
            fig = px.line(news_weekly, y='Sentiment', title="ì£¼ë³„ í‰ê·  ë‰´ìŠ¤ ê°ì„± ì ìˆ˜", labels={'Sentiment': 'ê°ì„± ì ìˆ˜'})
            fig.add_hline(y=0, line_dash="dash", line_color="red")
            st.plotly_chart(fig, use_container_width=True)

        st.markdown("---")
        st.subheader("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ (ìµœì‹ ìˆœ)")
        st.dataframe(raw_news_df.sort_values(by='ë‚ ì§œ', ascending=False))
    else:
        st.info("ì‚¬ì´ë“œë°”ì—ì„œ 'ìµœì‹  ë‰´ìŠ¤ ë¶„ì„í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

with tab4:
    st.header("ìƒê´€ê´€ê³„ ë¶„ì„")
    weekly_dfs = st.session_state.get('weekly_dfs', {})
    if weekly_dfs:
        dfs_to_concat = [df for df in weekly_dfs.values() if not df.empty]
        if dfs_to_concat:
            final_df = reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), dfs_to_concat)
            final_df = final_df.interpolate(method='linear', limit_direction='both').dropna(how='all')
            st.session_state['final_df'] = final_df
            
            st.subheader("í†µí•© ë°ì´í„° ì‹œê°í™”")
            df_long = final_df.reset_index().melt(id_vars='ë‚ ì§œ', var_name='ë°ì´í„° ì¢…ë¥˜', value_name='ê°’')
            fig = px.line(df_long, x='ë‚ ì§œ', y='ê°’', color='ë°ì´í„° ì¢…ë¥˜', title="í†µí•© ë°ì´í„° ì‹œê³„ì—´ ì¶”ì´")
            st.plotly_chart(fig, use_container_width=True)

            if len(final_df.columns) > 1:
                st.markdown("---")
                st.subheader("ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ")
                corr_matrix = final_df.corr(numeric_only=True)
                fig_heatmap = px.imshow(corr_matrix, text_auto=True, aspect="auto", color_continuous_scale='RdBu_r', range_color=[-1, 1])
                st.plotly_chart(fig_heatmap, use_container_width=True)
        else:
            st.warning("ë¶„ì„í•  ì£¼ë³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("2ë‹¨ê³„ì—ì„œ ë°ì´í„°ê°€ ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

with tab5:
    st.header("ì‹œê³„ì—´ ë¶„í•´ ë° ì˜ˆì¸¡ (by Prophet)")
    final_df = st.session_state.get('final_df', pd.DataFrame())
    if not final_df.empty:
        forecast_col = st.selectbox("ì˜ˆì¸¡ ëŒ€ìƒ ë³€ìˆ˜ ì„ íƒ", final_df.columns)
        if forecast_col:
            ts_data = final_df[[forecast_col]].dropna()
            if len(ts_data) < 24:
                st.warning(f"ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•´ ìµœì†Œ 24ì£¼ ì´ìƒì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬ ë°ì´í„°: {len(ts_data)}ì£¼")
            else:
                st.subheader(f"'{forecast_col}' ì‹œê³„ì—´ ë¶„í•´")
                period = 52 if len(ts_data) >= 104 else int(len(ts_data) / 2)
                decomposition = seasonal_decompose(ts_data[forecast_col], model='additive', period=period)
                
                fig_decompose = go.Figure()
                fig_decompose.add_trace(go.Scatter(x=decomposition.observed.index, y=decomposition.observed, mode='lines', name='Observed'))
                fig_decompose.add_trace(go.Scatter(x=decomposition.trend.index, y=decomposition.trend, mode='lines', name='Trend'))
                fig_decompose.add_trace(go.Scatter(x=decomposition.seasonal.index, y=decomposition.seasonal, mode='lines', name='Seasonal'))
                st.plotly_chart(fig_decompose, use_container_width=True)

                st.subheader(f"'{forecast_col}' ë¯¸ë˜ 12ì£¼ ì˜ˆì¸¡")
                prophet_df = ts_data.reset_index().rename(columns={'ë‚ ì§œ': 'ds', forecast_col: 'y'})
                
                m = Prophet()
                m.fit(prophet_df)
                future = m.make_future_dataframe(periods=12, freq='W')
                forecast = m.predict(future)

                fig_forecast = plot_plotly(m, forecast)
                fig_forecast.update_layout(title=f"'{forecast_col}' ë¯¸ë˜ ì˜ˆì¸¡", xaxis_title='ë‚ ì§œ', yaxis_title='ì˜ˆì¸¡ê°’')
                st.plotly_chart(fig_forecast, use_container_width=True)
                
                st.write("#### ì˜ˆì¸¡ ë°ì´í„° í…Œì´ë¸”")
                st.dataframe(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(12))
    else:
        st.info("4ë²ˆ íƒ­ì—ì„œ ë°ì´í„°ê°€ í†µí•©ë˜ì–´ì•¼ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
