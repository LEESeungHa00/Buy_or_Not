import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import requests
from datetime import datetime, timedelta
from functools import reduce
from pytrends.request import TrendReq
import json
import urllib.request
import yfinance as yf
from google.oauth2 import service_account
from google.cloud import bigquery
import pandas_gbq
from newspaper import build, Article
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from statsmodels.tsa.seasonal import seasonal_decompose
from prophet import Prophet
from prophet.plot import plot_plotly
import feedparser
from urllib.parse import quote
import torch
from captum.attr import LayerIntegratedGradients, TokenReferenceBase

# --- BigQuery Connection (Manual Method for Stability) ---
@st.cache_resource
def get_bq_connection():
    """BigQueryì— ì§ì ‘ ì—°ê²°í•˜ê³  í´ë¼ì´ì–¸íŠ¸ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        creds_dict = st.secrets["gcp_service_account"]
        creds = service_account.Credentials.from_service_account_info(creds_dict)
        client = bigquery.Client(credentials=creds, project=creds.project_id)
        return client
    except Exception as e:
        st.error(f"Google BigQuery ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. secrets.tomlì˜ [gcp_service_account] ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”: {e}")
        return None

# --- Sentiment Analysis Model & Explainer ---
@st.cache_resource
def load_sentiment_assets():
    """í•œ/ì˜ ê°ì„± ë¶„ì„ ëª¨ë¸ ë° ì„¤ëª… ë„êµ¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    with st.spinner("í•œ/ì˜ ê°ì„± ë¶„ì„ AI ëª¨ë¸ ë° ì„¤ëª… ë„êµ¬ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘..."):
        assets = {
            'ko': {
                'model': AutoModelForSequenceClassification.from_pretrained("snunlp/KR-FinBERT-SC"),
                'tokenizer': AutoTokenizer.from_pretrained("snunlp/KR-FinBERT-SC")
            },
            'en': {
                'model': AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english"),
                'tokenizer': AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
            }
        }
    return assets

def explain_sentiment(model, tokenizer, text, lang):
    """LayerIntegratedGradientsë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ ê¸°ì—¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)
    token_reference = TokenReferenceBase(reference_token_idx=tokenizer.pad_token_id)
    embeddings = model.base_model.embeddings if hasattr(model.base_model, 'embeddings') else model.distilbert.embeddings
    lig = LayerIntegratedGradients(model, embeddings)
    target_id = model.config.label2id.get('positive', 1) if lang == 'ko' else model.config.label2id.get('POSITIVE', 1)
    attributions, _ = lig.attribute(inputs['input_ids'], reference_inputs=token_reference.generate_reference(len(inputs['input_ids'][0]), device='cpu'), target=target_id, return_convergence_delta=True)
    attributions = attributions.sum(dim=-1).squeeze(0) / torch.norm(attributions)
    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
    word_attributions = [(token.replace('##', ''), attr.item()) for token, attr in zip(tokens, attributions) if token not in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]]
    return json.dumps(word_attributions, ensure_ascii=False)

# --- Data Fetching & Processing Functions ---
@st.cache_data(ttl=3600)
def get_categories_from_bq(_client):
    project_id = _client.project; table_id = f"{project_id}.data_explorer.tds_data"
    try:
        with st.spinner("BigQueryì—ì„œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
            query = f"SELECT DISTINCT Category FROM {table_id} WHERE Category IS NOT NULL ORDER BY Category"
            df = _client.query(query).to_dataframe()
        return sorted(df['Category'].astype(str).unique())
    except Exception as e:
        st.error(f"BigQuery í…Œì´ë¸”({table_id})ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ì„¸íŠ¸/í…Œì´ë¸” ì´ë¦„, ì„œë¹„ìŠ¤ ê³„ì • ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”. ì›ë³¸ ì˜¤ë¥˜: {e}")
        return []

def get_trade_data_from_bq(client, categories):
    if not categories: return pd.DataFrame()
    project_id = client.project; table_id = f"{project_id}.data_explorer.tds_data"
    try:
        query_params = [bigquery.ArrayQueryParameter("categories", "STRING", categories)]
        sql = f"SELECT * FROM {table_id} WHERE Category IN UNNEST(@categories)"
        job_config = bigquery.QueryJobConfig(query_parameters=query_params)
        with st.spinner(f"BigQueryì—ì„œ {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘..."):
            df = client.query(sql, job_config=job_config).to_dataframe()
        
        # [ìˆ˜ì •] ë°ì´í„° íƒ€ì… ë³€í™˜ì„ ë” ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬
        for col in df.columns:
            if 'price' in col.lower() or 'value' in col.lower() or 'volume' in col.lower():
                df[col] = pd.to_numeric(df[col], errors='coerce')
        # [ìˆ˜ì •] 'Date' ì»¬ëŸ¼ì´ ìµœìš°ì„ , ì—†ë‹¤ë©´ ë‹¤ë¥¸ ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì•„ ë³€í™˜
        date_col_found = None
        if 'Date' in df.columns:
            date_col_found = 'Date'
        elif 'date' in df.columns:
            date_col_found = 'date'
        
        if date_col_found:
             df[date_col_found] = pd.to_datetime(df[date_col_found], errors='coerce')
             df.rename(columns={date_col_found: 'Date'}, inplace=True) # ì»¬ëŸ¼ëª…ì„ 'Date'ë¡œ í†µì¼
        
        return df
    except Exception as e:
        st.error(f"BigQueryì—ì„œ TDS ë°ì´í„°ë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"); return pd.DataFrame()

def deduplicate_and_write_to_bq(client, df_new, table_name, subset_cols=None):
    project_id = client.project; table_id = f"{project_id}.data_explorer.{table_name}"
    try:
        try:
            sql = f"SELECT * FROM {table_id}"
            df_existing = client.query(sql).to_dataframe()
        except Exception: df_existing = pd.DataFrame()
        df_combined = pd.concat([df_existing, df_new], ignore_index=True)
        if subset_cols:
            df_deduplicated = df_combined.drop_duplicates(subset=subset_cols)
        else:
            df_deduplicated = df_combined.drop_duplicates()
        with st.spinner(f"ì¤‘ë³µì„ ì œê±°í•œ ë°ì´í„°ë¥¼ BigQuery '{table_name}'ì— ì €ì¥í•˜ëŠ” ì¤‘..."):
            pandas_gbq.to_gbq(df_deduplicated, table_id, project_id=project_id, if_exists="replace", credentials=client._credentials)
        st.sidebar.success(f"ë°ì´í„°ê°€ BigQuery '{table_name}'ì— ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e: st.error(f"BigQuery ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def add_trade_data_to_bq(client, df):
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = ['_'.join(map(str, col)).strip() for col in df.columns.values]
    df.columns = df.columns.str.replace(' ', '_').str.replace('[^A-Za-z0-9_]', '', regex=True)
    deduplicate_and_write_to_bq(client, df, "tds_data")

def fetch_and_analyze_news_base(client, keywords, start_date, end_date, models, article_source_func):
    project_id = client.project; table_name = "news_sentiment_cache"
    all_news_data = []
    for keyword in keywords:
        for lang, country in [('ko', 'KR'), ('en', 'US')]:
            with st.spinner(f"'{keyword}' ({lang}) ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„ ì¤‘..."):
                articles_to_process = article_source_func(keyword, lang, country)
                keyword_articles = []
                model, tokenizer = models[lang]['model'], models[lang]['tokenizer']
                for article in articles_to_process:
                    try:
                        article.download(); article.parse()
                        pub_date = article.publish_date
                        if pub_date and start_date <= pd.to_datetime(pub_date).tz_localize(None) <= end_date:
                            title = article.title[:256]
                            pipe = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
                            analysis = pipe(title)[0]
                            label, score = analysis['label'], analysis['score']
                            sentiment_score = score if label.lower() in ['positive', '5 stars'] else -score if label.lower() in ['negative', '1 star'] else 0.0
                            explanation_json = explain_sentiment(model, tokenizer, title, lang)
                            keyword_articles.append({'Date': pub_date.date(), 'Title': title, 'Sentiment': sentiment_score, 'Keyword': keyword, 'Language': lang, 'Explanation': explanation_json})
                    except Exception: continue
                if keyword_articles: all_news_data.append(pd.DataFrame(keyword_articles))
    if not all_news_data: st.sidebar.warning("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."); return pd.DataFrame()
    final_df = pd.concat(all_news_data, ignore_index=True)
    deduplicate_and_write_to_bq(client, final_df, "news_sentiment_cache", subset_cols=['Title', 'Keyword', 'Language'])
    final_df['Date'] = pd.to_datetime(final_df['Date'])
    return final_df

def fetch_historical_news(client, keywords, start_date, end_date, models):
    def get_articles(keyword, lang, country):
        keyword_encoded = quote(keyword); news_url = f"https://news.google.com/search?q={keyword_encoded}&hl={lang}&gl={country}&ceid={country}%3A{lang}"
        return build(news_url, memoize_articles=False, language=lang).articles[:50]
    return fetch_and_analyze_news_base(client, keywords, start_date, end_date, models, get_articles)

def fetch_latest_news_rss(client, keywords, models):
    def get_articles(keyword, lang, country):
        keyword_encoded = quote(keyword); rss_url = f"https://news.google.com/rss/search?q={keyword_encoded}&hl={lang}&gl={country}&ceid={country}:{lang}"
        feed = feedparser.parse(rss_url)
        return [Article(entry.link) for entry in feed.entries[:25]]
    return fetch_and_analyze_news_base(client, keywords, datetime.now() - timedelta(days=7), datetime.now(), models, get_articles)

def fetch_yfinance_data(tickers, start_date, end_date):
    all_data = []
    for name, ticker in tickers.items():
        data = yf.download(ticker, start=start_date, end=end_date, progress=False)
        if not data.empty:
            df = data[['Close']].copy().reset_index().rename(columns={'Date': 'ë‚ ì§œ', 'Close': f'{name}_ì„ ë¬¼ê°€ê²©_USD'})
            all_data.append(df)
    if not all_data: return pd.DataFrame()
    # [ìˆ˜ì •] ë‚ ì§œ ì»¬ëŸ¼ ì´ë¦„ì„ 'ë‚ ì§œ'ë¡œ í†µì¼í•˜ì—¬ merge
    return reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), all_data)

def call_naver_api(url, body, naver_keys):
    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", naver_keys['id']); request.add_header("X-Naver-Client-Secret", naver_keys['secret']); request.add_header("Content-Type", "application/json")
    response = urllib.request.urlopen(request, data=body.encode("utf-8"))
    if response.getcode() == 200:
        return json.loads(response.read().decode('utf-8'))
    return None

def fetch_trends_data(keywords, start_date, end_date, naver_keys):
    all_data = []
    NAVER_SHOPPING_CAT_MAP = {'ì»¤í”¼ ìƒë‘(Green Bean)': "50004457", 'ì•„ë³´ì¹´ë„(ì—´ëŒ€ê³¼ì¼)': "50002194"}
    for keyword in keywords:
        keyword_dfs = []
        with st.spinner(f"'{keyword}' íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ APIì—ì„œ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
            pytrends = TrendReq(hl='ko-KR', tz=540)
            pytrends.build_payload([keyword], cat=0, timeframe=f"{start_date.strftime('%Y-%m-%d')} {end_date.strftime('%Y-%m-%d')}", geo='KR')
            google_df = pytrends.interest_over_time()
            if not google_df.empty and keyword in google_df.columns:
                if 'isPartial' in google_df.columns: google_df = google_df.drop(columns=['isPartial'])
                keyword_dfs.append(google_df.reset_index().rename(columns={'date': 'ë‚ ì§œ', keyword: f'Google_{keyword}'}))
            if naver_keys['id'] and naver_keys['secret']:
                body = json.dumps({"startDate": start_date.strftime('%Y-%m-%d'), "endDate": end_date.strftime('%Y-%m-%d'), "timeUnit": "date", "keywordGroups": [{"groupName": keyword, "keywords": [keyword]}]})
                search_res = call_naver_api("https://openapi.naver.com/v1/datalab/search", body, naver_keys)
                if search_res: keyword_dfs.append(pd.DataFrame(search_res['results'][0]['data']).rename(columns={'period': 'ë‚ ì§œ', 'ratio': f'NaverSearch_{keyword}'}))
                if keyword in NAVER_SHOPPING_CAT_MAP:
                    body_shop = json.dumps({"startDate": start_date.strftime('%Y-%m-%d'), "endDate": end_date.strftime('%Y-%m-%d'), "timeUnit": "date", "category": NAVER_SHOPPING_CAT_MAP[keyword], "keyword": keyword})
                    shop_res = call_naver_api("https://openapi.naver.com/v1/datalab/shopping/categories", body_shop, naver_keys)
                    if shop_res: keyword_dfs.append(pd.DataFrame(shop_res['results'][0]['data']).rename(columns={'period': 'ë‚ ì§œ', 'ratio': f'NaverShop_{keyword}'}))
            if keyword_dfs:
                for i, df in enumerate(keyword_dfs): keyword_dfs[i]['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
                all_data.append(reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), keyword_dfs))
    if not all_data: return pd.DataFrame()
    return reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), all_data)

def fetch_kamis_data(client, item_info, start_date, end_date, kamis_keys):
    project_id = client.project; table_name = "kamis_cache"
    table_id = f"{project_id}.data_explorer.{table_name}"; item_code = item_info['item_code']; kind_code = item_info['kind_code']
    try:
        sql = f"SELECT Date AS ë‚ ì§œ, Price AS ë„ë§¤ê°€ê²©_ì› FROM {table_id} WHERE ItemCode = '{item_code}' AND KindCode = '{kind_code}' AND Date >= '{start_date.strftime('%Y-%m-%d')}' AND Date <= '{end_date.strftime('%Y-%m-%d')}'"
        df_cache = client.query(sql).to_dataframe()
        if len(df_cache) >= (end_date - start_date).days * 0.8:
            st.sidebar.info(f"'{item_info['item_name']}-{item_info['kind_name']}' KAMIS ë°ì´í„°ë¥¼ BigQuery ìºì‹œì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            df_cache['ë‚ ì§œ'] = pd.to_datetime(df_cache['ë‚ ì§œ'])
            return df_cache
    except Exception: pass
    all_data = []
    date_range = pd.date_range(start=start_date, end=end_date)
    progress_bar = st.sidebar.progress(0, text="KAMIS ë°ì´í„° API ì¡°íšŒ ì¤‘...")
    for i, date in enumerate(date_range):
        date_str = date.strftime('%Y-%m-%d')
        url = (f"http://www.kamis.or.kr/service/price/xml.do?p_product_cls_code=01&p_regday={date_str}"
               f"&p_item_category_code={item_info['cat_code']}&p_item_code={item_code}&p_kind_code={kind_code}"
               f"&p_product_rank_code={item_info['rank_code']}&p_convert_kg_yn=Y"
               f"&p_cert_key={kamis_keys['key']}&p_cert_id={kamis_keys['id']}&p_returntype=json")
        try:
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                if "data" in data and data["data"] and "item" in data["data"]:
                    price_str = data["data"]["item"][0].get('price', '0').replace(',', '')
                    if price_str.isdigit() and int(price_str) > 0:
                        all_data.append({'Date': date, 'Price': int(price_str), 'ItemCode': item_code, 'KindCode': kind_code})
        except Exception: continue
        finally: progress_bar.progress((i + 1) / len(date_range))
    progress_bar.empty()
    if not all_data: st.sidebar.warning("í•´ë‹¹ ê¸°ê°„ì— ëŒ€í•œ KAMIS ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return pd.DataFrame()
    df_new = pd.DataFrame(all_data)
    deduplicate_and_write_to_bq(client, df_new, table_name, subset_cols=['Date', 'ItemCode', 'KindCode'])
    df_new['Date'] = pd.to_datetime(df_new['Date'])
    # [ìˆ˜ì •] ë‚ ì§œ ì»¬ëŸ¼ ì´ë¦„ì„ 'ë‚ ì§œ'ë¡œ í†µì¼
    return df_new.rename(columns={'Date': 'ë‚ ì§œ', 'Price': 'ë„ë§¤ê°€ê²©_ì›'})

# --- Constants & App ---
COFFEE_TICKERS_YFINANCE = {"ë¯¸êµ­ ì»¤í”¼ C": "KC=F", "ëŸ°ë˜ ë¡œë¶€ìŠ¤íƒ€": "RC=F"}
KAMIS_FULL_DATA = {
    'ìŒ€': {'cat_code': '100', 'item_code': '111', 'kinds': {'20kg': '01', 'ë°±ë¯¸': '02', 'í˜„ë¯¸': '03', '10kg': '10'}},
    'ê°ì': {'cat_code': '100', 'item_code': '152', 'kinds': {'ìˆ˜ë¯¸(ë…¸ì§€)': '01', 'ìˆ˜ë¯¸(ì‹œì„¤)': '04'}},
    'ë°°ì¶”': {'cat_code': '200', 'item_code': '211', 'kinds': {'ë´„': '01', 'ì—¬ë¦„(ê³ ë­ì§€)': '02', 'ê°€ì„': '03', 'ì›”ë™': '06'}},
    'ì–‘íŒŒ': {'cat_code': '200', 'item_code': '245', 'kinds': {'ì–‘íŒŒ': '00', 'í–‡ì–‘íŒŒ': '02', 'ìˆ˜ì…': '10'}},
    'ì‚¬ê³¼': {'cat_code': '400', 'item_code': '411', 'kinds': {'í›„ì§€': '05', 'ì“°ê°€ë£¨(ì•„ì˜¤ë¦¬)': '06', 'í™ë¡œ': '07'}},
    'ë°”ë‚˜ë‚˜': {'cat_code': '400', 'item_code': '418', 'kinds': {'ìˆ˜ì…': '02'}},
    'ì•„ë³´ì¹´ë„': {'cat_code': '400', 'item_code': '430', 'kinds': {'ìˆ˜ì…': '00'}},
    'ê³ ë“±ì–´': {'cat_code': '600', 'item_code': '611', 'kinds': {'ìƒì„ ': '01', 'ëƒ‰ë™': '02', 'êµ­ì‚°(ì—¼ì¥)': '03'}},
}
st.set_page_config(layout="wide"); st.title("ğŸ“Š ë°ì´í„° íƒìƒ‰ ë° í†µí•© ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
bq_client = get_bq_connection(); sentiment_assets = load_sentiment_assets()
if bq_client is None: st.stop()
st.sidebar.header("âš™ï¸ ë¶„ì„ ì„¤ì •")

if 'data_loaded' not in st.session_state: st.session_state.data_loaded = False
if 'categories' not in st.session_state: st.session_state.categories = get_categories_from_bq(bq_client)

st.sidebar.subheader("1. ë¶„ì„ ëŒ€ìƒ ì„¤ì •")
if not st.session_state.categories:
    st.sidebar.warning("BigQueryì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ìƒˆ ë°ì´í„°ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
else:
    selected_categories = st.sidebar.multiselect("ë¶„ì„í•  í’ˆëª© ì¹´í…Œê³ ë¦¬ ì„ íƒ", st.session_state.categories)
    if st.sidebar.button("ğŸš€ ì„ íƒ ì™„ë£Œ ë° ë¶„ì„ ì‹œì‘", disabled=(not st.session_state.categories)):
        if not selected_categories: st.sidebar.warning("ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        else:
            st.session_state.raw_trade_df = get_trade_data_from_bq(bq_client, selected_categories)
            if st.session_state.raw_trade_df is not None and not st.session_state.raw_trade_df.empty:
                st.session_state.data_loaded = True
                st.session_state.selected_categories = selected_categories
                # [ìˆ˜ì •] st.rerun() ì œê±°
            else: st.sidebar.error("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

with st.sidebar.expander("â• ìƒˆ ìˆ˜ì¶œì… ë°ì´í„° ì¶”ê°€"):
    uploaded_file = st.file_uploader("ìƒˆ íŒŒì¼ ì—…ë¡œë“œ", type=['csv', 'xlsx'])
    if uploaded_file and st.button("ì—…ë¡œë“œ íŒŒì¼ BigQueryì— ì €ì¥"):
        try:
            df_new = pd.read_csv(uploaded_file) if uploaded_file.name.endswith('.csv') else pd.read_excel(uploaded_file)
            numeric_cols = ['Value', 'Volume', 'Unit_Price', 'UnitPrice']
            for col in numeric_cols:
                if col in df_new.columns:
                    df_new[col] = df_new[col].astype(str).str.replace(',', '').replace('-', np.nan)
                    df_new[col] = pd.to_numeric(df_new[col], errors='coerce')
            if 'Date' in df_new.columns: df_new['Date'] = pd.to_datetime(df_new['Date'], errors='coerce')
            add_trade_data_to_bq(bq_client, df_new)
            # [ìˆ˜ì •] st.rerun() ì œê±° ë° ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.clear()
            st.success("ë°ì´í„°ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ë¶„ì„ì„ ë‹¤ì‹œ ì‹œì‘í•˜ì„¸ìš”.")

if not st.session_state.data_loaded:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ê³  'ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."); st.stop()

# --- Analysis UI ---
raw_trade_df = st.session_state.raw_trade_df; selected_categories = st.session_state.selected_categories
st.sidebar.success(f"**{', '.join(selected_categories)}** ë°ì´í„° ë¡œë“œ ì™„ë£Œ!"); st.sidebar.markdown("---")
try:
    if 'Date' not in raw_trade_df.columns:
        st.error("ë¶ˆëŸ¬ì˜¨ ìˆ˜ì¶œì… ë°ì´í„°ì— 'Date' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
    raw_trade_df.dropna(subset=['Date'], inplace=True)
    file_start_date, file_end_date = raw_trade_df['Date'].min(), raw_trade_df['Date'].max()
    default_keywords = ", ".join(selected_categories) if selected_categories else ""
    keyword_input = st.sidebar.text_input("ê²€ìƒ‰ì–´/ë‰´ìŠ¤ ë¶„ì„ í‚¤ì›Œë“œ ì…ë ¥", default_keywords)
    search_keywords = [k.strip() for k in keyword_input.split(',') if k.strip()]
    st.sidebar.subheader("ë¶„ì„ ê¸°ê°„ ì„¤ì •")
    start_date_input = st.sidebar.date_input('ì‹œì‘ì¼', file_start_date, min_value=file_start_date, max_value=file_end_date)
    end_date_input = st.sidebar.date_input('ì¢…ë£Œì¼', file_end_date, min_value=start_date_input, max_value=file_end_date)
    start_date = pd.to_datetime(start_date_input); end_date = pd.to_datetime(end_date_input)
except Exception as e: st.error(f"ë°ì´í„° ê¸°ê°„ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"); st.stop()

raw_wholesale_df = st.session_state.get('wholesale_data', pd.DataFrame())
raw_search_df = st.session_state.get('search_data', pd.DataFrame())
raw_news_df = st.session_state.get('news_data', pd.DataFrame())

st.sidebar.subheader("ğŸ”— ì™¸ë¶€ ë°ì´í„° ì—°ë™")
is_coffee_selected = any('ì»¤í”¼' in str(cat) for cat in selected_categories)
if is_coffee_selected:
    if st.sidebar.button("ì„ ë¬¼ê°€ê²© ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
        st.session_state.wholesale_data = fetch_yfinance_data(COFFEE_TICKERS_YFINANCE, start_date, end_date)
        # [ìˆ˜ì •] st.rerun() ì œê±°
else:
    st.sidebar.markdown("##### KAMIS ë†ì‚°ë¬¼ ê°€ê²©")
    kamis_api_key = st.sidebar.text_input("KAMIS API Key", type="password")
    kamis_api_id = st.sidebar.text_input("KAMIS API ID", type="password")
    item_name = st.sidebar.selectbox("í’ˆëª© ì„ íƒ", list(KAMIS_FULL_DATA.keys()))
    if item_name:
        kind_name = st.sidebar.selectbox("í’ˆì¢… ì„ íƒ", list(KAMIS_FULL_DATA[item_name]['kinds'].keys()))
        if st.sidebar.button("KAMIS ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
            if kamis_api_key and kamis_api_id:
                item_info = {
                    'item_name': item_name, 'kind_name': kind_name,
                    'item_code': KAMIS_FULL_DATA[item_name]['item_code'],
                    'kind_code': KAMIS_FULL_DATA[item_name]['kinds'][kind_name],
                    'cat_code': KAMIS_FULL_DATA[item_name]['cat_code'],
                    'rank_code': '01'
                }
                st.session_state.wholesale_data = fetch_kamis_data(bq_client, item_info, start_date, end_date, {'key': kamis_api_key, 'id': kamis_api_id})
                # [ìˆ˜ì •] st.rerun() ì œê±°
            else: st.sidebar.error("KAMIS API Keyì™€ IDë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
st.sidebar.markdown("##### íŠ¸ë Œë“œ ë°ì´í„°")
naver_client_id = st.sidebar.text_input("Naver API Client ID", type="password")
naver_client_secret = st.sidebar.text_input("Naver API Client Secret", type="password")
if st.sidebar.button("íŠ¸ë Œë“œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
    if not search_keywords: st.sidebar.warning("ê²€ìƒ‰ì–´ë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        st.session_state.search_data = fetch_trends_data(search_keywords, start_date, end_date, {'id': naver_client_id, 'secret': naver_client_secret})
        # [ìˆ˜ì •] st.rerun() ì œê±°
st.sidebar.markdown("##### ë‰´ìŠ¤ ê°ì„± ë¶„ì„")
if st.sidebar.button("ìµœì‹  ë‰´ìŠ¤ ë¶„ì„í•˜ê¸° (RSS)"):
    if not search_keywords: st.sidebar.warning("ë¶„ì„í•  í‚¤ì›Œë“œë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        st.session_state.news_data = fetch_latest_news_rss(bq_client, search_keywords, sentiment_assets)
        # [ìˆ˜ì •] st.rerun() ì œê±°
with st.sidebar.expander("â³ ê³¼ê±° ë‰´ìŠ¤ ë°ì´í„° ì¼ê´„ ìˆ˜ì§‘"):
    st.warning("ì¼íšŒì„± ê¸°ëŠ¥ìœ¼ë¡œ, ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    if st.button("ê³¼ê±° ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘"):
        one_year_ago = datetime.now() - timedelta(days=365)
        st.session_state.news_data = fetch_historical_news(bq_client, search_keywords, one_year_ago, datetime.now(), sentiment_assets)
        # [ìˆ˜ì •] st.rerun() ì œê±°

# --- Main Display ---
tab_list = ["1ï¸âƒ£ ì›ë³¸ ë°ì´í„°", "2ï¸âƒ£ ë°ì´í„° í‘œì¤€í™”", "3ï¸âƒ£ ë‰´ìŠ¤ ê°ì„± ë¶„ì„", "4ï¸âƒ£ ìƒê´€ê´€ê³„ ë¶„ì„", "ğŸ“ˆ ì‹œê³„ì—´ ë¶„í•´ ë° ì˜ˆì¸¡"]
tab1, tab2, tab3, tab4, tab5 = st.tabs(tab_list)

with tab1:
    st.subheader("A. ìˆ˜ì¶œì… ë°ì´í„°"); st.dataframe(raw_trade_df.head())
    st.subheader("B. ì™¸ë¶€ ê°€ê²© ë°ì´í„°"); st.dataframe(raw_wholesale_df.head())
    st.subheader("C. íŠ¸ë Œë“œ ë°ì´í„°"); st.dataframe(raw_search_df.head())
    st.subheader("D. ë‰´ìŠ¤ ë°ì´í„°"); st.dataframe(raw_news_df.head())
    
with tab2:
    st.header("ë°ì´í„° í‘œì¤€í™”: ê°™ì€ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë§ì¶°ì£¼ê¸°")
    trade_df_in_range = raw_trade_df[(raw_trade_df['Date'] >= start_date) & (raw_trade_df['Date'] <= end_date)]
    filtered_trade_df = trade_df_in_range[trade_df_in_range['Category'].isin(selected_categories)].copy()
    
    if not filtered_trade_df.empty:
        filtered_trade_df.set_index('Date', inplace=True)
        trade_weekly = filtered_trade_df.resample('W-Mon').agg(ìˆ˜ì…ì•¡_USD=('Value', 'sum'), ìˆ˜ì…ëŸ‰_KG=('Volume', 'sum')).copy()
        trade_weekly['ìˆ˜ì…ë‹¨ê°€_USD_KG'] = trade_weekly['ìˆ˜ì…ì•¡_USD'] / trade_weekly['ìˆ˜ì…ëŸ‰_KG']
        trade_weekly.index.name = 'ë‚ ì§œ' # [ì¶”ê°€] ì¸ë±ìŠ¤ ì´ë¦„ í†µì¼

        wholesale_weekly = pd.DataFrame()
        if not raw_wholesale_df.empty:
            # [ìˆ˜ì •] ëª¨ë“  ì™¸ë¶€ ë°ì´í„°ëŠ” 'ë‚ ì§œ' ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬
            raw_wholesale_df['ë‚ ì§œ'] = pd.to_datetime(raw_wholesale_df['ë‚ ì§œ'])
            wholesale_weekly = raw_wholesale_df.set_index('ë‚ ì§œ').resample('W-Mon').mean(numeric_only=True)
            if 'ë„ë§¤ê°€ê²©_ì›' in wholesale_weekly.columns:
                wholesale_weekly['ë„ë§¤ê°€ê²©_USD'] = wholesale_weekly['ë„ë§¤ê°€ê²©_ì›'] / 1350 # í™˜ìœ¨ì€ ì˜ˆì‹œ
                wholesale_weekly.drop(columns=['ë„ë§¤ê°€ê²©_ì›'], inplace=True)
            wholesale_weekly.index.name = 'ë‚ ì§œ' # [ì¶”ê°€] ì¸ë±ìŠ¤ ì´ë¦„ í†µì¼

        search_weekly = pd.DataFrame()
        if not raw_search_df.empty:
            raw_search_df['ë‚ ì§œ'] = pd.to_datetime(raw_search_df['ë‚ ì§œ'])
            search_weekly = raw_search_df.set_index('ë‚ ì§œ').resample('W-Mon').mean(numeric_only=True)
            search_weekly.index.name = 'ë‚ ì§œ' # [ì¶”ê°€] ì¸ë±ìŠ¤ ì´ë¦„ í†µì¼

        news_weekly = pd.DataFrame()
        if not raw_news_df.empty:
            raw_news_df['Date'] = pd.to_datetime(raw_news_df['Date'])
            news_df_in_range = raw_news_df[(raw_news_df['Date'] >= start_date) & (raw_news_df['Date'] <= end_date)]
            if not news_df_in_range.empty:
                news_weekly = news_df_in_range.set_index('Date').resample('W-Mon').agg(ë‰´ìŠ¤ê°ì„±ì ìˆ˜=('Sentiment', 'mean')).copy()
                news_weekly.index.name = 'ë‚ ì§œ' # [ì¶”ê°€] ì¸ë±ìŠ¤ ì´ë¦„ í†µì¼
        
        st.session_state['trade_weekly'] = trade_weekly
        st.session_state['wholesale_weekly'] = wholesale_weekly
        st.session_state['search_weekly'] = search_weekly
        st.session_state['news_weekly'] = news_weekly
        st.write("### ì£¼ë³„ ì§‘ê³„ ë°ì´í„° ìƒ˜í”Œ"); 
        st.dataframe(trade_weekly.head())
        st.dataframe(wholesale_weekly.head())
        st.dataframe(search_weekly.head())
        st.dataframe(news_weekly.head())
    else: st.warning("ì„ íƒëœ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ìˆ˜ì¶œì… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

with tab3:
    st.header("ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ê²°ê³¼")
    if not raw_news_df.empty:
        st.subheader("ì£¼ë³„ í‰ê·  ê°ì„± ì ìˆ˜ ì¶”ì´")
        news_weekly_df = st.session_state.get('news_weekly', pd.DataFrame())
        if not news_weekly_df.empty:
            fig = px.line(news_weekly_df, y='ë‰´ìŠ¤ê°ì„±ì ìˆ˜', title="ì£¼ë³„ ë‰´ìŠ¤ ê°ì„± ì ìˆ˜")
            fig.add_hline(y=0, line_dash="dash", line_color="red")
            st.plotly_chart(fig, use_container_width=True)
        st.markdown("---")
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ì „ì²´ ë‰´ìŠ¤ ê°ì„± ë¶„í¬")
            def categorize_sentiment(score):
                if score > 0.1: return "ê¸ì • (Positive)"
                elif score < -0.1: return "ë¶€ì • (Negative)"
                else: return "ì¤‘ë¦½ (Neutral)"
            raw_news_df['Sentiment_Category'] = raw_news_df['Sentiment'].apply(categorize_sentiment)
            sentiment_counts = raw_news_df['Sentiment_Category'].value_counts().reset_index()
            sentiment_counts.columns = ['ê°ì„±', 'ê¸°ì‚¬ ìˆ˜']
            fig_pie = px.pie(sentiment_counts, names='ê°ì„±', values='ê¸°ì‚¬ ìˆ˜', title="ì „ì²´ ê¸°ì‚¬ ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ë¹„ìœ¨", color_discrete_map={'ê¸ì • (Positive)':'blue', 'ë¶€ì • (Negative)':'red', 'ì¤‘ë¦½ (Neutral)':'grey'})
            st.plotly_chart(fig_pie, use_container_width=True)
        with col2:
            st.subheader("í‚¤ì›Œë“œë³„ í‰ê·  ê°ì„± ì ìˆ˜")
            avg_sentiment_by_keyword = raw_news_df.groupby('Keyword')['Sentiment'].mean().reset_index().sort_values(by='Sentiment', ascending=False)
            fig_bar = px.bar(avg_sentiment_by_keyword, x='Keyword', y='Sentiment', title="í‚¤ì›Œë“œë³„ í‰ê·  ê°ì„± ì ìˆ˜ ë¹„êµ", color='Sentiment', color_continuous_scale='RdBu_r', range_color=[-1, 1], labels={'Sentiment': 'í‰ê·  ê°ì„± ì ìˆ˜'})
            st.plotly_chart(fig_bar, use_container_width=True)
        st.markdown("---")
        st.subheader("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ (ìµœì‹ ìˆœ)"); st.dataframe(raw_news_df.sort_values(by='Date', ascending=False))
    else: st.info("ì‚¬ì´ë“œë°”ì—ì„œ 'ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ì„í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

with tab4:
    st.header("ìƒê´€ê´€ê³„ ë¶„ì„")
    trade_weekly = st.session_state.get('trade_weekly', pd.DataFrame())
    wholesale_weekly = st.session_state.get('wholesale_weekly', pd.DataFrame())
    search_weekly = st.session_state.get('search_weekly', pd.DataFrame())
    news_weekly = st.session_state.get('news_weekly', pd.DataFrame())
    dfs_to_concat = [df for df in [trade_weekly, wholesale_weekly, search_weekly, news_weekly] if not df.empty]
    if dfs_to_concat:
        final_df = pd.concat(dfs_to_concat, axis=1).interpolate(method='linear', limit_direction='forward').dropna(how='all')
        st.session_state['final_df'] = final_df
        st.subheader("í†µí•© ë°ì´í„° ì‹œê°í™”")
        if not final_df.empty:
            # [ìˆ˜ì •] ì¸ë±ìŠ¤ ì´ë¦„ì´ 'ë‚ ì§œ'ë¡œ í†µì¼ë˜ì—ˆìœ¼ë¯€ë¡œ reset_index() í›„ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥
            df_to_plot = final_df.reset_index()
            df_long = df_to_plot.melt(id_vars='ë‚ ì§œ', var_name='ë°ì´í„° ì¢…ë¥˜', value_name='ê°’')
            fig = px.line(df_long, x='ë‚ ì§œ', y='ê°’', color='ë°ì´í„° ì¢…ë¥˜', labels={'ê°’': 'ê°’', 'ë‚ ì§œ': 'ë‚ ì§œ'}, title="ìµœì¢… í†µí•© ë°ì´í„° ì‹œê³„ì—´ ì¶”ì´")
            st.plotly_chart(fig, use_container_width=True)

        if len(final_df.columns) > 1:
            st.markdown("---"); st.subheader("ìƒê´€ê´€ê³„ ë¶„ì„")
            st.write("#### ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ")
            corr_matrix = final_df.corr(numeric_only=True)
            fig_heatmap = px.imshow(corr_matrix, text_auto=True, aspect="auto", color_continuous_scale='RdBu_r', range_color=[-1, 1])
            st.plotly_chart(fig_heatmap, use_container_width=True)
            st.write("#### â„¹ï¸ ì‹œì°¨ë³„ ìƒê´€ê´€ê³„ ë¶„ì„")
            base_vars = [col for col in final_df.columns if 'ìˆ˜ì…' in col]
            influencing_vars = [col for col in final_df.columns if 'ìˆ˜ì…' not in col]
            if base_vars and influencing_vars:
                col1_name = st.selectbox("ê¸°ì¤€ ë³€ìˆ˜ (ê²°ê³¼) ì„ íƒ", base_vars)
                col2_name = st.selectbox("ì˜í–¥ ë³€ìˆ˜ (ì›ì¸) ì„ íƒ", influencing_vars)
                @st.cache_data
                def calculate_cross_corr(df, col1, col2, max_lag=12):
                    lags = range(-max_lag, max_lag + 1)
                    correlations = [df[col1].corr(df[col2].shift(lag)) for lag in lags]
                    return pd.DataFrame({'Lag (ì£¼)': lags, 'ìƒê´€ê³„ìˆ˜': correlations})
                if col1_name and col2_name:
                    cross_corr_df = calculate_cross_corr(final_df, col1_name, col2_name)
                    fig_cross_corr = px.bar(cross_corr_df, x='Lag (ì£¼)', y='ìƒê´€ê³„ìˆ˜', title=f"'{col1_name}'ì™€ '{col2_name}'ì˜ ì‹œì°¨ë³„ ìƒê´€ê´€ê³„")
                    fig_cross_corr.add_hline(y=0); st.plotly_chart(fig_cross_corr, use_container_width=True)
                    st.info("""**ê²°ê³¼ í•´ì„ ê°€ì´ë“œ:** ...ìƒëµ...""")
            else: st.warning("ìƒê´€ê´€ê³„ë¥¼ ë¹„êµí•˜ë ¤ë©´ 'ìˆ˜ì…' ê´€ë ¨ ë³€ìˆ˜ì™€ 'ì™¸ë¶€' ë³€ìˆ˜ê°€ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤.")
    else: st.warning("2ë‹¨ê³„ì—ì„œ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

with tab5:
    st.header("ì‹œê³„ì—´ ë¶„í•´ ë° ì˜ˆì¸¡")
    final_df = st.session_state.get('final_df', pd.DataFrame())
    if not final_df.empty:
        forecast_col = st.selectbox("ì˜ˆì¸¡ ëŒ€ìƒ ë³€ìˆ˜ ì„ íƒ", final_df.columns)
        if forecast_col:
            ts_data = final_df[[forecast_col]].dropna()
            # [ìˆ˜ì •] ë¶„í•´ ë° ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„° ê¸¸ì´ ì¡°ê±´ ì™„í™” (ê²½ê³  ë©”ì‹œì§€ë¡œ ëŒ€ì²´)
            if len(ts_data) < 24: # ìµœì†Œ 24ì£¼ ë°ì´í„°ëŠ” í•„ìš”
                st.warning(f"ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•´ ìµœì†Œ 24ì£¼ ì´ìƒì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬ ë°ì´í„°ëŠ” {len(ts_data)}ì£¼ì…ë‹ˆë‹¤.")
            else:
                st.subheader(f"'{forecast_col}' ì‹œê³„ì—´ ë¶„í•´")
                # [ìˆ˜ì •] ì£¼ë³„ ë°ì´í„°ì´ë¯€ë¡œ period=52ë¡œ ê³ ì •í•˜ì§€ ì•Šê³  ë°ì´í„° ê¸¸ì´ì— ë§ê²Œ ì¡°ì •
                period = 52 if len(ts_data) >= 104 else int(len(ts_data) / 2)
                decomposition = seasonal_decompose(ts_data[forecast_col], model='additive', period=period)
                fig_decompose = go.Figure()
                fig_decompose.add_trace(go.Scatter(x=decomposition.observed.index, y=decomposition.observed, mode='lines', name='Observed'))
                fig_decompose.add_trace(go.Scatter(x=decomposition.trend.index, y=decomposition.trend, mode='lines', name='Trend'))
                fig_decompose.add_trace(go.Scatter(x=decomposition.seasonal.index, y=decomposition.seasonal, mode='lines', name='Seasonal'))
                st.plotly_chart(fig_decompose, use_container_width=True)
                st.write("#### ë¶ˆê·œì¹™ ìš”ì†Œ (Residual)"); st.line_chart(decomposition.resid)
                
                st.subheader(f"'{forecast_col}' ë¯¸ë˜ 12ì£¼ ì˜ˆì¸¡ (by Prophet)")
                # [ìˆ˜ì •] ì¸ë±ìŠ¤ ì´ë¦„ì´ 'ë‚ ì§œ'ë¡œ í†µì¼ë˜ì—ˆìœ¼ë¯€ë¡œ reset_index() í›„ rename
                prophet_df = ts_data.reset_index().rename(columns={'ë‚ ì§œ': 'ds', forecast_col: 'y'})
                m = Prophet()
                m.fit(prophet_df)
                future = m.make_future_dataframe(periods=12, freq='W')
                forecast = m.predict(future)
                fig_forecast = plot_plotly(m, forecast)
                fig_forecast.update_layout(title=f"'{forecast_col}' ë¯¸ë˜ ì˜ˆì¸¡ ê²°ê³¼", xaxis_title='ë‚ ì§œ', yaxis_title='ì˜ˆì¸¡ê°’')
                st.plotly_chart(fig_forecast, use_container_width=True)
                st.write("#### ì˜ˆì¸¡ ë°ì´í„° í…Œì´ë¸”"); st.dataframe(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(12))
    else:
        st.info("4ë²ˆ íƒ­ì—ì„œ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ í†µí•©ë˜ì–´ì•¼ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
