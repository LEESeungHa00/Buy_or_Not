import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
import requests
from datetime import datetime, timedelta, timezone
from functools import reduce
import json
import urllib.request
from google.oauth2 import service_account
from google.cloud import bigquery
import pandas_gbq
import feedparser
from urllib.parse import quote
from prophet import Prophet
from prophet.plot import plot_plotly, plot_components_plotly
from kamis_data import KAMIS_FULL_DATA

# Advanced Analysis Libraries
from scipy.stats import pearsonr, spearmanr, kendalltau
from statsmodels.tsa.stattools import adfuller
from statsmodels.stats.multitest import multipletests
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score

# Transformers / HuggingFace
from transformers import pipeline
from huggingface_hub import login as hf_login

# Optional: robust article text fetch
try:
    from newspaper import Article, Config as NewsConfig
    _HAS_NEWSPAPER = True
except ImportError:
    _HAS_NEWSPAPER = False

# ----------------------------
#  Configuration / Constants
# ----------------------------
st.set_page_config(layout="wide")
st.title("ğŸ“Š í†µí•© ë°ì´í„° ê¸°ë°˜ íƒìƒ‰ ë° ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ")

BQ_DATASET = "data_explorer"
BQ_TABLE_NAVER = "naver_trends_cache"
BQ_TABLE_NEWS = "news_sentiment_finbert"
BQ_TABLE_TRADE = "tds_data"

# ----------------------------
#  Helper Functions (Analysis & Interpretation)
# ----------------------------
@st.cache_data
def calculate_advanced_correlation(df, method='pearson', p_adjust_method='fdr_bh'):
    """Calculate correlation matrix and corresponding p-values."""
    df_numeric = df.select_dtypes(include=np.number).dropna(how='all', axis=1)
    cols = df_numeric.columns
    corr_matrix = pd.DataFrame(np.ones((len(cols), len(cols))), index=cols, columns=cols)
    pval_matrix = pd.DataFrame(np.ones((len(cols), len(cols))), index=cols, columns=cols)
    
    pvalues_list = []
    
    for i in range(len(cols)):
        for j in range(i + 1, len(cols)):
            col1_data = df_numeric[cols[i]].dropna()
            col2_data = df_numeric[cols[j]].dropna()
            
            common_index = col1_data.index.intersection(col2_data.index)
            if len(common_index) < 3: continue

            col1_data = col1_data.loc[common_index]
            col2_data = col2_data.loc[common_index]

            if method == 'pearson':
                corr, pval = pearsonr(col1_data, col2_data)
            elif method == 'spearman':
                corr, pval = spearmanr(col1_data, col2_data)
            else: # kendall
                corr, pval = kendalltau(col1_data, col2_data)
                
            corr_matrix.iloc[i, j] = corr_matrix.iloc[j, i] = corr
            pval_matrix.iloc[i, j] = pval_matrix.iloc[j, i] = pval
            pvalues_list.append(pval)

    # Adjust p-values for multiple comparisons
    if pvalues_list:
        _, pvals_corrected, _, _ = multipletests(pvalues_list, alpha=0.05, method=p_adjust_method)
        
        corrected_pval_matrix = pd.DataFrame(np.ones((len(cols), len(cols))), index=cols, columns=cols)
        k = 0
        for i in range(len(cols)):
            for j in range(i + 1, len(cols)):
                corrected_pval_matrix.iloc[i, j] = corrected_pval_matrix.iloc[j, i] = pvals_corrected[k]
                k += 1
    else:
        corrected_pval_matrix = pval_matrix

    return corr_matrix, corrected_pval_matrix


@st.cache_data
def find_best_lagged_correlation(df, driver_vars, outcome_vars, max_lag=12):
    """Find the best time lag for correlation between driver and outcome variables."""
    best_correlations = []
    for driver in driver_vars:
        for outcome in outcome_vars:
            if driver == outcome: continue
            
            max_corr_val = 0
            best_lag = 0
            
            for lag in range(-max_lag, max_lag + 1):
                try:
                    shifted_driver = df[driver].shift(lag)
                    corr = shifted_driver.corr(df[outcome])
                except Exception:
                    corr = np.nan

                if pd.notna(corr) and abs(corr) > abs(max_corr_val):
                    max_corr_val = corr
                    best_lag = lag
            
            if best_lag != 0:
                best_correlations.append({
                    'Driver (X)': driver,
                    'Outcome (Y)': outcome,
                    'Best Lag (Weeks)': best_lag,
                    'Correlation': max_corr_val
                })
    
    if not best_correlations:
        return pd.DataFrame()
        
    df_lags = pd.DataFrame(best_correlations)
    df_lags['Abs Correlation'] = df_lags['Correlation'].abs()
    return df_lags.sort_values('Abs Correlation', ascending=False).drop(columns=['Abs Correlation'])

def interpret_correlation_groups(corr_matrix, pval_matrix, threshold=0.05):
    """Generates a human-readable interpretation of correlations between driver and outcome groups."""
    driver_keywords = ['Naver', 'News_', 'Sentiment', 'ë„ë§¤']
    outcome_keywords = ['ìˆ˜ì…']

    all_cols = corr_matrix.columns
    driver_cols = [col for col in all_cols if any(key in col for key in driver_keywords)]
    outcome_cols = [col for col in all_cols if any(key in col for key in outcome_keywords)]
    
    driver_cols = [col for col in driver_cols if col not in outcome_cols]

    if not driver_cols or not outcome_cols:
        return "ìë™ í•´ì„ì„ ìœ„í•œ 'Driver(ì›ì¸)' ë˜ëŠ” 'Outcome(ê²°ê³¼)' ê·¸ë£¹ì˜ ë³€ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. (Driver: 'Naver', 'News', 'ë„ë§¤' ë“± / Outcome: 'ìˆ˜ì…' ë“±)"

    strong_corrs = []
    for driver in driver_cols:
        for outcome in outcome_cols:
            if driver not in corr_matrix.index or outcome not in corr_matrix.columns:
                continue

            corr_val = corr_matrix.loc[driver, outcome]
            pval = pval_matrix.loc[driver, outcome]
            
            if pval < threshold and abs(corr_val) >= 0.4: # Slightly lower threshold for discovery
                direction = "ê°•í•œ ì–‘ì˜" if corr_val > 0 else "ê°•í•œ ìŒì˜"
                interpretation = (
                    f"**'{driver}'** (ì›ì¸)ì™€(ê³¼) **'{outcome}'** (ê²°ê³¼) ì‚¬ì´ì—ëŠ” "
                    f"í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ **{direction} ìƒê´€ê´€ê³„**ê°€ ìˆìŠµë‹ˆë‹¤ (ìƒê´€ê³„ìˆ˜: {corr_val:.2f}, p-value: {pval:.3f})."
                )
                strong_corrs.append({'text': interpretation, 'value': abs(corr_val)})
    
    if not strong_corrs:
        return "í˜„ì¬ ì„¤ì •ëœ ìœ ì˜ìˆ˜ì¤€ì—ì„œ Driver(ì›ì¸) ë³€ìˆ˜ì™€ Outcome(ê²°ê³¼) ë³€ìˆ˜ ê°„ì˜ ì˜ë¯¸ ìˆëŠ” ê´€ê³„ëŠ” ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë³€ìˆ˜ ì„ íƒì´ë‚˜ ê¸°ê°„ì„ ë³€ê²½í•˜ì—¬ ë‹¤ì‹œ ë¶„ì„í•´ ë³´ì„¸ìš”."
        
    strong_corrs = sorted(strong_corrs, key=lambda x: x['value'], reverse=True)
    
    summary = "### ì£¼ìš” ë°œê²¬ (Driver vs. Outcome):\n\n" + "\n".join([f"- {corr['text']}" for corr in strong_corrs[:5]])
    summary += "\n\n* **ì–‘ì˜ ìƒê´€ê´€ê³„ (+):** ì›ì¸ ë³€ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ê²°ê³¼ ë³€ìˆ˜ë„ í•¨ê»˜ ì¦ê°€í•˜ëŠ” ê²½í–¥ì„ ë³´ì…ë‹ˆë‹¤."
    summary += "\n* **ìŒì˜ ìƒê´€ê´€ê³„ (-):** ì›ì¸ ë³€ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ê²°ê³¼ ë³€ìˆ˜ëŠ” ì˜¤íˆë ¤ ê°ì†Œí•˜ëŠ” ê²½í–¥ì„ ë³´ì…ë‹ˆë‹¤."
    return summary

def interpret_adf_test(adf_result):
    """Generates an interpretation of the ADF test result."""
    p_value = adf_result[1]
    if p_value < 0.05:
        return (
            f"**ê²°ë¡ : ëª¨ë¸ì´ ì•ˆì •ì ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.** (p-value: {p_value:.3f})\n\n"
            "ADF í…ŒìŠ¤íŠ¸ ê²°ê³¼, ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨)ë“¤ì´ íŠ¹ì • íŒ¨í„´ ì—†ì´ ë¬´ì‘ìœ„ì ì¸ 'ë°±ìƒ‰ì†ŒìŒ'ì— ê°€ê¹ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. "
            "ì´ëŠ” Prophet ëª¨ë¸ì´ ë°ì´í„°ì˜ ì£¼ìš” ì¶”ì„¸ì™€ ê³„ì ˆì„± íŒ¨í„´ì„ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµí–ˆë‹¤ëŠ” ê¸ì •ì ì¸ ì‹ í˜¸ì…ë‹ˆë‹¤."
        )
    else:
        return (
            f"**ê²°ë¡ : ëª¨ë¸ ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.** (p-value: {p_value:.3f})\n\n"
            "ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨)ì— ì•„ì§ ì„¤ëª…ë˜ì§€ ì•Šì€ íŠ¹ì • íŒ¨í„´ì´ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. "
            "ì´ëŠ” ëª¨ë¸ì´ ë°ì´í„°ì˜ ëª¨ë“  ì •ë³´ë¥¼ ì™„ì „íˆ í•™ìŠµí•˜ì§€ ëª»í–ˆì„ ê°€ëŠ¥ì„±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. "
            "ì˜ˆì¸¡ë ¥ì„ ë†’ì´ê¸° ìœ„í•´ ë‹¤ë¥¸ ì™¸ë¶€ ë³€ìˆ˜(Regressor)ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ Prophet íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )

def interpret_xgboost_results(r2, rmse, feature_imp_df, y_mean):
    """Generates an interpretation of XGBoost model results."""
    r2_perc = r2 * 100
    rmse_perc = (rmse / y_mean) * 100 if y_mean != 0 else 0

    top_features = feature_imp_df.sort_values('Value', ascending=False).head(3)['Feature'].tolist()

    interpretation = f"""
    ### XGBoost ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½:

    - **RÂ² Score (ì„¤ëª…ë ¥): {r2:.3f}**
      - **ì˜ë¯¸:** ìš°ë¦¬ ëª¨ë¸ì´ ì˜ˆì¸¡ ëŒ€ìƒ ë³€ìˆ˜ì˜ ì „ì²´ ë³€ë™ì„± ì¤‘ **ì•½ {r2_perc:.1f}%**ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤.
      - **íŒë‹¨:** 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤. ì´ ìˆ˜ì¹˜ê°€ ë†’ë‹¤ëŠ” ê²ƒì€ ëª¨ë¸ì´ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì˜ í¬ì°©í•˜ê³  ìˆë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.

    - **RMSE (í‰ê·  ì˜ˆì¸¡ ì˜¤ì°¨): {rmse:.3f}**
      - **ì˜ë¯¸:** ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì€ ì‹¤ì œê°’ê³¼ í‰ê· ì ìœ¼ë¡œ **ì•½ {rmse:.3f}** ë§Œí¼ì˜ ì˜¤ì°¨ë¥¼ ë³´ì…ë‹ˆë‹¤. (ì´ëŠ” ì˜ˆì¸¡ ëŒ€ìƒ í‰ê· ê°’ì˜ ì•½ {rmse_perc:.1f}% ìˆ˜ì¤€ì…ë‹ˆë‹¤.)
      - **íŒë‹¨:** 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤. ì´ ìˆ˜ì¹˜ê°€ ë‚®ë‹¤ëŠ” ê²ƒì€ ì˜ˆì¸¡ì´ ë” ì •ë°€í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

    ### ì˜ˆì¸¡ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹œ ë³€ìˆ˜ TOP 3:
    1. **{top_features[0]}**
    2. **{top_features[1]}**
    3. **{top_features[2]}**

    **ì¢…í•© ì˜ê²¬:**
    ì´ ë³€ìˆ˜ë“¤ì´ ë¯¸ë˜ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ê°€ì¥ ì¤‘ìš”í•œ ì—­í• ì„ í–ˆìŠµë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµ ìˆ˜ë¦½ ì‹œ ì´ í•µì‹¬ ì§€í‘œë“¤ì˜ ë³€í™”ë¥¼ ì£¼ì˜ ê¹Šê²Œ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
    """
    return interpretation

# ----------------------------
#  Helpers: BigQuery + Naver + KAMIS
# ----------------------------
@st.cache_resource
def get_bq_connection():
    try:
        creds_dict = st.secrets["gcp_service_account"]
        creds = service_account.Credentials.from_service_account_info(creds_dict)
        client = bigquery.Client(credentials=creds, project=creds.project_id)
        return client
    except Exception as e:
        st.error(f"Google BigQuery ì—°ê²° ì‹¤íŒ¨: secrets.tomlì„ í™•ì¸í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}")
        return None

def upload_df_to_bq(client, df, table_id):
    """Uploads a DataFrame to a specified BigQuery table."""
    try:
        pandas_gbq.to_gbq(
            df,
            f"{BQ_DATASET}.{table_id}",
            project_id=client.project,
            if_exists="append",
            credentials=client._credentials
        )
        return True, None
    except Exception as e:
        return False, str(e)

def call_naver_api(url, body, naver_keys):
    try:
        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", naver_keys['id'])
        request.add_header("X-Naver-Client-Secret", naver_keys['secret'])
        request.add_header("Content-Type", "application/json")
        response = urllib.request.urlopen(request, data=body.encode("utf-8"))
        if response.getcode() == 200:
            return json.loads(response.read().decode('utf-8'))
        return None
    except Exception as e:
        st.error(f"Naver API ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

@st.cache_data(ttl=3600)
def fetch_kamis_data(_client, item_info, start_date, end_date, kamis_keys):
    start_str, end_str = start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')
    url = (f"http://www.kamis.or.kr/service/price/xml.do?action=periodWholesaleProductList"
           f"&p_product_cls_code=01&p_startday={start_str}&p_endday={end_str}"
           f"&p_item_category_code={item_info['cat_code']}&p_item_code={item_info['item_code']}&p_kind_code={item_info['kind_code']}"
           f"&p_product_rank_code={item_info['rank_code']}&p_convert_kg_yn=Y"
           f"&p_cert_key={kamis_keys['key']}&p_cert_id={kamis_keys['id']}&p_returntype=json")
    try:
        response = requests.get(url, timeout=20, headers={"User-Agent": "Mozilla/5.0 KAMISClient/1.0"})
        if response.status_code == 200 and "data" in response.json() and "item" in response.json()["data"]:
            price_data = response.json()["data"]["item"]
            if not price_data:
                return pd.DataFrame()
            df_new = pd.DataFrame(price_data)[['regday', 'price']].rename(columns={'regday': 'ë‚ ì§œ', 'price': 'ë„ë§¤ê°€ê²©_ì›'})
            def format_kamis_date(date_str):
                processed_str = str(date_str).replace('/', '-')
                if processed_str.count('-') == 1:
                    return f"{start_date.year}-{processed_str}"
                return processed_str
            df_new['ë‚ ì§œ'] = pd.to_datetime(df_new['ë‚ ì§œ'].apply(format_kamis_date), errors='coerce')
            df_new['ë„ë§¤ê°€ê²©_ì›'] = pd.to_numeric(df_new['ë„ë§¤ê°€ê²©_ì›'].astype(str).str.replace(',', ''), errors='coerce')
            return df_new
    except Exception as e:
        st.sidebar.error(f"KAMIS API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
    return pd.DataFrame()
# ----------------------------
#  HuggingFace: ë¡œê·¸ì¸ + ì•ˆì „í•œ ëª¨ë¸ ë¡œë“œ
# ----------------------------
hf_token = st.secrets.get("huggingface", {}).get("token")
if hf_token:
    try:
        hf_login(token=hf_token)
    except Exception as e:
        st.sidebar.warning(f"HuggingFace ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")

DEFAULT_MODEL_IDS = {
    "finbert": "snunlp/KR-FinBERT-SC",
    "elite": "nlptown/bert-base-multilingual-uncased-sentiment",
    "product": "cardiffnlp/twitter-xlm-roberta-base-sentiment"
}
@st.cache_resource
def load_models(model_ids, token):
    models = {}
    for key, mid in model_ids.items():
        try:
            models[key] = pipeline("sentiment-analysis", model=mid, tokenizer=mid, token=token)
        except Exception:
            models[key] = None
    return models
models = load_models(DEFAULT_MODEL_IDS, hf_token)

# ----------------------------
#  ê°ì„±ë¶„ì„ ì ìˆ˜ ë³€í™˜ í•¨ìˆ˜
# ----------------------------
def _label_score_to_signed(pred):
    if not pred: return 0.0, "neutral"
    lbl, score = str(pred.get("label", "")).lower(), float(pred.get("score", 0.0))
    if "star" in lbl:
        try:
            n = int(lbl.split()[0])
            return (n - 3) / 2.0, "positive" if n > 3 else "negative" if n < 3 else "neutral"
        except (ValueError, IndexError):
            return 0.0, "neutral"
    if any(x in lbl for x in ["neg", "negative", "ë¶€ì •"]): return -score, "negative"
    if any(x in lbl for x in ["pos", "positive", "ê¸ì •"]): return score, "positive"
    return 0.0, "neutral"

def analyze_sentiment_multi(texts, _models_dict):
    results = []
    if not texts: return results
    
    preds = {key: (model(texts, truncation=True, max_length=512) if model else [None]*len(texts)) for key, model in _models_dict.items()}
        
    for i, _ in enumerate(texts):
        fin_s, fin_l = _label_score_to_signed(preds.get("finbert", [])[i] if preds.get("finbert") else None)
        el_s, el_l = _label_score_to_signed(preds.get("elite", [])[i] if preds.get("elite") else None)
        pr_s, pr_l = _label_score_to_signed(preds.get("product", [])[i] if preds.get("product") else None)
        results.append({
            "FinBERT_Sentiment": fin_s, "FinBERT_Label": fin_l,
            "Elite_Sentiment": el_s, "Elite_Label": el_l,
            "Product_Sentiment": pr_s, "Product_Label": pr_l,
            "Sentiment": fin_s, "Label": fin_l
        })
    return results
# ----------------------------
#  ë‰´ìŠ¤ ìˆ˜ì§‘/ë¶„ì„
# ----------------------------
def _fetch_article_text(url):
    if not _HAS_NEWSPAPER or not url: return ""
    try:
        cfg = NewsConfig()
        cfg.browser_user_agent = "Mozilla/5.0 (compatible; NewsBot/1.0)"
        cfg.request_timeout = 10
        art = Article(url, language='ko', config=cfg)
        art.download()
        art.parse()
        return art.text or ""
    except Exception:
        return ""

@st.cache_data(ttl=3600)
def get_news_with_multi_model_analysis(_bq_client, _models_dict, keyword, days_limit=7):
    project_id = _bq_client.project
    full_table_id = f"{project_id}.{BQ_DATASET}.{BQ_TABLE_NEWS}"
    try:
        time_limit = datetime.now(timezone.utc) - timedelta(days=days_limit)
        query = "SELECT * FROM `{}` WHERE Keyword = @keyword AND InsertedAt >= @time_limit ORDER BY ë‚ ì§œ DESC".format(full_table_id)
        job_config = bigquery.QueryJobConfig(query_parameters=[
            bigquery.ScalarQueryParameter("keyword", "STRING", keyword),
            bigquery.ScalarQueryParameter("time_limit", "TIMESTAMP", time_limit)
        ])
        df_cache = _bq_client.query(query, job_config=job_config).to_dataframe()
    except Exception:
        df_cache = pd.DataFrame()
    if not df_cache.empty:
        return df_cache
    all_news = []
    rss_url = f"https://news.google.com/rss/search?q={quote(keyword)}&hl=ko&gl=KR&ceid=KR:ko"
    feed = feedparser.parse(rss_url)
    for entry in feed.entries[:60]:
        title = entry.get('title', '').strip()
        link = entry.get('link', '').strip()
        if not title: continue
        pub_date = pd.to_datetime(entry.get('published')).date() if 'published' in entry else datetime.utcnow().date()
        body = _fetch_article_text(link)
        text_for_model = (title + ". " + body).strip() if body else title
        all_news.append({"ë‚ ì§œ": pub_date, "Title": title, "RawUrl": link, "ModelInput": text_for_model})
    if not all_news:
        st.error(f"'{keyword}'ì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    df_new = pd.DataFrame(all_news).drop_duplicates(subset=["Title"])
    with st.spinner(f"ë‹¤ì¤‘ ëª¨ë¸ë¡œ '{keyword}' ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì¤‘ ({len(df_new)}ê±´)..."):
        multi = analyze_sentiment_multi(df_new['ModelInput'].tolist(), _models_dict)
    multi_df = pd.DataFrame(multi)
    df_new = pd.concat([df_new.reset_index(drop=True), multi_df], axis=1)
    df_new['Keyword'] = keyword
    df_new['InsertedAt'] = datetime.now(timezone.utc)
    try:
        df_to_gbq = df_new.drop(columns=['ModelInput'])
        upload_df_to_bq(_bq_client, df_to_gbq, BQ_TABLE_NEWS)
    except Exception as e:
        st.sidebar.warning(f"BigQuery ë‰´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
    return df_new.drop(columns=['ModelInput'])

# ----------------------------
#  BigQuery ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ë“¤
# ----------------------------
@st.cache_data(ttl=3600)
def get_categories_from_bq(_client):
    try:
        query = f"SELECT DISTINCT Category FROM `{_client.project}.{BQ_DATASET}.{BQ_TABLE_TRADE}` ORDER BY Category"
        return sorted(_client.query(query).to_dataframe()['Category'].astype(str).unique())
    except Exception: return []

def get_trade_data_from_bq(client, categories):
    if not categories: return pd.DataFrame()
    try:
        sql = f"SELECT * FROM `{client.project}.{BQ_DATASET}.{BQ_TABLE_TRADE}` WHERE Category IN UNNEST(@categories)"
        job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ArrayQueryParameter("categories", "STRING", categories)])
        df = client.query(sql, job_config=job_config).to_dataframe()
        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
        for col in [c for c in df.columns if 'price' in c.lower() or 'value' in c.lower() or 'volume' in c.lower()]:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        return df
    except Exception as e:
        st.error(f"BigQuery TDS ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

@st.cache_data(ttl=3600)
def fetch_naver_trends_data(_client, keywords, start_date, end_date, naver_keys):
    project_id = _client.project
    table_id = f"{project_id}.{BQ_DATASET}.{BQ_TABLE_NAVER}"
    try:
        sql = f"SELECT * FROM `{table_id}` ORDER BY ë‚ ì§œ"
        df_cache = _client.query(sql).to_dataframe()
        if not df_cache.empty:
            df_cache['ë‚ ì§œ'] = pd.to_datetime(df_cache['ë‚ ì§œ'])
    except Exception:
        df_cache = pd.DataFrame(columns=['ë‚ ì§œ'])

    fetch_start_date = start_date
    if not df_cache.empty:
        last_cached_date = df_cache['ë‚ ì§œ'].max().date()
        if start_date.date() > last_cached_date:
            fetch_start_date = start_date
        elif end_date.date() > last_cached_date:
            fetch_start_date = pd.Timestamp(last_cached_date) + timedelta(days=1)
        else:
            return df_cache[(df_cache['ë‚ ì§œ'] >= pd.to_datetime(start_date)) & (df_cache['ë‚ ì§œ'] <= pd.to_datetime(end_date))].reset_index(drop=True)

    new_data_list = []
    if fetch_start_date <= end_date:
        current_start = fetch_start_date
        while current_start <= end_date:
            current_end = min(current_start + timedelta(days=89), end_date)
            all_data_chunk = []
            for keyword in keywords:
                body_search = json.dumps({
                    "startDate": current_start.strftime('%Y-%m-%d'),
                    "endDate": current_end.strftime('%Y-%m-%d'),
                    "timeUnit": "date",
                    "keywordGroups": [{"groupName": keyword, "keywords": [keyword]}]
                })
                search_res = call_naver_api("https://openapi.naver.com/v1/datalab/search", body_search, naver_keys)
                if search_res and search_res.get('results') and search_res['results'][0]['data']:
                    df_search = pd.DataFrame(search_res['results'][0]['data'])
                    if not df_search.empty:
                         df_search.rename(columns={'period': 'ë‚ ì§œ', 'ratio': f'Naver_{keyword}'}, inplace=True)
                         all_data_chunk.append(df_search)
            if all_data_chunk:
                merged_chunk = reduce(lambda l, r: pd.merge(l, r, on='ë‚ ì§œ', how='outer'), all_data_chunk)
                new_data_list.append(merged_chunk)
            current_start = current_end + timedelta(days=1)
    if new_data_list:
        df_new = pd.concat(new_data_list, ignore_index=True)
        df_new['ë‚ ì§œ'] = pd.to_datetime(df_new['ë‚ ì§œ'])
        if not df_cache.empty:
            df_combined = pd.concat([df_cache, df_new], ignore_index=True).sort_values('ë‚ ì§œ').drop_duplicates(subset=['ë‚ ì§œ'], keep='last')
        else:
            df_combined = df_new.sort_values('ë‚ ì§œ')
        try:
            pandas_gbq.to_gbq(df_combined, f"{BQ_DATASET}.{BQ_TABLE_NAVER}", project_id=project_id, if_exists="replace", credentials=_client._credentials)
        except Exception as e:
            st.warning(f"Naver íŠ¸ë Œë“œ ìºì‹œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        df_final = df_combined
    else:
        df_final = df_cache
    if df_final.empty:
        return pd.DataFrame()
    return df_final[(df_final['ë‚ ì§œ'] >= pd.to_datetime(start_date)) & (df_final['ë‚ ì§œ'] <= pd.to_datetime(end_date))].reset_index(drop=True)

# Initialize BQ Client and session state
bq_client = get_bq_connection()
if bq_client is None: st.stop()
if 'final_df' not in st.session_state: st.session_state.final_df = pd.DataFrame()

# ----------------------------
#  Sidebar: UI & Data Loading
# ----------------------------
st.sidebar.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
categories = get_categories_from_bq(bq_client)
selected_categories = st.sidebar.multiselect("ë¶„ì„í•  í’ˆëª© ì„ íƒ", categories, default=categories[:1] if categories else [])
start_date = pd.to_datetime(st.sidebar.date_input('ì‹œì‘ì¼', datetime(2022, 1, 1)))
end_date = pd.to_datetime(st.sidebar.date_input('ì¢…ë£Œì¼', datetime.now()))
news_keyword_input = st.sidebar.text_input("ë‰´ìŠ¤ ë¶„ì„ í‚¤ì›Œë“œ", selected_categories[0] if selected_categories else "")
search_keywords_input = st.sidebar.text_input("ë„¤ì´ë²„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„)", ",".join(selected_categories) if selected_categories else "")
search_keywords = [k.strip() for k in search_keywords_input.split(',') if k.strip()]

with st.sidebar.expander("ğŸ”‘ API í‚¤ ì…ë ¥ (ì„ íƒ)"):
    naver_client_id = st.text_input("Naver API Client ID", type="password")
    naver_client_secret = st.text_input("Naver API Client Secret", type="password")
    st.sidebar.markdown("---")
    kamis_api_key = st.text_input("KAMIS API Key", type="password")
    kamis_api_id = st.text_input("KAMIS API ID", type="password")
    st.sidebar.markdown("##### KAMIS ë†ì‚°ë¬¼ ê°€ê²© ì„ íƒ")
    kamis_item_name = st.sidebar.selectbox("í’ˆëª© ì„ íƒ", list(KAMIS_FULL_DATA.keys()))
    kamis_kind_name = None
    if kamis_item_name:
        kamis_kind_name = st.sidebar.selectbox("í’ˆì¢… ì„ íƒ", list(KAMIS_FULL_DATA[kamis_item_name]['kinds'].keys()))

if st.sidebar.button("ğŸš€ ëª¨ë“  ë°ì´í„° í†µí•© ë° ë¶„ì„ ì‹¤í–‰"):
    if not selected_categories:
        st.error("ë¶„ì„í•  í’ˆëª©ì„ 1ê°œ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ëª¨ë“  ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  í†µí•©í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            trade_df = get_trade_data_from_bq(bq_client, selected_categories)
            trade_df_in_range = trade_df[(trade_df['Date'] >= start_date) & (trade_df['Date'] <= end_date)]
            
            if trade_df_in_range.empty:
                st.error("ì„ íƒëœ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                trade_agg = trade_df_in_range.groupby('Date').agg(Value=('Value', 'sum'), Volume=('Volume', 'sum')).copy()
                trade_agg.set_index(pd.to_datetime(trade_agg.index), inplace=True)
                trade_weekly = trade_agg.resample('W-Mon').agg(ìˆ˜ì…ì•¡_USD=('Value', 'sum'), ìˆ˜ì…ëŸ‰_KG=('Volume', 'sum')).copy()
                trade_weekly['ìˆ˜ì…ë‹¨ê°€_USD_KG'] = trade_weekly['ìˆ˜ì…ì•¡_USD'] / trade_weekly['ìˆ˜ì…ëŸ‰_KG']
                trade_weekly.index.name = 'ë‚ ì§œ'
                
                all_weekly_dfs = {'trade': trade_weekly}

                if news_keyword_input:
                    news_df = get_news_with_multi_model_analysis(bq_client, models, news_keyword_input)
                    if not news_df.empty:
                        news_df['ë‚ ì§œ'] = pd.to_datetime(news_df['ë‚ ì§œ'])
                        sentiment_cols = [col for col in news_df.columns if 'Sentiment' in col]
                        news_weekly = news_df.set_index('ë‚ ì§œ')[sentiment_cols].resample('W-Mon').mean()
                        all_weekly_dfs['news'] = news_weekly.rename(columns=lambda x: 'News_' + x.replace("News_", ""))

                if search_keywords and naver_client_id and naver_client_secret:
                    naver_df = fetch_naver_trends_data(bq_client, search_keywords, start_date, end_date, {'id': naver_client_id, 'secret': naver_client_secret})
                    if not naver_df.empty:
                        naver_df['ë‚ ì§œ'] = pd.to_datetime(naver_df['ë‚ ì§œ'])
                        naver_weekly = naver_df.set_index('ë‚ ì§œ').resample('W-Mon').mean()
                        all_weekly_dfs['naver'] = naver_weekly

                if kamis_api_key and kamis_api_id and kamis_item_name and kamis_kind_name:
                    item_info = KAMIS_FULL_DATA[kamis_item_name].copy()
                    item_info['kind_code'] = item_info['kinds'][kamis_kind_name]
                    item_info['rank_code'] = '01'
                    kamis_df = fetch_kamis_data(bq_client, item_info, start_date, end_date, {'key': kamis_api_key, 'id': kamis_api_id})
                    if not kamis_df.empty:
                        kamis_df['ë‚ ì§œ'] = pd.to_datetime(kamis_df['ë‚ ì§œ'])
                        kamis_weekly = kamis_df.set_index('ë‚ ì§œ').resample('W-Mon').mean()
                        all_weekly_dfs['kamis'] = kamis_weekly

                final_df = reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), all_weekly_dfs.values())
                final_df = final_df.interpolate(method='time').fillna(method='bfill').fillna(method='ffill')
                st.session_state.final_df = final_df.dropna(how='all', axis=1).replace([np.inf, -np.inf], np.nan).dropna()
                st.success("ë°ì´í„° í†µí•© ì™„ë£Œ!")

# --- CSV/Excel Upload Section ---
st.sidebar.markdown("---")
st.sidebar.subheader("í’ˆëª© ë°ì´í„° íŒŒì¼ ì—…ë¡œë“œ")
uploaded_file = st.sidebar.file_uploader("CSV ë˜ëŠ” Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=['csv', 'xlsx'], help="í•„ìˆ˜ ì»¬ëŸ¼: Date, Value, Volume")
new_category_name = st.sidebar.text_input("ì—…ë¡œë“œí•  ë°ì´í„°ì˜ í’ˆëª©ëª…ì„ ì…ë ¥í•˜ì„¸ìš”.", help="ì˜ˆ: ì•„ë³´ì¹´ë„, ë°”ë‚˜ë‚˜ ë“±")
if st.sidebar.button("íŒŒì¼ì„ BigQueryì— ì—…ë¡œë“œ"):
    if uploaded_file and new_category_name.strip():
        with st.spinner("íŒŒì¼ ì²˜ë¦¬ ë° ì—…ë¡œë“œ ì¤‘..."):
            try:
                df_upload = pd.read_csv(uploaded_file) if uploaded_file.name.endswith('.csv') else pd.read_excel(uploaded_file, engine='openpyxl')
                df_upload.columns = [c.lower() for c in df_upload.columns]
                required_cols = {'date', 'value', 'volume'}
                if not required_cols.issubset(df_upload.columns):
                    st.sidebar.error("íŒŒì¼ì— í•„ìˆ˜ ì»¬ëŸ¼('Date', 'Value', 'Volume')ì´ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    df_upload = df_upload.rename(columns={'date': 'Date', 'value': 'Value', 'volume': 'Volume'})
                    df_upload['Date'] = pd.to_datetime(df_upload['Date'], errors='coerce')
                    df_upload['Value'] = pd.to_numeric(df_upload['Value'], errors='coerce')
                    df_upload['Volume'] = pd.to_numeric(df_upload['Volume'], errors='coerce')
                    df_upload['Category'] = new_category_name.strip()
                    df_to_bq = df_upload[['Date', 'Category', 'Value', 'Volume']].dropna()
                    success, error_msg = upload_df_to_bq(bq_client, df_to_bq, BQ_TABLE_TRADE)
                    if success:
                        st.sidebar.success(f"'{new_category_name}' ë°ì´í„° {len(df_to_bq)}ê±´ì„ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œí–ˆìŠµë‹ˆë‹¤!")
                        st.cache_data.clear()
                    else:
                        st.sidebar.error(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {error_msg}")
            except Exception as e:
                st.sidebar.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    else:
        st.sidebar.warning("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  í’ˆëª©ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# ----------------------------
#  Guide Content
# ----------------------------
guide_content = """
### ğŸš€ ëŒ€ì‹œë³´ë“œ ì‚¬ìš©ë²• ê°€ì´ë“œ
ì´ ëŒ€ì‹œë³´ë“œëŠ” ë°ì´í„° ì „ë¬¸ê°€ê°€ ì•„ë‹ˆë”ë¼ë„ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ë°ì´í„° ì† ìˆ¨ì€ ì˜ë¯¸ë¥¼ ì°¾ê³  ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê° ê¸°ëŠ¥ì˜ ì˜ë¯¸ì™€ í™œìš©ë²•ì„ ì´í•´í•˜ë©´ ë” ê°•ë ¥í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
---
### 1. ğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„ íƒ­ í™œìš©ë²•
ì´ íƒ­ì€ **"ì–´ë–¤ ë°ì´í„°ë“¤ì´ ì„œë¡œ ê´€ë ¨ì´ ìˆì„ê¹Œ?"** ë¼ëŠ” ì§ˆë¬¸ì— ë‹µì„ ì¤ë‹ˆë‹¤.
#### **â‘  ë¶„ì„ ë°©ë²• ì„ íƒ: `Pearson` vs `Spearman`**
* **`Pearson (í”¼ì–´ìŠ¨)`: ì§ì„  ê´€ê³„ë¥¼ ë³´ê³  ì‹¶ì„ ë•Œ** ("Aê°€ 2ë°° ì˜¤ë¥´ë©´ Bë„ ì •ì§í•˜ê²Œ 2ë°° ì˜¤ë¥¼ê¹Œ?")
* **`Spearman (ìŠ¤í”¼ì–´ë§Œ)`: ë°©í–¥ì„±ë§Œ ë³´ê³  ì‹¶ì„ ë•Œ (ê°•ë ¥ ì¶”ì²œ!)** ("Aê°€ ì˜¤ë¥´ë©´ Bë„ ì˜¤ë¥´ê¸´ ì˜¤ë¥´ë‚˜?")
> **ğŸ’¡ íŒ:** í—·ê°ˆë¦°ë‹¤ë©´, ì¼ë°˜ì ìœ¼ë¡œ **`Spearman`**ì„ ë¨¼ì € ì‚¬ìš©í•´ ë³´ì„¸ìš”.
#### **â‘¡ ìœ ì˜ìˆ˜ì¤€ (P-value) í•„í„° ì„¤ì •**
"ì´ ê´€ê³„ê°€ ìš°ì—°ì¼ê¹Œ, ì§„ì§œ ì˜ë¯¸ê°€ ìˆëŠ” ê±¸ê¹Œ?"ë¥¼ íŒë‹¨í•˜ëŠ” ê¸°ì¤€ì…ë‹ˆë‹¤.
* **`0.05` (ê¸°ë³¸ê°’, ì¶”ì²œ)**: ê´€ê³„ê°€ ìš°ì—°ì¼ í™•ë¥  5% ë¯¸ë§Œì´ë¼ëŠ” ì˜ë¯¸ë¡œ, **"í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ë‹¤"**ê³  íŒë‹¨í•©ë‹ˆë‹¤.
> **ğŸ’¡ íŒ:** ìŠ¬ë¼ì´ë”ë¥¼ `0.05`ì— ë§ì¶”ê³  ì—¬ê¸°ì„œ ë‚˜íƒ€ë‚˜ëŠ” ê´€ê³„ë“¤ì„ 'ì§„ì§œ' ì‹ í˜¸ë¡œ í•´ì„í•˜ì„¸ìš”.
#### **â‘¢ ì‹œì°¨ êµì°¨ìƒê´€ ë¶„ì„: ì„ í–‰/í›„í–‰ ë³€ìˆ˜ ì„ íƒ**
"Aì˜ ë³€í™”ê°€ ëª‡ ì£¼ í›„ì— Bì—ê²Œ ì˜í–¥ì„ ë¯¸ì¹ ê¹Œ?" ì™€ ê°™ì€ **ì„ í›„ ê´€ê³„**ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
* **`ì„ í–‰ ë³€ìˆ˜ (Driver)`**: ì›ì¸ì´ ë˜ê±°ë‚˜, ë¨¼ì € ì›€ì§ì´ëŠ” ë³€ìˆ˜. (ì˜ˆ: `News_Sentiment`, `Naver_ê²€ìƒ‰ëŸ‰`)
* **`í›„í–‰ ë³€ìˆ˜ (Outcome)`**: ì˜í–¥ì„ ë°›ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ê²°ê³¼ ë³€ìˆ˜. (ì˜ˆ: `ìˆ˜ì…ë‹¨ê°€_USD_KG`)
> **ğŸ’¡ ê²°ê³¼ í•´ì„:** 'Best Lag (Weeks)'ê°€ `2`ë¡œ ë‚˜ì™”ë‹¤ë©´, ì„ í–‰ ë³€ìˆ˜ê°€ 2ì£¼ ë¨¼ì € ì›€ì§ì´ê³  2ì£¼ ë’¤ì— í›„í–‰ ë³€ìˆ˜ê°€ ë°˜ì‘í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.
---
### 2. ğŸ“ˆ ì‹œê³„ì—´ ì˜ˆì¸¡ íƒ­ í™œìš©ë²•
ì´ íƒ­ì€ **"ê·¸ë˜ì„œ ë¯¸ë˜ì—ëŠ” ì–´ë–»ê²Œ ë ê¹Œ?"** ë¼ëŠ” ì§ˆë¬¸ì— ë‹µì„ ì¤ë‹ˆë‹¤.
#### **â‘  ì˜ˆì¸¡ ëŒ€ìƒ ë³€ìˆ˜ (y) ì„ íƒ**
ê°€ì¥ ê¶ê¸ˆí•œ **ë¯¸ë˜ ê°’**ì„ ì„ íƒí•©ë‹ˆë‹¤. (ì˜ˆ: `ìˆ˜ì…ë‹¨ê°€_USD_KG`, `ìˆ˜ì…ëŸ‰_KG`)
#### **â‘¡ Prophet ëª¨ë¸ íŒŒë¼ë¯¸í„° íŠœë‹**
AI ì˜ˆì¸¡ ëª¨ë¸ì˜ ì„±ê²©ì„ ì¡°ì ˆí•˜ì—¬ ìš°ë¦¬ ë°ì´í„°ì— ê¼­ ë§ê²Œ ë§Œë“­ë‹ˆë‹¤.
* **`Trend ìœ ì—°ì„±`**: ì¶”ì„¸ ë³€í™”ì— ì–¼ë§ˆë‚˜ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í• ì§€ ì¡°ì ˆí•©ë‹ˆë‹¤. (ì‹œì¥ì´ ì—­ë™ì ì¼ìˆ˜ë¡ ë†’ê²Œ ì„¤ì •)
* **`ê³„ì ˆì„± ê°•ë„`**: ë°˜ë³µë˜ëŠ” íŒ¨í„´ì„ ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ ë¯¿ì„ì§€ ì¡°ì ˆí•©ë‹ˆë‹¤. (ê³„ì ˆì„±ì´ ëšœë ·í• ìˆ˜ë¡ ë†’ê²Œ ì„¤ì •)
* **`ê³„ì ˆì„± ëª¨ë“œ`**: `multiplicative`ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ì¥ì— ë”°ë¼ ë³€ë™ í­ë„ í•¨ê»˜ ì»¤ì§ˆ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. (ì¼ë°˜ì ìœ¼ë¡œ ë” í”í•¨)
> **ğŸ’¡ íŒ:** ì²˜ìŒì—ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹¤í–‰í•˜ê³ , ì˜ˆì¸¡ ê·¸ë˜í”„ê°€ ê³¼ê±° ë°ì´í„°ë¥¼ ì˜ ëª» ë”°ë¼ê°ˆ ë•Œ ì´ ê°’ë“¤ì„ ì¡°ì ˆí•´ë³´ì„¸ìš”.
"""

# ----------------------------
#  Main Dashboard Tabs
# ----------------------------
if not st.session_state.final_df.empty:
    final_df = st.session_state.final_df
    
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„", "ğŸ“ˆ ì‹œê³„ì—´ ì˜ˆì¸¡", "ğŸ“„ í†µí•© ë°ì´í„°", "ğŸ“˜ ëŒ€ì‹œë³´ë“œ ì‚¬ìš©ë²•"])

    with tab1:
        st.header("ìƒê´€ê´€ê³„ ë¶„ì„")
        col1, col2 = st.columns(2)
        corr_method = col1.selectbox("ìƒê´€ê´€ê³„ ë¶„ì„ ë°©ë²•", ('pearson', 'spearman'), help="í”¼ì–´ìŠ¨: ì„ í˜•, ìŠ¤í”¼ì–´ë§Œ: ë¹„ì„ í˜• ê´€ê³„")
        pval_threshold = col2.slider("ìœ ì˜ìˆ˜ì¤€(P-value) í•„í„°", 0.0, 1.0, 0.05)
        corr_matrix, pval_matrix = calculate_advanced_correlation(final_df, method=corr_method)
        st.subheader(f"'{corr_method.capitalize()}' ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ")
        fig_heatmap = go.Figure(data=go.Heatmap(z=corr_matrix.values, x=corr_matrix.columns, y=corr_matrix.columns, colorscale='RdBu_r', zmin=-1, zmax=1, text=corr_matrix.round(2).astype(str), texttemplate="%{text}"))
        st.plotly_chart(fig_heatmap, use_container_width=True)
        with st.expander("ğŸ” íˆíŠ¸ë§µ ê²°ê³¼ í•´ì„í•˜ê¸° (Driver vs Outcome)"):
            st.markdown(interpret_correlation_groups(corr_matrix, pval_matrix, threshold=pval_threshold))
        st.markdown("---")
        st.subheader("ì‹œì°¨ êµì°¨ìƒê´€ ë¶„ì„")
        driver_cols = st.multiselect("ì„ í–‰ ë³€ìˆ˜(Driver) ì„ íƒ", final_df.columns, default=[c for c in final_df if any(k in c for k in ['News', 'Naver', 'ë„ë§¤'])])
        outcome_cols = st.multiselect("í›„í–‰ ë³€ìˆ˜(Outcome) ì„ íƒ", final_df.columns, default=[c for c in final_df if 'ìˆ˜ì…' in c])
        if driver_cols and outcome_cols:
            lag_df = find_best_lagged_correlation(final_df, driver_cols, outcome_cols)
            st.dataframe(lag_df.head(10))
            if not lag_df.empty:
                top_lag = lag_df.iloc[0]
                with st.expander("ğŸ” ì‹œì°¨ ë¶„ì„ ê²°ê³¼ í•´ì„í•˜ê¸°"):
                    st.info(f"ê°€ì¥ ê°•í•œ ì‹œì°¨ ê´€ê³„: **{top_lag['Driver (X)']}**ì˜ ë³€í™”ëŠ” **{top_lag['Best Lag (Weeks)']}ì£¼ í›„** **{top_lag['Outcome (Y)']}**ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤ (ìƒê´€ê³„ìˆ˜: {top_lag['Correlation']:.3f}).")

    with tab2:
        st.header("ì‹œê³„ì—´ ì˜ˆì¸¡ (Prophet & XGBoost)")
        prophet_df = final_df.reset_index().rename(columns={'ë‚ ì§œ': 'ds'})
        col1, col2 = st.columns(2)
        forecast_col = col1.selectbox("ì˜ˆì¸¡ ëŒ€ìƒ ë³€ìˆ˜ (y)", final_df.columns)
        forecast_periods = col2.number_input("ì˜ˆì¸¡ ê¸°ê°„ (ì£¼)", 4, 52, 12)
        prophet_df = prophet_df.rename(columns={forecast_col: 'y'})
        regressors = [c for c in prophet_df.columns if c not in ['ds', 'y']]
        selected_regressors = st.multiselect("ì™¸ë¶€ ì˜ˆì¸¡ ë³€ìˆ˜", regressors, default=regressors)
        st.subheader("Prophet ëª¨ë¸ íŒŒë¼ë¯¸í„° íŠœë‹")
        p_col1, p_col2, p_col3 = st.columns(3)
        changepoint_prior_scale = p_col1.slider("Trend ìœ ì—°ì„±", 0.01, 0.5, 0.05)
        seasonality_prior_scale = p_col2.slider("ê³„ì ˆì„± ê°•ë„", 0.01, 10.0, 1.0)
        seasonality_mode = p_col3.selectbox("ê³„ì ˆì„± ëª¨ë“œ", ('additive', 'multiplicative'))
        if st.button("ğŸš€ ì˜ˆì¸¡ ì‹¤í–‰", key="run_forecast"):
            with st.spinner("Prophet & XGBoost ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ì¤‘..."):
                m = Prophet(changepoint_prior_scale=changepoint_prior_scale, seasonality_prior_scale=seasonality_prior_scale, seasonality_mode=seasonality_mode)
                for reg in selected_regressors: m.add_regressor(reg)
                m.fit(prophet_df[['ds', 'y'] + selected_regressors])
                future = m.make_future_dataframe(periods=forecast_periods, freq='W')
                future_regressors = prophet_df[['ds'] + selected_regressors].set_index('ds')
                last_values = future_regressors.iloc[-1]
                future_regressors = future_regressors.reindex(future['ds']).fillna(method='ffill').fillna(last_values)
                future = pd.concat([future.set_index('ds'), future_regressors], axis=1).reset_index()
                forecast = m.predict(future)
                ml_df = forecast[['ds', 'trend', 'yearly', 'weekly']].set_index('ds').join(prophet_df.set_index('ds')).dropna()
                X, y = ml_df.drop(columns=['y']), ml_df['y']
                train_size = int(len(X) * 0.85)
                X_train, X_test, y_train, y_test = X.iloc[:train_size], X.iloc[train_size:], y.iloc[:train_size], y.iloc[train_size:]
                xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=1000, learning_rate=0.01, early_stopping_rounds=50)
                xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
                y_pred_xgb = xgb_model.predict(X_test)
                r2, rmse = r2_score(y_test, y_pred_xgb), np.sqrt(mean_squared_error(y_test, y_pred_xgb))
                feature_imp = pd.DataFrame(sorted(zip(xgb_model.feature_importances_, X.columns)), columns=['Value','Feature'])
            st.subheader("Prophet ì˜ˆì¸¡ ê²°ê³¼")
            st.plotly_chart(plot_plotly(m, forecast), use_container_width=True)
            with st.expander("ğŸ” Prophet ì˜ˆì¸¡ í•´ì„"): st.markdown("- **ê²€ì€ ì :** ì‹¤ì œ ë°ì´í„°\n- **íŒŒë€ì„ :** ì˜ˆì¸¡ê°’\n- **íŒŒë€ ì˜ì—­:** ë¶ˆí™•ì‹¤ì„± êµ¬ê°„")
            st.subheader("Prophet ìš”ì¸ ë¶„í•´")
            st.plotly_chart(plot_components_plotly(m, forecast), use_container_width=True)
            st.markdown("---")
            st.subheader("ëª¨ë¸ ì§„ë‹¨: ì”ì°¨ ë¶„ì„")
            df_pred = forecast.set_index('ds')[['yhat']].join(prophet_df.set_index('ds')[['y']]).dropna()
            residuals = df_pred['y'] - df_pred['yhat']
            diag_col1, diag_col2 = st.columns(2)
            adf_result = adfuller(residuals)
            diag_col1.metric("ì”ì°¨ ADF Test p-value", f"{adf_result[1]:.4f}")
            with diag_col1.expander("ğŸ” ADF í…ŒìŠ¤íŠ¸ í•´ì„"): st.markdown(interpret_adf_test(adf_result))
            fig_dist = ff.create_distplot([residuals], ['residuals'], show_rug=False)
            diag_col2.plotly_chart(fig_dist, use_container_width=True)
            st.markdown("---")
            st.subheader("XGBoost Meta-Forecasting ê²°ê³¼")
            xgb_col1, xgb_col2 = st.columns(2)
            xgb_col1.metric("Test RÂ² Score", f"{r2:.3f}")
            xgb_col2.metric("Test RMSE", f"{rmse:.3f}")
            fig_xgb = go.Figure()
            fig_xgb.add_trace(go.Scatter(x=y_test.index, y=y_test, name='Actual'))
            fig_xgb.add_trace(go.Scatter(x=y_test.index, y=y_pred_xgb, name='XGBoost Prediction'))
            st.plotly_chart(fig_xgb, use_container_width=True)
            st.plotly_chart(px.bar(feature_imp, x="Value", y="Feature", orientation='h', title="Feature Importance"), use_container_width=True)
            with st.expander("ğŸ” XGBoost ê²°ê³¼ í•´ì„"): st.markdown(interpret_xgboost_results(r2, rmse, feature_imp, y_test.mean()))

    with tab3:
        st.header("í†µí•© ë°ì´í„° (ì£¼ë³„)")
        st.dataframe(final_df)
        st.download_button("CSVë¡œ ë‹¤ìš´ë¡œë“œ", final_df.to_csv(index=False).encode('utf-8-sig'), "integrated_weekly_data.csv")
        
    with tab4:
        st.header("ëŒ€ì‹œë³´ë“œ ì‚¬ìš©ë²• ê°€ì´ë“œ")
        st.markdown(guide_content)

else:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ë°ì´í„°ë¥¼ ì„ íƒí•˜ê³  'ëª¨ë“  ë°ì´í„° í†µí•© ë° ë¶„ì„ ì‹¤í–‰' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

