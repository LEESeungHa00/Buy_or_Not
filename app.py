import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import requests
from datetime import datetime, timedelta
from functools import reduce
from pytrends.request import TrendReq
import json
import urllib.request
import yfinance as yf
from google.oauth2 import service_account
from google.cloud import bigquery
import pandas_gbq

# --- BigQuery Connection (Manual Method for Stability) ---
@st.cache_resource
def get_bq_connection():
    """BigQueryì— ì§ì ‘ ì—°ê²°í•˜ê³  í´ë¼ì´ì–¸íŠ¸ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        creds_dict = st.secrets["gcp_service_account"]
        creds = service_account.Credentials.from_service_account_info(creds_dict)
        client = bigquery.Client(credentials=creds, project=creds.project_id)
        return client
    except Exception as e:
        st.error(f"Google BigQuery ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. secrets.tomlì˜ [gcp_service_account] ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”: {e}")
        return None

# --- Data Fetching & Processing Functions (Optimized BigQuery Version) ---
@st.cache_data(ttl=3600)
def get_categories_from_bq(_client):
    """BigQueryì—ì„œ ê³ ìœ  ì¹´í…Œê³ ë¦¬ ëª©ë¡ë§Œ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    project_id = _client.project
    table_id = f"{project_id}.data_explorer.tds_data"
    try:
        with st.spinner("BigQueryì—ì„œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
            query = f"SELECT DISTINCT Category FROM `{table_id}` WHERE Category IS NOT NULL ORDER BY Category"
            df = _client.query(query).to_dataframe()
        return sorted(df['Category'].astype(str).unique())
    except Exception as e:
        st.error(f"BigQueryì—ì„œ ì¹´í…Œê³ ë¦¬ë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def get_trade_data_from_bq(client, categories):
    """BigQueryì˜ tds_data í…Œì´ë¸”ì—ì„œ ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì˜ ë°ì´í„°ë§Œ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not categories: return pd.DataFrame()
    project_id = client.project; table_id = f"{project_id}.data_explorer.tds_data"
    try:
        query_params = [bigquery.ArrayQueryParameter("categories", "STRING", categories)]
        sql = f"SELECT * FROM `{table_id}` WHERE Category IN UNNEST(@categories)"
        job_config = bigquery.QueryJobConfig(query_parameters=query_params)
        with st.spinner(f"BigQueryì—ì„œ ì„ íƒëœ {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘..."):
            df = client.query(sql, job_config=job_config).to_dataframe()
        for col in df.columns:
            if 'price' in col.lower() or 'value' in col.lower() or 'volume' in col.lower():
                df[col] = pd.to_numeric(df[col], errors='coerce')
            elif 'date' in col.lower():
                df[col] = pd.to_datetime(df[col], errors='coerce')
        return df
    except Exception as e:
        st.error(f"BigQueryì—ì„œ TDS ë°ì´í„°ë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"); return pd.DataFrame()

def add_data_to_bq(client, df, table_name): # --- [FIX] Generic function to add data ---
    """ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì§€ì •ëœ BigQuery í…Œì´ë¸”ì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    project_id = client.project
    table_id = f"{project_id}.data_explorer.{table_name}"
    try:
        df.columns = df.columns.str.replace(' ', '_').str.replace('[^A-Za-z0-9_]', '', regex=True)
        with st.spinner(f"ìƒˆ ë°ì´í„°ë¥¼ BigQuery '{table_name}' í…Œì´ë¸”ì— ì €ì¥í•˜ëŠ” ì¤‘..."):
            pandas_gbq.to_gbq(df, table_id, project_id=project_id, if_exists="append", credentials=client._credentials)
        st.success(f"ìƒˆ ë°ì´í„°ê°€ BigQuery '{table_name}' í…Œì´ë¸”ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.error(f"BigQueryì— ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# --- API Fetching with BigQuery Caching ---
def fetch_yfinance_data(client, tickers, start_date, end_date):
    project_id = client.project; table_name = "yfinance_cache"
    table_id = f"{project_id}.data_explorer.{table_name}"
    all_data = []

    for name, ticker in tickers.items():
        try:
            sql = f"SELECT Date AS ì¡°ì‚¬ì¼ì, Close AS Price FROM `{table_id}` WHERE Ticker = '{ticker}' AND Date >= '{start_date}' AND Date <= '{end_date}'"
            df_cache = client.query(sql).to_dataframe()
            if len(df_cache) >= (end_date - start_date).days * 0.9:
                st.sidebar.info(f"'{name}' ë°ì´í„°ë¥¼ BigQuery ìºì‹œì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                all_data.append(df_cache.rename(columns={'Price': f'{name}_ì„ ë¬¼ê°€ê²©_USD'}))
                continue
        except Exception: pass

        with st.spinner(f"'{name}' ë°ì´í„°ë¥¼ Yahoo Finance APIì—ì„œ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
            data = yf.download(ticker, start=start_date, end=end_date, progress=False)
            if not data.empty:
                df = data[['Close']].copy().reset_index()
                df['Ticker'] = ticker
                add_data_to_bq(client, df.rename(columns={'Date': 'Date', 'Close': 'Close'}), table_name=table_name)
                all_data.append(df.rename(columns={'Date': 'ì¡°ì‚¬ì¼ì', 'Close': f'{name}_ì„ ë¬¼ê°€ê²©_USD'}))
            else:
                st.sidebar.warning(f"'{name}'ì— ëŒ€í•œ API ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    if not all_data: return pd.DataFrame()
    return reduce(lambda left, right: pd.merge(left, right, on='ì¡°ì‚¬ì¼ì', how='outer'), all_data)


def fetch_trends_data(client, keywords, start_date, end_date, naver_keys):
    project_id = client.project; table_name = "trends_cache"
    table_id = f"{project_id}.data_explorer.{table_name}"
    all_data = []

    for keyword in keywords:
        try:
            sql = f"SELECT * FROM `{table_id}` WHERE Keyword = '{keyword}' AND ë‚ ì§œ >= '{start_date}' AND ë‚ ì§œ <= '{end_date}'"
            df_cache = client.query(sql).to_dataframe()
            if len(df_cache) >= (end_date - start_date).days * 0.9:
                st.sidebar.info(f"'{keyword}' ê²€ìƒ‰ëŸ‰ ë°ì´í„°ë¥¼ BigQuery ìºì‹œì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                reshaped_df = df_cache.pivot(index='ë‚ ì§œ', columns='Source', values='Value').reset_index()
                reshaped_df.columns = ['ë‚ ì§œ'] + [f'{col}_{keyword}' for col in reshaped_df.columns if col != 'ë‚ ì§œ']
                all_data.append(reshaped_df)
                continue
        except Exception: pass
        
        keyword_dfs = []
        with st.spinner(f"'{keyword}' ê²€ìƒ‰ëŸ‰ ë°ì´í„°ë¥¼ APIì—ì„œ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
            pytrends = TrendReq(hl='ko-KR', tz=540)
            timeframe = f"{start_date.strftime('%Y-%m-%d')} {end_date.strftime('%Y-%m-%d')}"
            pytrends.build_payload([keyword], cat=0, timeframe=timeframe, geo='KR', gprop='')
            google_df = pytrends.interest_over_time()
            if not google_df.empty and keyword in google_df.columns:
                google_df_renamed = google_df.reset_index().rename(columns={'date': 'ë‚ ì§œ', keyword: f'Google_{keyword}'})
                keyword_dfs.append(google_df_renamed)

            if naver_keys['id'] and naver_keys['secret']:
                url = "https://openapi.naver.com/v1/datalab/search"
                body = json.dumps({"startDate": start_date.strftime('%Y-%m-%d'), "endDate": end_date.strftime('%Y-%m-%d'), "timeUnit": "date", "keywordGroups": [{"groupName": keyword, "keywords": [keyword]}]})
                request = urllib.request.Request(url)
                request.add_header("X-Naver-Client-Id", naver_keys['id']); request.add_header("X-Naver-Client-Secret", naver_keys['secret']); request.add_header("Content-Type", "application/json")
                response = urllib.request.urlopen(request, data=body.encode("utf-8"))
                if response.getcode() == 200:
                    naver_raw = json.loads(response.read().decode('utf-8'))
                    naver_df = pd.DataFrame(naver_raw['results'][0]['data']).rename(columns={'period': 'ë‚ ì§œ', 'ratio': f'Naver_{keyword}'})
                    naver_df['ë‚ ì§œ'] = pd.to_datetime(naver_df['ë‚ ì§œ'])
                    keyword_dfs.append(naver_df)
            
            if keyword_dfs:
                merged_df = reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), keyword_dfs)
                all_data.append(merged_df)
                
                # Reshape for caching
                df_to_cache = merged_df.melt(id_vars=['ë‚ ì§œ'], var_name='Source', value_name='Value')
                df_to_cache[['Source', 'Keyword']] = df_to_cache['Source'].str.split('_', expand=True)
                add_data_to_bq(client, df_to_cache, table_name=table_name)


    if not all_data: return pd.DataFrame()
    return reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), all_data)


def fetch_kamis_data(client, item_info, start_date, end_date, kamis_keys):
    # (KAMIS data is volatile and API is per-day, so fetching fresh is often better)
    all_data = []
    date_range = pd.date_range(start=start_date, end=end_date)
    if len(date_range) > 180: st.sidebar.warning(f"KAMIS ì¡°íšŒ ê¸°ê°„ì´ {len(date_range)}ì¼ë¡œ ê¹ë‹ˆë‹¤. ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    progress_bar = st.sidebar.progress(0, text="KAMIS ë°ì´í„° ì¡°íšŒ ì¤‘...")
    
    for i, date in enumerate(date_range):
        date_str = date.strftime('%Y-%m-%d')
        url = (f"http://www.kamis.or.kr/service/price/xml.do?p_product_cls_code=02&p_item_category_code={item_info['cat_code']}"
               f"&p_item_code={item_info['item_code']}&p_regday={date_str}&p_convert_kg_yn=Y"
               f"&p_cert_key={kamis_keys['key']}&p_cert_id={kamis_keys['id']}&p_returntype=json")
        try:
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                items = data.get("data", {}).get("item", [])
                if items:
                    price_str = items[0].get('dpr1', '0').replace(',', '')
                    if price_str.isdigit() and int(price_str) > 0:
                        all_data.append({'ì¡°ì‚¬ì¼ì': date, 'ë„ë§¤ê°€ê²©_ì›': int(price_str)})
        except Exception: continue
        finally: progress_bar.progress((i + 1) / len(date_range), text=f"KAMIS ë°ì´í„° ì¡°íšŒ ì¤‘... {date_str}")
    
    progress_bar.empty()
    if not all_data: st.sidebar.warning("í•´ë‹¹ ê¸°ê°„ì— ëŒ€í•œ KAMIS ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return pd.DataFrame()
    df = pd.DataFrame(all_data)
    return df

# --- Constants ---
COFFEE_TICKERS_YFINANCE = {"ë¯¸êµ­ ì»¤í”¼ C": "KC=F", "ëŸ°ë˜ ë¡œë¶€ìŠ¤íƒ€": "RC=F"}
KAMIS_CATEGORIES = {"ì±„ì†Œë¥˜": "100", "ê³¼ì¼ë¥˜": "200", "ì¶•ì‚°ë¬¼": "300", "ìˆ˜ì‚°ë¬¼": "400"}
KAMIS_ITEMS = {"ì±„ì†Œë¥˜": {"ë°°ì¶”": "111", "ë¬´": "112", "ì–‘íŒŒ": "114", "ë§ˆëŠ˜": "141"}, "ê³¼ì¼ë¥˜": {"ì‚¬ê³¼": "211", "ë°”ë‚˜ë‚˜": "214", "ì•„ë³´ì¹´ë„": "215"}, "ì¶•ì‚°ë¬¼": {"ì†Œê³ ê¸°": "311", "ë¼ì§€ê³ ê¸°": "312"}, "ìˆ˜ì‚°ë¬¼": {"ê³ ë“±ì–´": "411", "ì˜¤ì§•ì–´": "413"}}

# --- Streamlit App ---
st.set_page_config(layout="wide")
st.title("ğŸ“Š ë°ì´í„° íƒìƒ‰ ë° í†µí•© ëŒ€ì‹œë³´ë“œ (Google BigQuery ì—°ë™)")

bq_client = get_bq_connection()
if bq_client is None: st.stop()

st.sidebar.header("âš™ï¸ ë¶„ì„ ì„¤ì •")

# --- App Startup Workflow (Optimized BigQuery Version) ---
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
    st.session_state.categories = get_categories_from_bq(bq_client)

if not st.session_state.categories:
    st.info("BigQueryì— ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    st.sidebar.subheader("ìƒˆ ìˆ˜ì¶œì… ë°ì´í„° ì¶”ê°€")
    uploaded_file = st.sidebar.file_uploader("ìƒˆ íŒŒì¼ ì—…ë¡œë“œí•˜ì—¬ BigQueryì— ì¶”ê°€", type=['csv', 'xlsx'])
    if uploaded_file:
        if st.sidebar.button("ì—…ë¡œë“œ íŒŒì¼ BigQueryì— ì €ì¥"):
            df_new = pd.read_csv(uploaded_file) if uploaded_file.name.endswith('.csv') else pd.read_excel(uploaded_file)
            add_data_to_bq(bq_client, df_new, table_name="tds_data")
            st.session_state.clear()
            st.rerun()
    st.stop()

if not st.session_state.data_loaded:
    st.sidebar.subheader("1. ë¶„ì„ ëŒ€ìƒ ì„¤ì •")
    selected_categories = st.sidebar.multiselect("ë¶„ì„í•  í’ˆëª© ì¹´í…Œê³ ë¦¬ ì„ íƒ", st.session_state.categories)
    if st.sidebar.button("ğŸš€ ì„ íƒ ì™„ë£Œ ë° ë¶„ì„ ì‹œì‘"):
        if not selected_categories:
            st.sidebar.warning("ë¶„ì„í•  ì¹´í…Œê³ ë¦¬ë¥¼ í•˜ë‚˜ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
        else:
            st.session_state.raw_trade_df = get_trade_data_from_bq(bq_client, selected_categories)
            if st.session_state.raw_trade_df is not None and not st.session_state.raw_trade_df.empty:
                st.session_state.data_loaded = True
                st.session_state.selected_categories = selected_categories
                st.rerun()
            else:
                st.sidebar.error("ì„ íƒí•œ ì¹´í…Œê³ ë¦¬ì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    else:
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ê³  'ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
        st.stop()

# --- Analysis UI (runs only after data is loaded) ---
raw_trade_df = st.session_state.raw_trade_df
selected_categories = st.session_state.selected_categories

st.sidebar.success(f"**{', '.join(selected_categories)}** ì¹´í…Œê³ ë¦¬ ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
st.sidebar.markdown("---")

try:
    file_start_date, file_end_date = raw_trade_df['Date'].min(), raw_trade_df['Date'].max()
    default_keywords = ", ".join(selected_categories) if selected_categories else ""
    keyword_input = st.sidebar.text_input("3. ê²€ìƒ‰ì–´ ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„)", default_keywords)
    search_keywords = [k.strip() for k in keyword_input.split(',') if k.strip()]
    st.sidebar.subheader("4. ë¶„ì„ ê¸°ê°„ ì„¤ì •")
    start_date_input = st.sidebar.date_input('ì‹œì‘ì¼', file_start_date, min_value=file_start_date, max_value=file_end_date)
    end_date_input = st.sidebar.date_input('ì¢…ë£Œì¼', file_end_date, min_value=start_date_input, max_value=file_end_date)
    start_date = pd.to_datetime(start_date_input)
    end_date = pd.to_datetime(end_date_input)
except Exception as e:
    st.error(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. 'Date' ë˜ëŠ” 'Category' ì»¬ëŸ¼ì˜ ë°ì´í„° í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”: {e}")
    st.stop()

# --- External Data Loading Section ---
st.sidebar.subheader("ğŸ”— ì™¸ë¶€ ê°€ê²© ë°ì´í„°")
is_coffee_selected = any('ì»¤í”¼' in str(cat) for cat in selected_categories)
if is_coffee_selected:
    st.sidebar.info("Yahoo Financeì—ì„œ ì„ ë¬¼ê°€ê²©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    if st.sidebar.button("ì„ ë¬¼ê°€ê²© ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
        df = fetch_yfinance_data(bq_client, COFFEE_TICKERS_YFINANCE, start_date, end_date)
        st.session_state['wholesale_data'] = df
else:
    st.sidebar.info("KAMISì—ì„œ ë†ì‚°ë¬¼ ë„ë§¤ê°€ê²© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    kamis_api_key = st.sidebar.text_input("KAMIS API Key", type="password")
    kamis_api_id = st.sidebar.text_input("KAMIS API ID", type="password")
    cat_name = st.sidebar.selectbox("í’ˆëª© ë¶„ë¥˜ ì„ íƒ", list(KAMIS_CATEGORIES.keys()))
    if cat_name:
        item_name = st.sidebar.selectbox("ì„¸ë¶€ í’ˆëª© ì„ íƒ", list(KAMIS_ITEMS[cat_name].keys()))
        if st.sidebar.button("KAMIS ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
            if kamis_api_key and kamis_api_id:
                item_info = {'item_code': KAMIS_ITEMS[cat_name][item_name], 'cat_code': KAMIS_CATEGORIES[cat_name]}
                df = fetch_kamis_data(bq_client, item_info, start_date, end_date, {'key': kamis_api_key, 'id': kamis_api_id})
                st.session_state['wholesale_data'] = df
            else: st.sidebar.error("KAMIS API Keyì™€ IDë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
raw_wholesale_df = st.session_state.get('wholesale_data', pd.DataFrame())

# --- Search Data Loading Section ---
st.sidebar.subheader("ğŸ“° ê²€ìƒ‰ëŸ‰ ë°ì´í„°")
naver_client_id = st.sidebar.text_input("Naver API Client ID", type="password")
naver_client_secret = st.sidebar.text_input("Naver API Client Secret", type="password")
if st.sidebar.button("ê²€ìƒ‰ëŸ‰ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
    if not search_keywords: st.sidebar.warning("ê²€ìƒ‰ì–´ë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        df = fetch_trends_data(bq_client, search_keywords, start_date, end_date, {'id': naver_client_id, 'secret': naver_client_secret})
        st.session_state['search_data'] = df
raw_search_df = st.session_state.get('search_data', pd.DataFrame())

# --- Main Display Area ---
tab1, tab2, tab3 = st.tabs(["1ï¸âƒ£ ì›ë³¸ ë°ì´í„° í™•ì¸", "2ï¸âƒ£ ë°ì´í„° í‘œì¤€í™”", "3ï¸âƒ£ ìµœì¢… í†µí•© ë°ì´í„°"])
with tab1:
    st.subheader("A. ìˆ˜ì¶œì… ë°ì´í„° (from BigQuery)"); 
    st.dataframe(raw_trade_df.head())
    st.subheader("B. ì™¸ë¶€ ê°€ê²© ë°ì´í„°"); st.dataframe(raw_wholesale_df.head())
    st.subheader("C. ê²€ìƒ‰ëŸ‰ ë°ì´í„°"); st.dataframe(raw_search_df.head())

with tab2:
    if not selected_categories: st.warning("ë¶„ì„í•  ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
    else:
        st.subheader("2-1. ë¶„ì„ ëŒ€ìƒ í’ˆëª© í•„í„°ë§")
        trade_df_in_range = raw_trade_df[(raw_trade_df['Date'] >= start_date) & (raw_trade_df['Date'] <= end_date)]
        filtered_trade_df = trade_df_in_range[trade_df_in_range['Category'].isin(selected_categories)].copy()
        st.write(f"ì„ íƒëœ ì¹´í…Œê³ ë¦¬: **{', '.join(selected_categories)}**"); st.dataframe(filtered_trade_df.head())
        
        st.subheader("2-2. ì£¼(Week) ë‹¨ìœ„ ë°ì´í„°ë¡œ ì§‘ê³„")
        if not filtered_trade_df.empty:
            filtered_trade_df.set_index('Date', inplace=True)
            value_col = 'Value'
            volume_col = 'Volume'
            
            trade_weekly = filtered_trade_df.resample('W-Mon').agg({value_col: 'sum', volume_col: 'sum'})
            trade_weekly['ìˆ˜ì…ë‹¨ê°€_USD_KG'] = trade_weekly[value_col] / trade_weekly[volume_col]
            trade_weekly.columns = ['ìˆ˜ì…ì•¡_USD', 'ìˆ˜ì…ëŸ‰_KG', 'ìˆ˜ì…ë‹¨ê°€_USD_KG']
            
            wholesale_weekly = pd.DataFrame()
            if not raw_wholesale_df.empty:
                date_col = 'ì¡°ì‚¬ì¼ì' if 'ì¡°ì‚¬ì¼ì' in raw_wholesale_df.columns else 'ë‚ ì§œ'
                raw_wholesale_df[date_col] = pd.to_datetime(raw_wholesale_df[date_col], errors='coerce')
                wholesale_df_processed = raw_wholesale_df.set_index(date_col)
                price_cols = [col for col in wholesale_df_processed.columns if 'ê°€ê²©' in col]
                agg_dict = {col: 'mean' for col in price_cols}
                if agg_dict:
                    wholesale_weekly = wholesale_df_processed.resample('W-Mon').agg(agg_dict)
                    if 'ë„ë§¤ê°€ê²©_ì›' in wholesale_weekly.columns:
                        wholesale_weekly['ë„ë§¤ê°€ê²©_USD'] = wholesale_weekly['ë„ë§¤ê°€ê²©_ì›'] / 1350
                        wholesale_weekly.drop(columns=['ë„ë§¤ê°€ê²©_ì›'], inplace=True)
            
            search_weekly = pd.DataFrame()
            if not raw_search_df.empty:
                raw_search_df['ë‚ ì§œ'] = pd.to_datetime(raw_search_df['ë‚ ì§œ'], errors='coerce')
                search_df_processed = raw_search_df.set_index('ë‚ ì§œ')
                numeric_cols = search_df_processed.select_dtypes(include=np.number).columns
                search_weekly = search_df_processed.resample('W-Mon').agg({col: 'mean' for col in numeric_cols})
            
            st.write("â–¼ ì¼ë³„(Daily) vs ì£¼ë³„(Weekly) ìˆ˜ì…ëŸ‰ ë¹„êµ")
            col1, col2 = st.columns(2)
            with col1: st.line_chart(filtered_trade_df[volume_col])
            with col2: st.line_chart(trade_weekly['ìˆ˜ì…ëŸ‰_KG'])
        else: st.warning("ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

with tab3:
    if 'trade_weekly' in locals() and not trade_weekly.empty:
        dfs_to_concat = [trade_weekly]
        if 'wholesale_weekly' in locals() and not wholesale_weekly.empty: dfs_to_concat.append(wholesale_weekly)
        if 'search_weekly' in locals() and not search_weekly.empty: dfs_to_concat.append(search_weekly)
        
        final_df = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='outer'), dfs_to_concat)
        final_df = final_df.interpolate(method='linear', limit_direction='forward').dropna(how='all')
        
        st.dataframe(final_df)
        st.subheader("ìµœì¢… í†µí•© ë°ì´í„° ì‹œê°í™”")
        if not final_df.empty:
            fig = px.line(final_df, labels={'value': 'ê°’', 'index': 'ë‚ ì§œ', 'variable': 'ë°ì´í„° ì¢…ë¥˜'}, title="ìµœì¢… í†µí•© ë°ì´í„° ì‹œê³„ì—´ ì¶”ì´")
            st.plotly_chart(fig, use_container_width=True)
    else: st.warning("í†µí•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

