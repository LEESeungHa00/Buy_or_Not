import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
import requests
from datetime import datetime, timedelta, timezone
from functools import reduce
import json
import urllib.request
from google.oauth2 import service_account
from google.cloud import bigquery
import pandas_gbq
import feedparser
from urllib.parse import quote
from prophet import Prophet
from prophet.plot import plot_plotly, plot_components_plotly
from prophet.diagnostics import cross_validation, performance_metrics
from kamis_data import KAMIS_FULL_DATA
import yfinance as yf
import itertools

# Advanced Analysis Libraries
from scipy.stats import pearsonr, spearmanr
from statsmodels.tsa.stattools import adfuller
from statsmodels.stats.multitest import multipletests
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import MinMaxScaler

# Transformers / HuggingFace
from transformers import pipeline
from huggingface_hub import login as hf_login

# Optional: robust article text fetch
try:
    from newspaper import Article, Config as NewsConfig
    _HAS_NEWSPAPER = True
except ImportError:
    _HAS_NEWSPAPER = False

# ----------------------------
#  Configuration / Constants
# ----------------------------
st.set_page_config(layout="wide")
st.title("ğŸ“Š í†µí•© ë°ì´í„° ê¸°ë°˜ íƒìƒ‰ ë° ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ")

BQ_DATASET = "data_explorer"
BQ_TABLE_TRADE = "tds_data"
BQ_TABLE_NAVER = "naver_trends_cache"
BQ_TABLE_NEWS = "news_sentiment_finbert"

COMMODITY_TICKERS = {
    "Coffee": "KC=F", "Cocoa": "CC=F", "Orange Juice": "OJ=F",
    "Crude Oil (WTI)": "CL=F", "Gold": "GC=F", "Corn": "ZC=F",
    "Soybeans": "ZS=F", "Wheat": "ZW=F", "Natural Gas": "NG=F", "Copper": "HG=F",
}

DEFAULT_MODEL_IDS = {
    "finbert": "snunlp/KR-FinBERT-SC",
    "elite": "nlptown/bert-base-multilingual-uncased-sentiment",
    "product": "cardiffnlp/twitter-xlm-roberta-base-sentiment"
}

# --- Guide Content ---
guide_content = """
### 1. ìƒê´€ê´€ê³„ ë¶„ì„ íƒ­
- **ë¶„ì„ ë°©ë²• ì„ íƒ (Pearson vs Spearman)**
    - **`Pearson`**: ë‘ ë³€ìˆ˜ê°€ **'ì„ í˜• ê´€ê³„'(ì •ë¹„ë¡€/ë°˜ë¹„ë¡€)**ì— ê°€ê¹Œìš¸ ë•Œ ì„ íƒí•©ë‹ˆë‹¤. (ì˜ˆ: ê´‘ê³ ë¹„ì™€ ë§¤ì¶œ)
    - **`Spearman`**: **'ìˆœìœ„ ê´€ê³„'(í•œìª½ì´ ì¦ê°€í•  ë•Œ ë‹¤ë¥¸ ìª½ë„ ì¼ê´€ë˜ê²Œ ì¦ê°€/ê°ì†Œ)**ë¥¼ ë³´ê³  ì‹¶ì„ ë•Œ ì„ íƒí•©ë‹ˆë‹¤. ë³µì¡í•œ ë¹„ì„ í˜• ê´€ê³„ë„ ì¡ì•„ë‚¼ ìˆ˜ ìˆì–´ ë” ë²”ìš©ì ì…ë‹ˆë‹¤.
- **P-value í•„í„°**: ì´ ê´€ê³„ê°€ **'ìš°ì—°ì¼ í™•ë¥ '**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. í†µê³„ì ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” ê´€ê³„ë¥¼ ì°¾ê¸° ìœ„í•´ ë³´í†µ **0.05 ì´í•˜**ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. (ì¦‰, ìš°ì—°ì¼ í™•ë¥ ì´ 5% ë¯¸ë§Œì¸ ê´€ê³„ë§Œ ë³´ê² ë‹¤ëŠ” ì˜ë¯¸)
- **ì‹œì°¨ ë¶„ì„ (ì„ í–‰/í›„í–‰ ë³€ìˆ˜)**
    - **`ì„ í–‰ ë³€ìˆ˜(Driver)`**: ì›ì¸ì´ ë  ìˆ˜ ìˆëŠ” ë³€ìˆ˜ ê·¸ë£¹ (ì˜ˆ: ë‰´ìŠ¤ ê°ì„±, ì„ ë¬¼ ê°€ê²©)
    - **`í›„í–‰ ë³€ìˆ˜(Outcome)`**: ê²°ê³¼ì ìœ¼ë¡œ ì˜í–¥ì„ ë°›ëŠ” ë³€ìˆ˜ ê·¸ë£¹ (ì˜ˆ: ìˆ˜ì…ëŸ‰, ìˆ˜ì…ë‹¨ê°€)
    - **ëª©ì **: "êµ­ì œ ìœ ê°€ê°€ ì˜¤ë¥´ë©´, ëª‡ ì£¼ í›„ì— ìˆ˜ì… ë‹¨ê°€ê°€ ìƒìŠ¹í• ê¹Œ?" ì™€ ê°™ì€ ì„ í›„ ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ë¯¸ë˜ ì˜ˆì¸¡ì˜ ë‹¨ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
- **ì‚°ì ë„ í–‰ë ¬**: ì—¬ëŸ¬ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ í•œëˆˆì— ì‹œê°ì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ë“¤ì´ ìš°ìƒí–¥í•˜ë©´ ì–‘ì˜ ê´€ê³„, ìš°í•˜í–¥í•˜ë©´ ìŒì˜ ê´€ê³„ë¥¼ ì˜ì‹¬í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

### 2. ì‹œê³„ì—´ ì˜ˆì¸¡ íƒ­
#### **ê°€. ì˜ˆì¸¡ íŒŒë¼ë¯¸í„°, ì–´ë–»ê²Œ ì„¤ì •í•´ì•¼ í• ê¹Œìš”?**
- **`ğŸ¤– ìµœì  íŒŒë¼ë¯¸í„° ìë™ íƒìƒ‰`**: ê°€ì¥ ë¨¼ì € ëˆŒëŸ¬ë³´ì„¸ìš”! ë°ì´í„°ì— ê°€ì¥ ì í•©í•œ íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ ìŠ¬ë¼ì´ë”ì™€ ì„ íƒ ë°•ìŠ¤ë¥¼ ì„¤ì •í•´ì£¼ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. ì´ ìµœì ì˜ ìƒíƒœì—ì„œë¶€í„° ë¯¸ì„¸ ì¡°ì •ì„ ì‹œì‘í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
- **`Trend ìœ ì—°ì„± (changepoint_prior_scale)`**
    - **ì—­í• **: ë°ì´í„°ì˜ ì¥ê¸°ì ì¸ ì¶”ì„¸(Trend)ê°€ ì–¼ë§ˆë‚˜ ê¸‰ê²©í•˜ê²Œ ë³€í•˜ëŠ”ì§€ë¥¼ ëª¨ë¸ì—ê²Œ ì•Œë ¤ì£¼ëŠ” ê°’ì…ë‹ˆë‹¤.
    - **ì„¤ì • ê°€ì´ë“œ**:
        - **ë‚®ì€ ê°’ (0.01 ~ 0.1)**: ì•ˆì •ì ì¸ ì‹œì¥. ì¶”ì„¸ê°€ ê±°ì˜ ë³€í•˜ì§€ ì•Šê±°ë‚˜ ì™„ë§Œí•˜ê²Œ ë³€í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. (ì˜ˆ: ìŒ€ ê°€ê²©)
        - **ë†’ì€ ê°’ (0.1 ~ 0.5)**: ì—­ë™ì ì¸ ì‹œì¥. ì‹ ì œí’ˆ ì¶œì‹œ, ì •ì±… ë³€í™” ë“±ìœ¼ë¡œ ì¶”ì„¸ê°€ ìì£¼, í¬ê²Œ êº¾ì¼ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. (ì˜ˆ: ìœ í–‰ì„± íŒ¨ì…˜ ì•„ì´í…œ)
- **`ê³„ì ˆì„± ê°•ë„ (seasonality_prior_scale)`**
    - **ì—­í• **: 1ë…„ ì£¼ê¸°, 1ì£¼ì¼ ì£¼ê¸° ë“± ë°˜ë³µë˜ëŠ” íŒ¨í„´(ê³„ì ˆì„±)ì„ ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ ë¯¿ì„ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    - **ì„¤ì • ê°€ì´ë“œ**:
        - **ë‚®ì€ ê°’ (0.1 ~ 1.0)**: ê³„ì ˆì„±ì´ ì•½í•˜ê±°ë‚˜ ë¶ˆê·œì¹™í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        - **ë†’ì€ ê°’ (1.0 ~ 10.0)**: ì—¬ë¦„íœ´ê°€, ì—°ë§ ì‡¼í•‘ ì‹œì¦Œì²˜ëŸ¼ ë§¤ë…„ ëšœë ·í•œ íŒ¨í„´ì´ ë°˜ë³µë  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- **`ê³„ì ˆì„± ëª¨ë“œ (seasonality_mode)`**
    - **ì—­í• **: ê³„ì ˆì„± íŒ¨í„´ì´ ì „ì²´ ì¶”ì„¸ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    - **`additive (ë§ì…ˆ ëª¨ë“œ)`**: ê³„ì ˆì„±ì˜ ë³€ë™í­ì´ ì¶”ì„¸ì™€ ìƒê´€ì—†ì´ **ì¼ì •í•  ë•Œ** ì‚¬ìš©í•©ë‹ˆë‹¤. (ì˜ˆ: ë§¤ë…„ ì—¬ë¦„ ë§¤ì¶œì´ ì•½ '1ì²œë§Œ ì›'ì”© ì¦ê°€)
    - **`multiplicative (ê³±ì…ˆ ëª¨ë“œ)`**: ê³„ì ˆì„±ì˜ ë³€ë™í­ì´ ì¶”ì„¸ì— ë”°ë¼ **í•¨ê»˜ ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§ˆ ë•Œ** ì‚¬ìš©í•©ë‹ˆë‹¤. (ì˜ˆ: ë¹„ì¦ˆë‹ˆìŠ¤ê°€ ì„±ì¥í•¨ì— ë”°ë¼, ë§¤ë…„ ì—¬ë¦„ ë§¤ì¶œì´ '10%'ì”© ì¦ê°€)

#### **ë‚˜. Prophet ì˜ˆì¸¡ ê²°ê³¼, ì–´ë–»ê²Œ í•´ì„í•´ì•¼ í• ê¹Œìš”?**
- **Prophet ì˜ˆì¸¡ ê·¸ë˜í”„**
    - **ì •í™•ë„ íŒë‹¨ ê¸°ì¤€**: "ì˜ˆì¸¡ì„ (íŒŒë€ì„ )ì´ ê³¼ê±°ì˜ ì‹¤ì œ ë°ì´í„°(ê²€ì€ ì )ì˜ ì „ë°˜ì ì¸ íë¦„ì„ ì˜ ë”°ë¼ê°€ëŠ”ê°€?"ê°€ 1ì°¨ì ì¸ ê¸°ì¤€ì…ë‹ˆë‹¤. íŠ¹íˆ, ìµœê·¼ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì˜ ë§ì¶”ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. 'ë¶ˆí™•ì‹¤ì„± êµ¬ê°„(í•˜ëŠ˜ìƒ‰ ì˜ì—­)'ì´ ë„ˆë¬´ ë„“ì§€ ì•Šê³ , ì‹¤ì œ ê°’ë“¤ì´ ëŒ€ë¶€ë¶„ ê·¸ ì•ˆì— ìˆë‹¤ë©´ ì‹ ë¢°í•  ë§Œí•œ ì˜ˆì¸¡ì…ë‹ˆë‹¤.
- **ìš”ì¸ ë¶„í•´ ê·¸ë˜í”„**
    - **`trend`**: ë°ì´í„°ì˜ ì¥ê¸°ì ì¸ ë°©í–¥ì„±ì…ë‹ˆë‹¤. ì´ ì„ ì´ ìš°ìƒí–¥í•˜ë©´ ì¥ê¸°ì ìœ¼ë¡œ ì‚¬ì—…ì´ ì„±ì¥í•˜ê³  ìˆìŒì„, ìš°í•˜í–¥í•˜ë©´ ì¶•ì†Œë˜ê³  ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
    - **`yearly` / `weekly`**: í•´ë‹¹ ì£¼ê¸°ì˜ 'ìˆœìˆ˜í•œ' ì˜í–¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, `yearly` ê·¸ë˜í”„ê°€ 7ì›”ì— ê°€ì¥ ë†’ë‹¤ë©´, ë‹¤ë¥¸ ëª¨ë“  ìš”ì¸ì„ ì œì™¸í•˜ê³ ë„ "7ì›”"ì´ë¼ëŠ” ì‹œì  ìì²´ê°€ ì˜ˆì¸¡ê°’ì„ ëŒì–´ì˜¬ë¦¬ëŠ” íš¨ê³¼ê°€ ìˆë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.
    - **`extra_regressors` (ì™¸ë¶€ ì˜ˆì¸¡ ë³€ìˆ˜)**: í•´ë‹¹ ì™¸ë¶€ ë³€ìˆ˜ê°€ ì˜ˆì¸¡ê°’ì— ë¯¸ì¹œ **'ì¶”ê°€ì ì¸ ì˜í–¥ë ¥'**ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì„ ë¬¼ ê°€ê²©(regressor) ê·¸ë˜í”„ê°€ ì–‘ìˆ˜(+) ê°’ì„ ë³´ì´ë©´, ê·¸ ì‹œì ì˜ ë†’ì€ ì„ ë¬¼ ê°€ê²©ì´ ìµœì¢… ì˜ˆì¸¡ê°’ì„ ëŒì–´ì˜¬ë¦¬ëŠ” ì—­í• ì„ í–ˆë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.

#### **ë‹¤. ëª¨ë¸ ì§„ë‹¨: ë‚´ ì˜ˆì¸¡ ëª¨ë¸, ë¯¿ì„ë§Œ í•œê°€ìš”?**
- **ADF Test**
    - **ì •ì˜**: ëª¨ë¸ì´ ë†“ì¹œ **"ì˜ˆì¸¡ ì˜¤ì°¨(ì”ì°¨)"ì— ì—¬ì „íˆ ìœ ì˜ë¯¸í•œ íŒ¨í„´ì´ ë‚¨ì•„ìˆëŠ”ì§€**ë¥¼ ê²€ì‚¬í•˜ëŠ” 'íŒ¨í„´ íƒì§€ê¸°'ì…ë‹ˆë‹¤.
    - **ì•ˆì •ì„± íŒë‹¨**: P-valueê°€ **`0.05` ë¯¸ë§Œ**ì´ë©´ "ì˜¤ì°¨ì— íŒ¨í„´ì´ ë‚¨ì•„ìˆì„ í™•ë¥ ì´ 5% ë¯¸ë§Œì´ë‹¤", ì¦‰ **"ì˜¤ì°¨ëŠ” ê±°ì˜ ë¬´ì‘ìœ„ì ì´ë¯€ë¡œ ëª¨ë¸ì´ ì•ˆì •ì ì´ë‹¤"**ë¼ê³  ê²°ë¡  ë‚´ë¦½ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ 0.05 ì´ìƒì´ë©´, ì•„ì§ ëª¨ë¸ì´ í•™ìŠµí•˜ì§€ ëª»í•œ íŒ¨í„´ì´ ë‚¨ì•„ìˆì–´ ê°œì„ ì´ í•„ìš”í•˜ë‹¤ëŠ” ì‹ í˜¸ì…ë‹ˆë‹¤.

#### **ë¼. ìµœì¢… ì˜ˆì¸¡ (XGBoost): ë” ê¹Šì€ ì¸ì‚¬ì´íŠ¸ ë°œê²¬í•˜ê¸°**
- **`RÂ² Score` (ì„¤ëª…ë ¥ / ê²°ì •ê³„ìˆ˜)**
    - **ì˜ë¯¸**: ëª¨ë¸ì´ **"ì–¼ë§ˆë‚˜ ë¯¸ë˜ë¥¼ ì˜ ì„¤ëª…í•˜ëŠ”ê°€?"**ë¥¼ 0~1 ì‚¬ì´ì˜ ì ìˆ˜ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. 0.7 ì´ë¼ëŠ” ê°’ì€, ìš°ë¦¬ ëª¨ë¸ì´ ë¯¸ë˜ ë³€ë™ì„±ì˜ 70%ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.
    - **ì˜ë¯¸ ìˆëŠ” ìˆ˜ì¤€**: ì¼ë°˜ì ìœ¼ë¡œ **0.6 ì´ìƒ**ì´ë©´ ì¤€ìˆ˜í•œ ëª¨ë¸, **0.8 ì´ìƒ**ì´ë©´ ë§¤ìš° ì¢‹ì€ ëª¨ë¸ë¡œ í‰ê°€í•©ë‹ˆë‹¤. **ìŒìˆ˜(-)ê°€ ë‚˜ì˜¤ë©´, ë‹¨ìˆœíˆ í‰ê· ê°’ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ê²ƒë³´ë‹¤ë„ ì„±ëŠ¥ì´ ë‚˜ì˜ë‹¤ëŠ” ìµœì•…ì˜ ì‹ í˜¸ì…ë‹ˆë‹¤.**
- **`RMSE` (í‰ê·  ì˜¤ì°¨)**
    - **ì˜ë¯¸**: **"ê·¸ë˜ì„œ ì˜ˆì¸¡ì´ í‰ê· ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ê°€?"**ë¥¼ ì‹¤ì œ ë‹¨ìœ„ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸ì…ë‹ˆë‹¤.
"""

# ----------------------------
#  Helper Functions (Analysis & Interpretation)
# ----------------------------
@st.cache_data
def calculate_advanced_correlation(df, method='pearson'):
    df_numeric = df.select_dtypes(include=np.number).dropna(how='all', axis=1)
    cols = df_numeric.columns
    corr_matrix = pd.DataFrame(np.ones((len(cols), len(cols))), index=cols, columns=cols)
    pval_matrix = pd.DataFrame(np.ones((len(cols), len(cols))), index=cols, columns=cols)
    pvalues_list = []
    for i in range(len(cols)):
        for j in range(i + 1, len(cols)):
            col1_data, col2_data = df_numeric[cols[i]].dropna(), df_numeric[cols[j]].dropna()
            common_index = col1_data.index.intersection(col2_data.index)
            if len(common_index) < 3: continue
            col1_data, col2_data = col1_data.loc[common_index], col2_data.loc[common_index]
            corr, pval = (pearsonr if method == 'pearson' else spearmanr)(col1_data, col2_data)
            corr_matrix.iloc[i, j] = corr_matrix.iloc[j, i] = corr
            pval_matrix.iloc[i, j] = pval_matrix.iloc[j, i] = pval
            pvalues_list.append(pval)
    if pvalues_list:
        _, pvals_corrected, _, _ = multipletests(pvalues_list, alpha=0.05, method='fdr_bh')
        k = 0
        for i in range(len(cols)):
            for j in range(i + 1, len(cols)):
                pval_matrix.iloc[i, j] = pval_matrix.iloc[j, i] = pvals_corrected[k]; k += 1
    return corr_matrix, pval_matrix

@st.cache_data
def find_best_lagged_correlation(df, driver_vars, outcome_vars, max_lag=12):
    best_correlations = []
    for driver in driver_vars:
        for outcome in outcome_vars:
            if driver == outcome: continue
            max_corr_val, best_lag = 0, 0
            for lag in range(-max_lag, max_lag + 1):
                try:
                    corr = df[driver].shift(lag).corr(df[outcome])
                    if pd.notna(corr) and abs(corr) > abs(max_corr_val):
                        max_corr_val, best_lag = corr, lag
                except Exception: continue
            if best_lag != 0:
                best_correlations.append({'Driver (X)': driver, 'Outcome (Y)': outcome, 'Best Lag (Weeks)': best_lag, 'Correlation': max_corr_val})
    if not best_correlations: return pd.DataFrame()
    df_lags = pd.DataFrame(best_correlations)
    df_lags['Abs Correlation'] = df_lags['Correlation'].abs()
    return df_lags.sort_values('Abs Correlation', ascending=False).drop(columns=['Abs Correlation'])

def interpret_correlation(corr_matrix, pval_matrix, threshold=0.05):
    strong_corrs = []
    driver_keywords = ['news', 'naver', 'sentiment', 'ê°€ê²©', 'futures']
    outcome_keywords = ['ìˆ˜ì…', 'volume']
    driver_cols = [c for c in corr_matrix.columns if any(kw in c.lower() for kw in driver_keywords)]
    outcome_cols = [c for c in corr_matrix.columns if any(kw in c.lower() for kw in outcome_keywords)]
    for driver in driver_cols:
        for outcome in outcome_cols:
            if driver == outcome or outcome not in corr_matrix.index or driver not in corr_matrix.columns: continue
            corr_val, pval = corr_matrix.loc[driver, outcome], pval_matrix.loc[driver, outcome]
            if pval < threshold and abs(corr_val) >= 0.4:
                direction = "ì–‘ì˜" if corr_val > 0 else "ìŒì˜"
                interpretation = f"**'{driver}'**ì™€(ê³¼) **'{outcome}'** ì‚¬ì´ì—ëŠ” í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ **{direction} ìƒê´€ê´€ê³„**ê°€ ìˆìŠµë‹ˆë‹¤ (ìƒê´€ê³„ìˆ˜: {corr_val:.2f}, p-value: {pval:.3f})."
                strong_corrs.append({'text': interpretation, 'value': abs(corr_val)})
    if not strong_corrs: return "Driver(ê°ì„±, ê²€ìƒ‰ëŸ‰ ë“±)ì™€ Outcome(ìˆ˜ì…ëŸ‰, ê¸ˆì•¡ ë“±) ë³€ìˆ˜ ê·¸ë£¹ ê°„ì— í†µê³„ì ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” ê°•í•œ ê´€ê³„ëŠ” ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    strong_corrs = sorted(strong_corrs, key=lambda x: x['value'], reverse=True)
    summary = "### ì£¼ìš” ë°œê²¬ (Driver vs Outcome):\n\n" + "\n".join([f"- {corr['text']}" for corr in strong_corrs[:5]])
    summary += "\n\n* **ì–‘ì˜ ìƒê´€ê´€ê³„ (+):** Driver ë³€ìˆ˜ê°€ ì¦ê°€í•  ë•Œ Outcome ë³€ìˆ˜ë„ í•¨ê»˜ ì¦ê°€í•˜ëŠ” ê²½í–¥ì„ ë³´ì…ë‹ˆë‹¤."
    summary += "\n* **ìŒì˜ ìƒê´€ê´€ê³„ (-):** Driver ë³€ìˆ˜ê°€ ì¦ê°€í•  ë•Œ Outcome ë³€ìˆ˜ëŠ” ì˜¤íˆë ¤ ê°ì†Œí•˜ëŠ” ê²½í–¥ì„ ë³´ì…ë‹ˆë‹¤."
    return summary

# ----------------------------
#  Helpers: Data Fetching & Processing
# ----------------------------
@st.cache_resource
def get_bq_connection():
    try:
        creds_dict = st.secrets["gcp_service_account"]
        creds = service_account.Credentials.from_service_account_info(creds_dict)
        return bigquery.Client(credentials=creds, project=creds.project_id)
    except Exception as e:
        st.error(f"Google BigQuery ì—°ê²° ì‹¤íŒ¨: {e}"); return None

def upload_df_to_bq(client, df, table_id):
    try:
        pandas_gbq.to_gbq(df, f"{BQ_DATASET}.{table_id}", project_id=client.project, if_exists="append", credentials=client._credentials)
        return True, None
    except Exception as e: return False, str(e)

@st.cache_resource
def load_models(_model_ids, token):
    models = {}
    for key, mid in _model_ids.items():
        try:
            models[key] = pipeline("sentiment-analysis", model=mid, tokenizer=mid, token=token)
        except Exception: models[key] = None
    return models

@st.cache_data(ttl=3600)
def get_news_with_multi_model_analysis(_bq_client, _models_dict, keyword):
    all_news = []
    rss_url = f"https://news.google.com/rss/search?q={quote(keyword)}&hl=ko&gl=KR&ceid=KR:ko"
    feed = feedparser.parse(rss_url)
    for entry in feed.entries[:50]:
        title = entry.get('title', '').strip()
        if not title: continue
        pub_date = pd.to_datetime(entry.get('published')).date() if 'published' in entry else datetime.utcnow().date()
        all_news.append({"ë‚ ì§œ": pub_date, "Title": title, "ModelInput": title})
    if not all_news: return pd.DataFrame()
    df_new = pd.DataFrame(all_news).drop_duplicates(subset=["ModelInput"])
    with st.spinner(f"'{keyword}' ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì¤‘..."):
        def _label_score_to_signed(pred):
            if not pred: return 0.0
            lbl, score = str(pred.get("label", "")).lower(), float(pred.get("score", 0.0))
            if "star" in lbl:
                try: n = int(lbl.split()[0]); return (n - 3) / 2.0
                except (ValueError, IndexError): return 0.0
            if any(x in lbl for x in ["neg", "negative", "ë¶€ì •"]): return -score
            if any(x in lbl for x in ["pos", "positive", "ê¸ì •"]): return score
            return 0.0
        preds = {key: (model(df_new['ModelInput'].tolist(), truncation=True, max_length=512) if model else [None]*len(df_new)) for key, model in _models_dict.items()}
        multi = []
        for i in range(len(df_new)):
            multi.append({
                "News_FinBERT": _label_score_to_signed(preds.get("finbert", [])[i]),
                "News_Elite": _label_score_to_signed(preds.get("elite", [])[i]),
                "News_Product": _label_score_to_signed(preds.get("product", [])[i])
            })
    return pd.concat([df_new.reset_index(drop=True), pd.DataFrame(multi)], axis=1).drop(columns=['ModelInput'])

@st.cache_data(ttl=3600)
def get_categories_from_bq(_client):
    try:
        query = f"SELECT DISTINCT Category FROM `{_client.project}.{BQ_DATASET}.{BQ_TABLE_TRADE}` ORDER BY Category"
        return sorted(_client.query(query).to_dataframe()['Category'].astype(str).unique())
    except Exception: return []

def get_trade_data_from_bq(client, categories):
    if not categories: return pd.DataFrame()
    try:
        sql = f"SELECT * FROM `{client.project}.{BQ_DATASET}.{BQ_TABLE_TRADE}` WHERE Category IN UNNEST(@categories)"
        job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ArrayQueryParameter("categories", "STRING", categories)])
        df = client.query(sql, job_config=job_config).to_dataframe()
        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
        return df
    except Exception as e: st.error(f"BigQuery TDS ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}"); return pd.DataFrame()

@st.cache_data(ttl=3600)
def fetch_yfinance_data(ticker_name, ticker_symbol, start_date, end_date):
    data_frames = []
    current_start = start_date
    while current_start < end_date:
        current_end = current_start + timedelta(days=365 * 2)
        if current_end > end_date: current_end = end_date
        try:
            data = yf.download(ticker_symbol, start=current_start, end=current_end, progress=False)
            if not data.empty: data_frames.append(data)
        except Exception as e:
            st.sidebar.error(f"Yahoo Finance ë°ì´í„° ì¼ë¶€ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        current_start = current_end + timedelta(days=1)
    if not data_frames:
        st.sidebar.warning(f"'{ticker_name}'ì— ëŒ€í•œ ì„ ë¬¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ê°„ì´ë‚˜ í‹°ì»¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return pd.DataFrame()
    full_data = pd.concat(data_frames)
    df = full_data[['Close']].rename(columns={'Close': f'Futures_{ticker_name}'})
    df.index.name = 'ë‚ ì§œ'
    return df

@st.cache_data(ttl=3600)
def fetch_kamis_data(item_info, start_date, end_date, kamis_keys):
    start_str, end_str = start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')
    url = (f"http://www.kamis.or.kr/service/price/xml.do?action=periodWholesaleProductList"
           f"&p_product_cls_code=01&p_startday={start_str}&p_endday={end_str}"
           f"&p_item_category_code={item_info['cat_code']}&p_item_code={item_info['item_code']}&p_kind_code={item_info['kind_code']}"
           f"&p_product_rank_code={item_info['rank_code']}&p_convert_kg_yn=Y"
           f"&p_cert_key={kamis_keys['key']}&p_cert_id={kamis_keys['id']}&p_returntype=json")
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200 and "data" in response.json():
            price_data = response.json().get("data", {}).get("item", [])
            if not price_data: return pd.DataFrame()
            df_new = pd.DataFrame(price_data)[['regday', 'price']].rename(columns={'regday': 'ë‚ ì§œ', 'price': 'ë„ë§¤ê°€ê²©_ì›'})
            df_new['ë‚ ì§œ'] = pd.to_datetime(df_new['ë‚ ì§œ'].str.replace('/', '-'))
            df_new['ë„ë§¤ê°€ê²©_ì›'] = pd.to_numeric(df_new['ë„ë§¤ê°€ê²©_ì›'].str.replace(',', ''))
            return df_new
    except Exception: return pd.DataFrame()

def call_naver_api(url, body, naver_keys):
    try:
        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", naver_keys['id'])
        request.add_header("X-Naver-Client-Secret", naver_keys['secret'])
        request.add_header("Content-Type", "application/json")
        response = urllib.request.urlopen(request, data=body.encode("utf-8"))
        if response.getcode() == 200:
            return json.loads(response.read().decode('utf-8'))
        return None
    except Exception as e:
        st.sidebar.error(f"Naver API ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

@st.cache_data(ttl=3600)
def fetch_naver_trends_data(_client, keywords, start_date, end_date, naver_keys):
    if not naver_keys.get('id') or not naver_keys.get('secret'): return pd.DataFrame()
    all_data = []
    for keyword in keywords:
        body = json.dumps({
            "startDate": start_date.strftime('%Y-%m-%d'), "endDate": end_date.strftime('%Y-%m-%d'),
            "timeUnit": "date", "keywordGroups": [{"groupName": keyword, "keywords": [keyword]}]
        })
        response = call_naver_api("https://openapi.naver.com/v1/datalab/search", body, naver_keys)
        if response and response.get('results'):
            data = response['results'][0]['data']
            if data:
                df_keyword = pd.DataFrame(data).rename(columns={'period': 'ë‚ ì§œ', 'ratio': f'Naver_{keyword}'})
                df_keyword['ë‚ ì§œ'] = pd.to_datetime(df_keyword['ë‚ ì§œ'])
                all_data.append(df_keyword)
    if not all_data:
        st.sidebar.warning(f"Naver Trendsì—ì„œ '{','.join(keywords)}' ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    return reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), all_data) if len(all_data) > 1 else all_data[0]

@st.cache_data
def find_best_prophet_params(_df, _regressors):
    param_grid = {
        'changepoint_prior_scale': [0.01, 0.05, 0.1, 0.5],
        'seasonality_prior_scale': [0.1, 1.0, 10.0],
        'seasonality_mode': ['additive', 'multiplicative'],
    }
    all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]
    rmses = []
    initial_days = str(int(len(_df) * 0.5)) + ' days'
    period_days = str(int(len(_df) * 0.2)) + ' days'
    horizon_days = str(int(len(_df) * 0.2)) + ' days'
    if int(len(_df) * 0.2) < 30 : return all_params[1]
    for params in all_params:
        try:
            m = Prophet(**params)
            for reg in _regressors: m.add_regressor(reg)
            m.fit(_df)
            df_cv = cross_validation(m, initial=initial_days, period=period_days, horizon=horizon_days, parallel="processes")
            df_p = performance_metrics(df_cv, rolling_window=1)
            rmses.append(df_p['rmse'].values[0])
        except Exception: rmses.append(float('inf'))
    best_params = all_params[np.argmin(rmses)]
    return best_params

# ----------------------------
#  App Main Logic
# ----------------------------
bq_client = get_bq_connection()
if bq_client is None: st.stop()
hf_token = st.secrets.get("huggingface", {}).get("token")
models = load_models(DEFAULT_MODEL_IDS, hf_token)
if 'final_df' not in st.session_state: st.session_state.final_df = pd.DataFrame()
if 'raw_news_df' not in st.session_state: st.session_state.raw_news_df = pd.DataFrame()
if 'best_params' not in st.session_state: st.session_state.best_params = {}

st.sidebar.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
categories = get_categories_from_bq(bq_client)
selected_categories = st.sidebar.multiselect("ë¶„ì„í•  í’ˆëª© ì„ íƒ", categories, default=categories[:1] if categories else [])
start_date = pd.to_datetime(st.sidebar.date_input('ì‹œì‘ì¼', datetime(2022, 1, 1)))
end_date = pd.to_datetime(st.sidebar.date_input('ì¢…ë£Œì¼', datetime.now()))
news_keyword_input = st.sidebar.text_input("ë‰´ìŠ¤ ë¶„ì„ í‚¤ì›Œë“œ", selected_categories[0] if selected_categories else "")
naver_keywords_input = st.sidebar.text_input("ë„¤ì´ë²„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ (ì‰¼í‘œ êµ¬ë¶„)", selected_categories[0] if selected_categories else "")
with st.sidebar.expander("ğŸ”‘ API í‚¤ ì…ë ¥ (ì„ íƒ)"):
    naver_client_id = st.text_input("Naver API Client ID", type="password")
    naver_client_secret = st.text_input("Naver API Client Secret", type="password")
    kamis_api_key = st.text_input("KAMIS API Key", type="password")
    kamis_api_id = st.text_input("KAMIS API ID", type="password")
st.sidebar.subheader("ğŸŒ ì™¸ë¶€ ê°€ê²© ë°ì´í„° ì†ŒìŠ¤")
price_source = st.sidebar.radio("ê°€ê²© ì†ŒìŠ¤ ì„ íƒ", ["KAMIS êµ­ë‚´ ë„ë§¤ê°€ê²©", "Yahoo Finance ì„ ë¬¼ ê°€ê²©"])
if price_source == "KAMIS êµ­ë‚´ ë„ë§¤ê°€ê²©":
    kamis_item_name = st.sidebar.selectbox("í’ˆëª© ì„ íƒ", list(KAMIS_FULL_DATA.keys()))
    kamis_kind_name = st.sidebar.selectbox("í’ˆì¢… ì„ íƒ", list(KAMIS_FULL_DATA[kamis_item_name]['kinds'].keys())) if kamis_item_name else ""
else: selected_commodity = st.sidebar.selectbox("ì„ ë¬¼ í’ˆëª© ì„ íƒ", list(COMMODITY_TICKERS.keys()))

if st.sidebar.button("ğŸš€ ëª¨ë“  ë°ì´í„° í†µí•© ë° ë¶„ì„ ì‹¤í–‰"):
    with st.spinner("ë°ì´í„° í†µí•© ë° ë¶„ì„ ì¤‘..."):
        trade_df = get_trade_data_from_bq(bq_client, selected_categories)
        news_df = get_news_with_multi_model_analysis(bq_client, models, news_keyword_input)
        st.session_state.raw_news_df = news_df
        naver_keys = {'id': naver_client_id, 'secret': naver_client_secret}
        naver_keywords = [k.strip() for k in naver_keywords_input.split(',') if k.strip()]
        naver_df = fetch_naver_trends_data(bq_client, naver_keywords, start_date, end_date, naver_keys)
        external_price_df = pd.DataFrame()
        if price_source == "KAMIS êµ­ë‚´ ë„ë§¤ê°€ê²©":
            if kamis_item_name and kamis_kind_name and kamis_api_key and kamis_api_id:
                kamis_keys = {'key': kamis_api_key, 'id': kamis_api_id}
                item_info = KAMIS_FULL_DATA[kamis_item_name].copy()
                item_info['kind_code'] = item_info['kinds'][kamis_kind_name]; item_info['rank_code'] = '01'
                external_price_df = fetch_kamis_data(item_info, start_date, end_date, kamis_keys)
        else:
            if selected_commodity:
                ticker = COMMODITY_TICKERS[selected_commodity]
                external_price_df = fetch_yfinance_data(selected_commodity, ticker, start_date, end_date)
        trade_weekly = trade_df.set_index('Date').resample('W-Mon').agg(ìˆ˜ì…ì•¡_USD=('Value', 'sum'), ìˆ˜ì…ëŸ‰_KG=('Volume', 'sum')).copy()
        trade_weekly['ìˆ˜ì…ë‹¨ê°€_USD_KG'] = trade_weekly['ìˆ˜ì…ì•¡_USD'] / trade_weekly['ìˆ˜ì…ëŸ‰_KG']
        dfs_to_merge = [trade_weekly]
        if not news_df.empty:
            news_df['ë‚ ì§œ'] = pd.to_datetime(news_df['ë‚ ì§œ'])
            dfs_to_merge.append(news_df.drop(columns=['Title']).set_index('ë‚ ì§œ').resample('W-Mon').mean())
        if not naver_df.empty:
            naver_df['ë‚ ì§œ'] = pd.to_datetime(naver_df['ë‚ ì§œ'])
            dfs_to_merge.append(naver_df.set_index('ë‚ ì§œ').resample('W-Mon').mean())
        if not external_price_df.empty:
            if 'ë‚ ì§œ' in external_price_df.columns:
                external_price_df['ë‚ ì§œ'] = pd.to_datetime(external_price_df['ë‚ ì§œ'])
                external_price_df = external_price_df.set_index('ë‚ ì§œ')
            dfs_to_merge.append(external_price_df.resample('W-Mon').mean())
        final_df = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='outer'), dfs_to_merge)
        final_df = final_df.interpolate(method='time').fillna(method='bfill').fillna(method='ffill')
        st.session_state.final_df = final_df.replace([np.inf, -np.inf], np.nan).dropna()
        st.session_state.best_params = {}
        st.success("ë°ì´í„° í†µí•© ì™„ë£Œ!")

if not st.session_state.final_df.empty:
    final_df = st.session_state.final_df
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„", "ğŸ“ˆ ì‹œê³„ì—´ ì˜ˆì¸¡", "ğŸ“„ í†µí•© ë°ì´í„°", "ğŸ“° ìˆ˜ì§‘ ë‰´ìŠ¤ ì›ë³¸", "ğŸ“˜ ëŒ€ì‹œë³´ë“œ ì‚¬ìš©ë²•"])
    with tab1:
        st.header("ìƒê´€ê´€ê³„ ë¶„ì„")
        col1, col2 = st.columns(2)
        corr_method = col1.selectbox("ìƒê´€ê´€ê³„ ë¶„ì„ ë°©ë²•", ('pearson', 'spearman'), key="corr_method")
        pval_threshold = col2.slider("ìœ ì˜ìˆ˜ì¤€ (P-value) í•„í„°", 0.0, 1.0, 0.05, key="pval_slider")
        corr_matrix, pval_matrix = calculate_advanced_correlation(final_df, method=corr_method)
        st.subheader(f"'{corr_method.capitalize()}' ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ")
        fig_heatmap = go.Figure(data=go.Heatmap(z=corr_matrix.values, x=corr_matrix.columns, y=corr_matrix.columns, colorscale='RdBu_r', zmin=-1, zmax=1, text=corr_matrix.round(2).astype(str), texttemplate="%{text}"))
        st.plotly_chart(fig_heatmap, use_container_width=True)
        with st.expander("ğŸ” íˆíŠ¸ë§µ ê²°ê³¼ í•´ì„í•˜ê¸°"):
            st.markdown(interpret_correlation(corr_matrix, pval_matrix, threshold=pval_threshold))
        st.markdown("---")
        st.subheader("ì‹œì°¨ êµì°¨ìƒê´€ ë¶„ì„")
        driver_cols = st.multiselect("ì„ í–‰ ë³€ìˆ˜ (Driver)", final_df.columns, default=[c for c in final_df if any(kw in c.lower() for kw in ['news', 'naver', 'ê°€ê²©', 'futures'])])
        outcome_cols = st.multiselect("í›„í–‰ ë³€ìˆ˜ (Outcome)", final_df.columns, default=[c for c in final_df if 'ìˆ˜ì…' in c])
        if driver_cols and outcome_cols:
            lag_df = find_best_lagged_correlation(final_df, driver_cols, outcome_cols)
            st.dataframe(lag_df.head(10))
            if not lag_df.empty:
                st.info(f"ê°€ì¥ ê°•í•œ ì‹œì°¨ ê´€ê³„: **{lag_df.iloc[0]['Driver (X)']}**ì˜ ë³€í™”ëŠ” **{lag_df.iloc[0]['Best Lag (Weeks)']}ì£¼ í›„** **{lag_df.iloc[0]['Outcome (Y)']}**ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤ (ìƒê´€ê³„ìˆ˜: {lag_df.iloc[0]['Correlation']:.3f}).")
        st.markdown("---")
        st.subheader("ì‚°ì ë„ í–‰ë ¬ (Scaled for Visualization)")
        scaler = MinMaxScaler()
        scaled_df = pd.DataFrame(scaler.fit_transform(final_df), columns=final_df.columns, index=final_df.index)
        dims = st.multiselect("ì‚°ì ë„ í‘œì‹œ ë³€ìˆ˜", scaled_df.columns, default=list(scaled_df.columns[:8]))
        if dims:
            st.plotly_chart(px.scatter_matrix(scaled_df[dims]), use_container_width=True)
    with tab2:
        st.header("ì‹œê³„ì—´ ì˜ˆì¸¡ (Prophet & XGBoost)")
        prophet_df = final_df.reset_index().rename(columns={final_df.index.name if final_df.index.name else 'index': 'ds'})
        col1, col2 = st.columns(2)
        forecast_col = col1.selectbox("ì˜ˆì¸¡ ëŒ€ìƒ ë³€ìˆ˜ (y)", final_df.columns, key="forecast_col")
        forecast_periods = col2.number_input("ì˜ˆì¸¡ ê¸°ê°„ (ì£¼)", 4, 52, 12, key="forecast_periods")
        prophet_df = prophet_df.rename(columns={forecast_col: 'y'})
        regressors = [c for c in prophet_df.columns if c not in ['ds', 'y']]
        selected_regressors = st.multiselect("ì™¸ë¶€ ì˜ˆì¸¡ ë³€ìˆ˜ (Regressors)", regressors, default=regressors, key="regressors")
        st.subheader("Prophet ëª¨ë¸ íŒŒë¼ë¯¸í„° íŠœë‹")
        if st.button("ğŸ¤– ìµœì  íŒŒë¼ë¯¸í„° ìë™ íƒìƒ‰"):
            with st.spinner("êµì°¨ ê²€ì¦ì„ í†µí•´ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ íƒìƒ‰ ì¤‘ì…ë‹ˆë‹¤... (ë°ì´í„° ì–‘ì— ë”°ë¼ 1~2ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
                best_params = find_best_prophet_params(prophet_df[['ds', 'y'] + selected_regressors], selected_regressors)
                st.session_state.best_params = best_params
                st.success("ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰ ì™„ë£Œ!")
        bp = st.session_state.best_params
        p_col1, p_col2, p_col3 = st.columns(3)
        changepoint_prior_scale = p_col1.slider("Trend ìœ ì—°ì„±", 0.01, 0.5, bp.get('changepoint_prior_scale', 0.05), key="cps")
        seasonality_prior_scale = p_col2.slider("ê³„ì ˆì„± ê°•ë„", 0.01, 10.0, bp.get('seasonality_prior_scale', 1.0), key="sps")
        seasonality_mode_options = ['additive', 'multiplicative']
        seasonality_mode_index = seasonality_mode_options.index(bp.get('seasonality_mode', 'additive'))
        seasonality_mode = p_col3.selectbox("ê³„ì ˆì„± ëª¨ë“œ", seasonality_mode_options, index=seasonality_mode_index, key="sm")
        if st.button("ğŸš€ ì˜ˆì¸¡ ì‹¤í–‰", key="run_forecast"):
            m = Prophet(changepoint_prior_scale=changepoint_prior_scale, seasonality_prior_scale=seasonality_prior_scale, seasonality_mode=seasonality_mode)
            for reg in selected_regressors: m.add_regressor(reg)
            m.fit(prophet_df[['ds', 'y'] + selected_regressors])
            future = m.make_future_dataframe(periods=forecast_periods, freq='W')
            future_regressors = prophet_df[['ds'] + selected_regressors].set_index('ds'); last_values = future_regressors.iloc[-1]; future_regressors = future_regressors.reindex(future['ds']).fillna(method='ffill').fillna(last_values); future = pd.concat([future.set_index('ds'), future_regressors], axis=1).reset_index()
            forecast = m.predict(future)
            st.subheader("Prophet ì˜ˆì¸¡ ê²°ê³¼"); st.plotly_chart(plot_plotly(m, forecast), use_container_width=True)
            st.subheader("Prophet ìš”ì¸ ë¶„í•´"); st.plotly_chart(plot_components_plotly(m, forecast), use_container_width=True)
            st.markdown("---"); st.subheader("ëª¨ë¸ ì§„ë‹¨: ì”ì°¨ ë¶„ì„")
            df_pred = forecast.set_index('ds')[['yhat']].join(prophet_df.set_index('ds')[['y']]).dropna(); residuals = df_pred['y'] - df_pred['yhat']
            diag_col1, diag_col2 = st.columns(2)
            with diag_col1:
                st.markdown("**ì”ì°¨ ì •ìƒì„± ê²€ì • (ADF Test)**"); adf_result = adfuller(residuals); st.write(f"p-value: {adf_result[1]:.4f}")
            with diag_col2:
                st.markdown("**ì”ì°¨ ë¶„í¬**"); st.plotly_chart(ff.create_distplot([residuals], ['residuals'], bin_size=.2, show_rug=False), use_container_width=True)
            st.markdown("---"); st.subheader("ê³ ê¸‰ ì˜ˆì¸¡: XGBoost Meta-Forecasting")
            ml_df = forecast[['ds', 'trend']].set_index('ds'); available_seasonal_components = []
            if 'yearly' in forecast.columns: ml_df = ml_df.join(forecast[['ds', 'yearly']].set_index('ds')); available_seasonal_components.append('yearly')
            if 'weekly' in forecast.columns: ml_df = ml_df.join(forecast[['ds', 'weekly']].set_index('ds')); available_seasonal_components.append('weekly')
            ml_df = ml_df.join(prophet_df.set_index('ds')).dropna()
            X = ml_df[['trend'] + available_seasonal_components + selected_regressors]; y = ml_df['y']
            train_size = int(len(X) * 0.85); X_train, X_test, y_train, y_test = X.iloc[:train_size], X.iloc[train_size:], y.iloc[:train_size], y.iloc[train_size:]
            xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=1000, learning_rate=0.01, early_stopping_rounds=50); xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
            y_pred_xgb = xgb_model.predict(X_test); r2, rmse = r2_score(y_test, y_pred_xgb), np.sqrt(mean_squared_error(y_test, y_pred_xgb))
            st.metric("XGBoost Test RÂ² Score", f"{r2:.3f}"); st.metric("XGBoost Test RMSE", f"{rmse:.3f}")
            fig_xgb = go.Figure(); fig_xgb.add_trace(go.Scatter(x=y_train.index, y=y_train, name='Train')); fig_xgb.add_trace(go.Scatter(x=y_test.index, y=y_test, name='Test (Actual)')); fig_xgb.add_trace(go.Scatter(x=y_test.index, y=y_pred_xgb, name='XGBoost Prediction'))
            st.plotly_chart(fig_xgb, use_container_width=True)
            feature_imp = pd.DataFrame(sorted(zip(xgb_model.feature_importances_, X.columns)), columns=['Value','Feature'])
            st.plotly_chart(px.bar(feature_imp, x="Value", y="Feature", orientation='h', title="Feature Importance"), use_container_width=True)
            with st.expander("ğŸ” XGBoost ì¢…í•© ê²°ê³¼ í•´ì„"): st.markdown(guide_content.split("`RÂ² Score`")[1])
    with tab3:
        st.header("í†µí•© ë°ì´í„° (ì£¼ë³„)")
        st.dataframe(final_df)
        st.download_button(label="CSVë¡œ ë‹¤ìš´ë¡œë“œ", data=final_df.to_csv(index=False).encode('utf-8-sig'), file_name="integrated_weekly_data.csv", mime='text/csv',)
    with tab4:
        st.header("ğŸ“° ìˆ˜ì§‘ ë‰´ìŠ¤ ì›ë³¸")
        if not st.session_state.raw_news_df.empty:
            st.dataframe(st.session_state.raw_news_df.sort_values(by='ë‚ ì§œ', ascending=False))
        else:
            st.info("ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„ì„ ì‹¤í–‰í•˜ë©´ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©ê³¼ ê°ì„± ì ìˆ˜ë¥¼ ì—¬ê¸°ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    with tab5:
        st.header("ğŸ“˜ ëŒ€ì‹œë³´ë“œ ì‚¬ìš©ë²• ê°€ì´ë“œ")
        st.markdown(guide_content, unsafe_allow_html=True)
else:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ë°ì´í„°ë¥¼ ì„ íƒí•˜ê³  'ëª¨ë“  ë°ì´í„° í†µí•© ë° ë¶„ì„ ì‹¤í–‰' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

