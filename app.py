import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import requests
from datetime import datetime, timedelta
from functools import reduce
from pytrends.request import TrendReq
import json
import urllib.request
import yfinance as yf

# --- Data Fetching and Caching Functions ---
@st.cache_data
def load_trade_data(uploaded_file):
    """
    ì—…ë¡œë“œëœ ìˆ˜ì¶œì… ë°ì´í„° íŒŒì¼ì„ ì½ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” ìºì‹œë˜ì–´ íŒŒì¼ ì¬ë¡œë”©ìœ¼ë¡œ ì¸í•œ ë”œë ˆì´ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    try:
        df = pd.read_csv(uploaded_file) if uploaded_file.name.endswith('.csv') else pd.read_excel(uploaded_file)
        if 'Date' not in df.columns or 'Category' not in df.columns:
            st.error("ì—…ë¡œë“œëœ íŒŒì¼ì— 'Date'ì™€ 'Category' ì»¬ëŸ¼ì´ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤.")
            return None
        df['Date'] = pd.to_datetime(df['Date'])
        return df
    except Exception as e:
        st.error(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

@st.cache_data(ttl=3600)
def fetch_yfinance_data(ticker, name, start_date, end_date):
    """Yahoo Financeì—ì„œ ì§€ì •ëœ í‹°ì»¤ì˜ ê³¼ê±° ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        data = yf.download(ticker, start=start_date, end=end_date, progress=False)
        if data.empty:
            st.warning(f"'{name}({ticker})'ì— ëŒ€í•œ Yahoo Finance ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        df = data[['Close']].copy()
        df.reset_index(inplace=True)
        df.rename(columns={'Date': 'ì¡°ì‚¬ì¼ì', 'Close': f'{name} ì„ ë¬¼ê°€ê²©(USD)'}, inplace=True)
        return df
    except Exception as e:
        st.error(f"Yahoo Finance ('{name}') ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

@st.cache_data(ttl=3600)
def fetch_google_trends(keyword, start_date, end_date):
    """Google Trendsì—ì„œ ì§€ì •ëœ ê¸°ê°„ì˜ ê²€ìƒ‰ëŸ‰ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    pytrends = TrendReq(hl='ko-KR', tz=540)
    timeframe = f"{start_date.strftime('%Y-%m-%d')} {end_date.strftime('%Y-%m-%d')}"
    
    try:
        pytrends.build_payload([keyword], cat=0, timeframe=timeframe, geo='KR', gprop='')
        df = pytrends.interest_over_time()
        if df.empty or keyword not in df.columns:
            st.warning(f"'{keyword}'ì— ëŒ€í•œ Google Trends ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        df.reset_index(inplace=True)
        df.rename(columns={'date': 'ë‚ ì§œ', keyword: f'Google_{keyword}'}, inplace=True)
        return df[['ë‚ ì§œ', f'Google_{keyword}']]
    except Exception as e:
        st.error(f"Google Trends ('{keyword}') ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

@st.cache_data(ttl=3600)
def fetch_naver_datalab(client_id, client_secret, keyword, start_date, end_date):
    """Naver DataLab APIë¥¼ í˜¸ì¶œí•˜ì—¬ ê²€ìƒ‰ëŸ‰ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        url = "https://openapi.naver.com/v1/datalab/search"
        body = {
            "startDate": start_date.strftime('%Y-%m-%d'),
            "endDate": end_date.strftime('%Y-%m-%d'),
            "timeUnit": "date",
            "keywordGroups": [{"groupName": keyword, "keywords": [keyword]}]
        }
        body = json.dumps(body)

        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", client_id)
        request.add_header("X-Naver-Client-Secret", client_secret)
        request.add_header("Content-Type", "application/json")
        response = urllib.request.urlopen(request, data=body.encode("utf-8"))
        
        rescode = response.getcode()
        if rescode == 200:
            response_body = response.read()
            result = json.loads(response_body.decode('utf-8'))
            df = pd.DataFrame(result['results'][0]['data'])
            df.rename(columns={'period': 'ë‚ ì§œ', 'ratio': f'Naver_{keyword}'}, inplace=True)
            df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
            return df
        else:
            st.error(f"Naver API ì˜¤ë¥˜ ë°œìƒ ('{keyword}'): Error Code {rescode}")
            return None
    except Exception as e:
        st.error(f"Naver DataLab ('{keyword}') API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

@st.cache_data(ttl=3600)
def fetch_kamis_data(api_key, api_id, item_code, category_code, start_date, end_date):
    """KAMISì—ì„œ ì¼ë³„ í’ˆëª©ë³„ ë„ë§¤ê°€ê²© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    all_data = []
    date_range = pd.date_range(start=start_date, end=end_date)
    
    # [ìˆ˜ì •] ì¡°íšŒ ê¸°ê°„ì´ ê¸¸ ê²½ìš° ê²½ê³  ë©”ì‹œì§€ í‘œì‹œ
    if len(date_range) > 180:
        st.sidebar.warning(f"ì¡°íšŒ ê¸°ê°„ì´ {len(date_range)}ì¼ë¡œ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ë¡œë”©ì— ë§¤ìš° ì˜¤ëœ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    progress_bar = st.sidebar.progress(0, text="KAMIS ë°ì´í„° ì¡°íšŒ ì¤‘...")
    
    for i, date in enumerate(date_range):
        date_str = date.strftime('%Y-%m-%d')
        url = (
            "http://www.kamis.or.kr/service/price/xml.do"
            f"?p_product_cls_code=02&p_item_category_code={category_code}"
            f"&p_item_code={item_code}&p_regday={date_str}"
            "&p_convert_kg_yn=Y"
            f"&p_cert_key={api_key}&p_cert_id={api_id}&p_returntype=json"
        )
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            data = response.json()
            
            items = data.get("data", {}).get("item", [])
            if items:
                price = items[0].get('dpr1', '0').replace(',', '')
                if price and int(price) > 0:
                    all_data.append({'ì¡°ì‚¬ì¼ì': date, 'ë„ë§¤ê°€ê²©(ì›)': int(price)})
        except (requests.exceptions.RequestException, json.JSONDecodeError, IndexError) as e:
            print(f"KAMIS data fetch error for {date_str}: {e}")
        
        progress_bar.progress((i + 1) / len(date_range), text=f"KAMIS ë°ì´í„° ì¡°íšŒ ì¤‘... {date_str}")
    
    progress_bar.empty()
    if not all_data:
        st.sidebar.warning("í•´ë‹¹ ê¸°ê°„ì— ëŒ€í•œ KAMIS ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
        
    df = pd.DataFrame(all_data)
    return df

# --- Constants ---
COFFEE_TICKERS_YFINANCE = {"ë¯¸êµ­ ì»¤í”¼ C": "KC=F", "ëŸ°ë˜ ë¡œë¶€ìŠ¤íƒ€": "RC=F"}

# --- [ìˆ˜ì •] KAMIS í’ˆëª© ì„ íƒì§€ ëŒ€í­ í™•ì¥ ---
KAMIS_CATEGORIES = {
    "ì±„ì†Œë¥˜": "100", "ê³¼ì¼ë¥˜": "200", "ì¶•ì‚°ë¬¼": "300", "ìˆ˜ì‚°ë¬¼": "400"
}
KAMIS_ITEMS = {
    "ì±„ì†Œë¥˜": {"ë°°ì¶”": "111", "ë¬´": "112", "ì–‘íŒŒ": "114", "ë§ˆëŠ˜": "141", "ì˜¤ì´": "123", "í† ë§ˆí† ": "126"},
    "ê³¼ì¼ë¥˜": {"ì‚¬ê³¼": "211", "ë°°": "212", "ë°”ë‚˜ë‚˜": "214", "ì•„ë³´ì¹´ë„": "215", "ì˜¤ë Œì§€": "223", "ë ˆëª¬": "224"},
    "ì¶•ì‚°ë¬¼": {"ì†Œê³ ê¸°": "311", "ë¼ì§€ê³ ê¸°": "312", "ë‹­ê³ ê¸°": "313", "ê³„ë€": "314"},
    "ìˆ˜ì‚°ë¬¼": {"ê³ ë“±ì–´": "411", "ì˜¤ì§•ì–´": "413", "ìƒˆìš°": "421", "ì—°ì–´": "423"}
}


# --- Streamlit App ---
st.set_page_config(layout="wide")
st.title("ğŸ“Š ë°ì´í„° íƒìƒ‰ ë° í†µí•© ëŒ€ì‹œë³´ë“œ")
st.info("ê°ê¸° ë‹¤ë¥¸ ì†ŒìŠ¤ì˜ ì›ë³¸ ë°ì´í„°ë¥¼ í™•ì¸í•˜ê³ , ì´ë“¤ì´ ì–´ë–»ê²Œ í•˜ë‚˜ì˜ ë¶„ì„ìš© ë°ì´í„°ì…‹ìœ¼ë¡œ í†µí•©ë˜ëŠ”ì§€ ë‹¨ê³„ë³„ë¡œ ì‚´í´ë´…ë‹ˆë‹¤.")

# --- Sidebar Controls ---
st.sidebar.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
uploaded_file = st.sidebar.file_uploader("1. ìˆ˜ì¶œì… ë°ì´í„° íŒŒì¼ ì—…ë¡œë“œ (CSV or Excel)", type=['csv', 'xlsx'])

# Initialize variables
raw_trade_df, selected_categories, search_keywords = None, [], []
start_date, end_date = None, None

if uploaded_file:
    raw_trade_df = load_trade_data(uploaded_file)
    if raw_trade_df is not None:
        file_start_date, file_end_date = raw_trade_df['Date'].min(), raw_trade_df['Date'].max()
        
        category_options = sorted(raw_trade_df['Category'].unique())
        selected_categories = st.sidebar.multiselect("2. ë¶„ì„í•  í’ˆëª© ì¹´í…Œê³ ë¦¬ ì„ íƒ", category_options, default=category_options[0] if category_options else None)
        keyword_input = st.sidebar.text_input("3. ê²€ìƒ‰ì–´ ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„)", ", ".join(selected_categories) if selected_categories else "")
        search_keywords = [k.strip() for k in keyword_input.split(',') if k.strip()]

        st.sidebar.subheader("4. ë¶„ì„ ê¸°ê°„ ì„¤ì •")
        start_date = st.sidebar.date_input('ì‹œì‘ì¼', file_start_date, min_value=file_start_date, max_value=file_end_date)
        end_date = st.sidebar.date_input('ì¢…ë£Œì¼', file_end_date, min_value=start_date, max_value=file_end_date)

    else: st.stop()
else:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ìˆ˜ì¶œì… ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."); st.stop()

# --- External Data Loading Section ---
raw_wholesale_df = None
st.sidebar.subheader("ğŸ”— ì™¸ë¶€ ê°€ê²© ë°ì´í„°")

is_coffee_selected = any('ì»¤í”¼' in str(cat) for cat in selected_categories)

if is_coffee_selected:
    st.sidebar.info("ì»¤í”¼ ê´€ë ¨ í’ˆëª©ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.\nYahoo Financeì—ì„œ ì„ ë¬¼ê°€ê²©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    if st.sidebar.button("ì„ ë¬¼ê°€ê²© ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (Yahoo Finance)"):
        all_futures_data = []
        for name, ticker in COFFEE_TICKERS_YFINANCE.items():
             with st.spinner(f"{name}({ticker}) ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                data = fetch_yfinance_data(ticker, name, start_date, end_date)
                if data is not None: all_futures_data.append(data)
        if all_futures_data:
            st.session_state['futures_data'] = reduce(lambda left, right: pd.merge(left, right, on='ì¡°ì‚¬ì¼ì', how='outer'), all_futures_data)
            st.sidebar.success("ì„ ë¬¼ê°€ê²© ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
        else: st.sidebar.error("ì„ ë¬¼ê°€ê²© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    if 'futures_data' in st.session_state:
        raw_wholesale_df = st.session_state['futures_data'].sort_values('ì¡°ì‚¬ì¼ì')
else:
    st.sidebar.info("KAMISì—ì„œ ë†ì‚°ë¬¼ ë„ë§¤ê°€ê²© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    kamis_api_key = st.sidebar.text_input("KAMIS API Key", value="9b2f72e0-1909-4c08-8f4f-9b6f55b44c88", type="password")
    kamis_api_id = st.sidebar.text_input("KAMIS API ID", type="password")
    
    selected_kamis_category_name = st.sidebar.selectbox("í’ˆëª© ë¶„ë¥˜ ì„ íƒ", list(KAMIS_CATEGORIES.keys()))
    if selected_kamis_category_name:
        kamis_category_code = KAMIS_CATEGORIES[selected_kamis_category_name]
        item_options = KAMIS_ITEMS[selected_kamis_category_name]
        selected_item_name = st.sidebar.selectbox("ì„¸ë¶€ í’ˆëª© ì„ íƒ", list(item_options.keys()))
        kamis_item_code = item_options[selected_item_name]

    if st.sidebar.button("KAMIS ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
        if kamis_api_key and kamis_api_id:
            kamis_df = fetch_kamis_data(kamis_api_key, kamis_api_id, kamis_item_code, kamis_category_code, start_date, end_date)
            st.session_state['kamis_data'] = kamis_df
            if kamis_df is not None: st.sidebar.success("KAMIS ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
        else: st.sidebar.error("KAMIS API Keyì™€ IDë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    if 'kamis_data' in st.session_state: raw_wholesale_df = st.session_state['kamis_data']

# --- Search/News Data Loading Section ---
st.sidebar.subheader("ğŸ“° ê²€ìƒ‰ëŸ‰/ë‰´ìŠ¤ ë°ì´í„°")
naver_client_id = st.sidebar.text_input("Naver API Client ID", type="password")
naver_client_secret = st.sidebar.text_input("Naver API Client Secret", type="password")

if st.sidebar.button("Google/Naver ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
    if not search_keywords: st.sidebar.warning("ê²€ìƒ‰ì–´ë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        all_trends_data = []
        for keyword in search_keywords:
            with st.spinner(f"'{keyword}' Google Trends ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                google_data = fetch_google_trends(keyword, start_date, end_date)
                if google_data is not None: all_trends_data.append(google_data)
            if naver_client_id and naver_client_secret:
                with st.spinner(f"'{keyword}' Naver DataLab ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
                    naver_data = fetch_naver_datalab(naver_client_id, naver_client_secret, keyword, start_date, end_date)
                    if naver_data is not None: all_trends_data.append(naver_data)
        if all_trends_data:
            st.session_state['search_data'] = reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), all_trends_data)
            st.sidebar.success("ê²€ìƒ‰ëŸ‰ ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
        else: st.sidebar.error("ê²€ìƒ‰ëŸ‰ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

raw_search_df = st.session_state.get('search_data', pd.DataFrame({'ë‚ ì§œ': pd.to_datetime([])}))
if not raw_search_df.empty: raw_search_df.sort_values('ë‚ ì§œ', inplace=True)

# --- Main Display Area ---
tab1, tab2, tab3 = st.tabs(["1ï¸âƒ£ ì›ë³¸ ë°ì´í„° í™•ì¸", "2ï¸âƒ£ ë°ì´í„° í‘œì¤€í™” (Preprocessing)", "3ï¸âƒ£ ìµœì¢… í†µí•© ë°ì´í„°"])

with tab1:
    st.header("1. ê°ê¸° ë‹¤ë¥¸ ë°ì´í„° ì†ŒìŠ¤ì˜ ì›ë³¸ í˜•íƒœ")
    st.subheader("A. ìˆ˜ì¶œì… ë°ì´í„° (ì‚¬ìš©ì ì œê³µ)"); st.dataframe(raw_trade_df.head())
    st.subheader("B. ì™¸ë¶€ ê°€ê²© ë°ì´í„° (ì„ ë¬¼/ë„ë§¤)")
    if raw_wholesale_df is not None: st.dataframe(raw_wholesale_df.head())
    else: st.warning("ì‚¬ì´ë“œë°”ì—ì„œ ì™¸ë¶€ ê°€ê²© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    st.subheader("C. ê²€ìƒ‰ëŸ‰ ë°ì´í„° (Google/Naver)")
    if not raw_search_df.empty: st.dataframe(raw_search_df.head())
    else: st.warning("ì‚¬ì´ë“œë°”ì—ì„œ ê²€ìƒ‰ëŸ‰ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”.")

with tab2:
    st.header("2. ë°ì´í„° í‘œì¤€í™”: ê°™ì€ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë§ì¶°ì£¼ê¸°")
    if not selected_categories: st.warning("ë¶„ì„í•  ì¹´í…Œê³ ë¦¬ë¥¼ ì‚¬ì´ë“œë°”ì—ì„œ í•˜ë‚˜ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
    else:
        st.subheader("2-1. ë¶„ì„ ëŒ€ìƒ í’ˆëª© í•„í„°ë§")
        trade_df_in_range = raw_trade_df[(raw_trade_df['Date'] >= pd.to_datetime(start_date)) & (raw_trade_df['Date'] <= pd.to_datetime(end_date))]
        filtered_trade_df = trade_df_in_range[trade_df_in_range['Category'].isin(selected_categories)].copy()
        
        st.write(f"ì„ íƒëœ ì¹´í…Œê³ ë¦¬: **{', '.join(selected_categories)}**"); st.dataframe(filtered_trade_df.head())
        st.subheader("2-2. ì£¼(Week) ë‹¨ìœ„ ë°ì´í„°ë¡œ ì§‘ê³„")
        if not filtered_trade_df.empty:
            filtered_trade_df.set_index('Date', inplace=True)
            trade_weekly = filtered_trade_df.resample('W-Mon').agg({'Value': 'sum', 'Volume': 'sum'})
            trade_weekly['Unit Price'] = trade_weekly['Value'] / trade_weekly['Volume']
            trade_weekly.columns = ['ìˆ˜ì…ì•¡(USD)', 'ìˆ˜ì…ëŸ‰(KG)', 'ìˆ˜ì…ë‹¨ê°€(USD/KG)']
            
            wholesale_weekly = pd.DataFrame()
            if raw_wholesale_df is not None:
                wholesale_df_processed = raw_wholesale_df.set_index('ì¡°ì‚¬ì¼ì')
                price_cols = [col for col in wholesale_df_processed.columns if 'ê°€ê²©' in col]
                agg_dict = {col: 'mean' for col in price_cols}
                if agg_dict:
                    wholesale_weekly = wholesale_df_processed.resample('W-Mon').agg(agg_dict)
                    if 'ë„ë§¤ê°€ê²©(ì›)' in wholesale_weekly.columns:
                        wholesale_weekly['ë„ë§¤ê°€ê²©(USD)'] = wholesale_weekly['ë„ë§¤ê°€ê²©(ì›)'] / 1350
                        wholesale_weekly.drop(columns=['ë„ë§¤ê°€ê²©(ì›)'], inplace=True)
            
            search_weekly = pd.DataFrame()
            if not raw_search_df.empty:
                search_df_processed = raw_search_df.set_index('ë‚ ì§œ')
                numeric_cols = search_df_processed.select_dtypes(include=np.number).columns
                search_weekly = search_df_processed.resample('W-Mon').agg({col: 'mean' for col in numeric_cols})
            
            st.write("â–¼ ì¼ë³„(Daily) ë°ì´í„°ê°€ ì£¼ë³„(Weekly) ë°ì´í„°ë¡œ ì§‘ê³„ëœ ê²°ê³¼")
            col1, col2 = st.columns(2)
            with col1: st.write("**Before (ì¼ë³„ ìˆ˜ì…ëŸ‰)**"); st.line_chart(filtered_trade_df['Volume'])
            with col2: st.write("**After (ì£¼ë³„ ìˆ˜ì…ëŸ‰)**"); st.line_chart(trade_weekly['ìˆ˜ì…ëŸ‰(KG)'])
        else: st.warning("ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

with tab3:
    st.header("3. ìµœì¢… í†µí•© ë°ì´í„°ì…‹")
    if 'trade_weekly' in locals() and not trade_weekly.empty:
        dfs_to_concat = [trade_weekly]
        if 'wholesale_weekly' in locals() and not wholesale_weekly.empty: dfs_to_concat.append(wholesale_weekly)
        if 'search_weekly' in locals() and not search_weekly.empty: dfs_to_concat.append(search_weekly)
        
        final_df = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='outer'), dfs_to_concat)
        final_df = final_df.dropna(thresh=2).fillna(method='ffill')
        
        st.dataframe(final_df)
        st.subheader("ìµœì¢… ë°ì´í„° ì‹œê°í™”")
        fig = px.line(final_df, labels={'value': 'ê°’', 'index': 'ë‚ ì§œ', 'variable': 'ë°ì´í„° ì¢…ë¥˜'}, title="ìµœì¢… í†µí•© ë°ì´í„° ì‹œê³„ì—´ ì¶”ì´")
        st.plotly_chart(fig, use_container_width=True)
    else: st.warning("í†µí•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. 2ë‹¨ê³„ í‘œì¤€í™” ê³¼ì •ì„ ë¨¼ì € í™•ì¸í•´ì£¼ì„¸ìš”.")

