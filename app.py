import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import requests
from datetime import datetime
from bs4 import BeautifulSoup
from functools import reduce
from pytrends.request import TrendReq
import json
import urllib.request

# --- Web Scraping Function ---
@st.cache_data(ttl=3600) # ë°ì´í„°ë¥¼ 1ì‹œê°„ ë™ì•ˆ ìºì‹±
def fetch_investing_data(index_name, url):
    """investing.comì—ì„œ ì§€ì •ëœ ìƒí’ˆì˜ ê³¼ê±° ë°ì´í„°ë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤."""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        table = soup.find('table', {'data-test': 'historical-data-table'})
        if not table:
            st.error(f"Investing.com í˜ì´ì§€({index_name})ì—ì„œ ë°ì´í„° í…Œì´ë¸”ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        dates, prices = [], []
        for row in table.find('tbody').find_all('tr'):
            cells = row.find_all('td')
            if len(cells) > 1:
                date_str = cells[0].find('time')['datetime']
                price_str = cells[1].text.strip().replace(',', '')
                dates.append(pd.to_datetime(date_str))
                prices.append(float(price_str))

        column_name = f'{index_name} ì„ ë¬¼ê°€ê²©(USD)'
        df = pd.DataFrame({'ì¡°ì‚¬ì¼ì': dates, column_name: prices})
        return df

    except requests.exceptions.RequestException as e:
        st.error(f"ì›¹ì‚¬ì´íŠ¸({index_name})ì— ì ‘ì†í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None
    except Exception as e:
        st.error(f"ë°ì´í„°({index_name})ë¥¼ íŒŒì‹±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# --- Real-time Data Fetching Functions ---
@st.cache_data(ttl=3600)
def fetch_google_trends(keyword, start_date, end_date):
    """Google Trendsì—ì„œ ì§€ì •ëœ ê¸°ê°„ì˜ ê²€ìƒ‰ëŸ‰ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    pytrends = TrendReq(hl='ko-KR', tz=540)
    timeframe = f"{start_date.strftime('%Y-%m-%d')} {end_date.strftime('%Y-%m-%d')}"
    
    try:
        pytrends.build_payload([keyword], cat=0, timeframe=timeframe, geo='KR', gprop='')
        df = pytrends.interest_over_time()
        if df.empty or keyword not in df.columns:
            st.warning(f"'{keyword}'ì— ëŒ€í•œ Google Trends ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        df.reset_index(inplace=True)
        df.rename(columns={'date': 'ë‚ ì§œ', keyword: 'Google ê²€ìƒ‰ëŸ‰'}, inplace=True)
        return df[['ë‚ ì§œ', 'Google ê²€ìƒ‰ëŸ‰']]
    except Exception as e:
        st.error(f"Google Trends ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

@st.cache_data(ttl=3600)
def fetch_naver_datalab(client_id, client_secret, keyword, start_date, end_date):
    """Naver DataLab APIë¥¼ í˜¸ì¶œí•˜ì—¬ ê²€ìƒ‰ëŸ‰ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        url = "https://openapi.naver.com/v1/datalab/search"
        body = {
            "startDate": start_date.strftime('%Y-%m-%d'),
            "endDate": end_date.strftime('%Y-%m-%d'),
            "timeUnit": "date",
            "keywordGroups": [{"groupName": keyword, "keywords": [keyword]}]
        }
        body = json.dumps(body)

        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", client_id)
        request.add_header("X-Naver-Client-Secret", client_secret)
        request.add_header("Content-Type", "application/json")
        response = urllib.request.urlopen(request, data=body.encode("utf-8"))
        
        rescode = response.getcode()
        if rescode == 200:
            response_body = response.read()
            result = json.loads(response_body.decode('utf-8'))
            df = pd.DataFrame(result['results'][0]['data'])
            df.rename(columns={'period': 'ë‚ ì§œ', 'ratio': 'Naver ê²€ìƒ‰ëŸ‰'}, inplace=True)
            df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
            return df
        else:
            st.error(f"Naver API ì˜¤ë¥˜ ë°œìƒ: Error Code {rescode}")
            return None
    except Exception as e:
        st.error(f"Naver DataLab API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# --- Streamlit App ---
st.set_page_config(layout="wide")
st.title("ğŸ“Š ë°ì´í„° íƒìƒ‰ ë° í†µí•© ëŒ€ì‹œë³´ë“œ")
st.info("ê°ê¸° ë‹¤ë¥¸ ì†ŒìŠ¤ì˜ ì›ë³¸ ë°ì´í„°ë¥¼ í™•ì¸í•˜ê³ , ì´ë“¤ì´ ì–´ë–»ê²Œ í•˜ë‚˜ì˜ ë¶„ì„ìš© ë°ì´í„°ì…‹ìœ¼ë¡œ í†µí•©ë˜ëŠ”ì§€ ë‹¨ê³„ë³„ë¡œ ì‚´í´ë´…ë‹ˆë‹¤.")

# --- Sidebar Controls ---
st.sidebar.header("âš™ï¸ ë¶„ì„ ì„¤ì •")
uploaded_file = st.sidebar.file_uploader("ìˆ˜ì¶œì… ë°ì´í„° íŒŒì¼ ì—…ë¡œë“œ (CSV or Excel)", type=['csv', 'xlsx'])

st.sidebar.subheader("ë¶„ì„ ëŒ€í‘œ í’ˆëª©")
selected_product_category = st.sidebar.selectbox("ë¶„ì„í•  ëŒ€í‘œ í’ˆëª© ì„ íƒ", ['ì¸ìŠ¤í„´íŠ¸ ì»¤í”¼', 'ì›ë‘ ì»¤í”¼', 'ìº¡ìŠ ì»¤í”¼', 'ì•„ë³´ì¹´ë„'])

# --- Keyword Mapping & Constants ---
KEYWORD_MAPPING = {
    'ë§¥ì‹¬ ëª¨ì¹´ê³¨ë“œ': 'ì¸ìŠ¤í„´íŠ¸ ì»¤í”¼', 'ìŠ¤íƒ€ë²…ìŠ¤ íŒŒì´í¬í”Œë ˆì´ìŠ¤': 'ì›ë‘ ì»¤í”¼', 'ë„¤ìŠ¤ì¹´í˜ ëŒì²´êµ¬ìŠ¤í† ': 'ìº¡ìŠ ì»¤í”¼',
    'ì»¤í”¼': 'ì¸ìŠ¤í„´íŠ¸ ì»¤í”¼', 'ì¸ìŠ¤í„´íŠ¸ ì»¤í”¼': 'ì¸ìŠ¤í„´íŠ¸ ì»¤í”¼', 'ì•„ë³´ì¹´ë„': 'ì•„ë³´ì¹´ë„'
}
COFFEE_INDICES = {
    "ëŸ°ë˜ ì»¤í”¼": "https://kr.investing.com/commodities/london-coffee-historical-data",
    "ë¯¸êµ­ ì»¤í”¼ C": "https://kr.investing.com/commodities/us-coffee-c-historical-data"
}

# --- Data Loading Logic ---
raw_trade_df = None
if uploaded_file:
    try:
        raw_trade_df = pd.read_csv(uploaded_file) if uploaded_file.name.endswith('.csv') else pd.read_excel(uploaded_file)
        if 'Date' in raw_trade_df.columns:
            raw_trade_df['Date'] = pd.to_datetime(raw_trade_df['Date'])
            start_date = raw_trade_df['Date'].min()
            end_date = raw_trade_df['Date'].max()
        else:
            st.error("ì—…ë¡œë“œëœ íŒŒì¼ì— 'Date' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()
    except Exception as e:
        st.error(f"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.stop()
else:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ìˆ˜ì¶œì… ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    st.stop()

# External Price Data Loading
raw_wholesale_df = None
st.sidebar.subheader("ğŸ”— ì™¸ë¶€ ê°€ê²© ë°ì´í„°")
if 'ì»¤í”¼' in selected_product_category:
    st.sidebar.info("ì»¤í”¼ í’ˆëª©ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.\nInvesting.comì—ì„œ ì„ ë¬¼ê°€ê²©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    for name, url in COFFEE_INDICES.items():
        if st.sidebar.button(f"{name} ì„ ë¬¼ê°€ê²© ê°€ì ¸ì˜¤ê¸°"):
            with st.spinner(f"Investing.comì—ì„œ {name} ë°ì´í„°ë¥¼ ìŠ¤í¬ë˜í•‘ ì¤‘..."):
                data = fetch_investing_data(name, url)
                if data is not None:
                    st.session_state[f'{name}_data'] = data
                    st.sidebar.success(f"{name} ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
    loaded_futures_dfs = [st.session_state[f'{name}_data'] for name in COFFEE_INDICES if f'{name}_data' in st.session_state]
    if loaded_futures_dfs:
        raw_wholesale_df = reduce(lambda left, right: pd.merge(left, right, on='ì¡°ì‚¬ì¼ì', how='outer'), loaded_futures_dfs)
        raw_wholesale_df.sort_values('ì¡°ì‚¬ì¼ì', inplace=True)
else:
    wholesale_data_file = st.sidebar.file_uploader("ë„ë§¤ê°€ê²© ë°ì´í„° ì—…ë¡œë“œ (KAMIS ë“±)", type=['csv', 'xlsx'])
    # ... (file upload logic for non-coffee items) ...

# Search/News Data Loading
st.sidebar.subheader("ğŸ“° ê²€ìƒ‰ëŸ‰/ë‰´ìŠ¤ ë°ì´í„°")
naver_client_id = st.sidebar.text_input("Naver API Client ID", type="password")
naver_client_secret = st.sidebar.text_input("Naver API Client Secret", type="password")

if st.sidebar.button("Google/Naver ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"):
    with st.spinner("Google Trends ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
        google_data = fetch_google_trends(selected_product_category, start_date, end_date)
        if google_data is not None: st.session_state['google_trends_data'] = google_data
    
    if naver_client_id and naver_client_secret:
         with st.spinner("Naver DataLab ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
            naver_data = fetch_naver_datalab(naver_client_id, naver_client_secret, selected_product_category, start_date, end_date)
            if naver_data is not None: st.session_state['naver_search_data'] = naver_data
    else:
        st.sidebar.warning("Naver API í‚¤ë¥¼ ì…ë ¥í•˜ë©´ Naver ê²€ìƒ‰ëŸ‰ë„ í•¨ê»˜ ê°€ì ¸ì˜µë‹ˆë‹¤.")

# Merge Search Data
loaded_search_dfs = []
if 'google_trends_data' in st.session_state: loaded_search_dfs.append(st.session_state['google_trends_data'])
if 'naver_search_data' in st.session_state: loaded_search_dfs.append(st.session_state['naver_search_data'])

raw_search_df = pd.DataFrame({'ë‚ ì§œ': pd.to_datetime([])})
if loaded_search_dfs:
    raw_search_df = reduce(lambda left, right: pd.merge(left, right, on='ë‚ ì§œ', how='outer'), loaded_search_dfs)
    raw_search_df.sort_values('ë‚ ì§œ', inplace=True)

# --- Display Tabs ---
tab1, tab2, tab3 = st.tabs(["1ï¸âƒ£ ì›ë³¸ ë°ì´í„° í™•ì¸", "2ï¸âƒ£ ë°ì´í„° í‘œì¤€í™” (Preprocessing)", "3ï¸âƒ£ ìµœì¢… í†µí•© ë°ì´í„°"])

with tab1:
    st.header("1. ê°ê¸° ë‹¤ë¥¸ ë°ì´í„° ì†ŒìŠ¤ì˜ ì›ë³¸ í˜•íƒœ")
    st.subheader("A. ìˆ˜ì¶œì… ë°ì´í„° (ì‚¬ìš©ì ì œê³µ)")
    st.dataframe(raw_trade_df.head())
    st.subheader("B. ì™¸ë¶€ ê°€ê²© ë°ì´í„° (ì„ ë¬¼/ë„ë§¤)")
    if raw_wholesale_df is not None: st.dataframe(raw_wholesale_df.head())
    else: st.warning("ì‚¬ì´ë“œë°”ì—ì„œ ì™¸ë¶€ ê°€ê²© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    st.subheader("C. ê²€ìƒ‰ëŸ‰ ë°ì´í„° (Google/Naver)")
    if not raw_search_df.empty: st.dataframe(raw_search_df.head())
    else: st.warning("ì‚¬ì´ë“œë°”ì—ì„œ ê²€ìƒ‰ëŸ‰ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”.")

with tab2:
    st.header("2. ë°ì´í„° í‘œì¤€í™”: ê°™ì€ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë§ì¶°ì£¼ê¸°")
    st.subheader("2-1. í’ˆëª© ì´ë¦„ í†µí•© (Keyword Mapping)")
    filtered_trade_df = raw_trade_df.copy()
    if 'Reported Product Name' in filtered_trade_df.columns:
        filtered_trade_df['ëŒ€í‘œ í’ˆëª©'] = filtered_trade_df['Reported Product Name'].map(KEYWORD_MAPPING)
        filtered_trade_df = filtered_trade_df[filtered_trade_df['ëŒ€í‘œ í’ˆëª©'] == selected_product_category]
        if not filtered_trade_df.empty:
            st.dataframe(filtered_trade_df[['Date', 'Reported Product Name', 'ëŒ€í‘œ í’ˆëª©', 'Value', 'Volume']].head())
        else:
            st.warning(f"ìˆ˜ì¶œì… ë°ì´í„°ì—ì„œ '{selected_product_category}'ì— í•´ë‹¹í•˜ëŠ” í’ˆëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    st.subheader("2-2. ì£¼(Week) ë‹¨ìœ„ ë°ì´í„°ë¡œ ì§‘ê³„")
    if not filtered_trade_df.empty:
        filtered_trade_df = filtered_trade_df.set_index('Date')
        trade_weekly = filtered_trade_df.resample('W-Mon').agg({'Value': 'sum', 'Volume': 'sum'})
        trade_weekly['Unit Price'] = trade_weekly['Value'] / trade_weekly['Volume']
        trade_weekly.columns = ['ìˆ˜ì…ì•¡(USD)', 'ìˆ˜ì…ëŸ‰(KG)', 'ìˆ˜ì…ë‹¨ê°€(USD/KG)']

        wholesale_weekly = pd.DataFrame()
        if raw_wholesale_df is not None:
            # ... (Wholesale weekly aggregation logic) ...
            wholesale_df_processed = raw_wholesale_df.set_index('ì¡°ì‚¬ì¼ì')
            price_cols = [col for col in wholesale_df_processed.columns if 'ê°€ê²©' in col]
            agg_dict = {col: 'mean' for col in price_cols}
            if agg_dict:
                wholesale_weekly = wholesale_df_processed.resample('W-Mon').agg(agg_dict)
                if 'ë„ë§¤ê°€ê²©(ì›)' in wholesale_weekly.columns:
                    wholesale_weekly['ë„ë§¤ê°€ê²©(USD)'] = wholesale_weekly['ë„ë§¤ê°€ê²©(ì›)'] / 1350
                    wholesale_weekly = wholesale_weekly.drop(columns=['ë„ë§¤ê°€ê²©(ì›)'])
        
        search_weekly = pd.DataFrame()
        if not raw_search_df.empty:
            search_df_processed = raw_search_df.set_index('ë‚ ì§œ')
            numeric_cols = search_df_processed.select_dtypes(include=np.number).columns
            search_weekly = search_df_processed.resample('W-Mon').agg({col: 'mean' for col in numeric_cols})
        
        col1, col2 = st.columns(2)
        with col1: st.write("**Before (ì¼ë³„ ìˆ˜ì…ëŸ‰)**"); st.line_chart(filtered_trade_df['Volume'])
        with col2: st.write("**After (ì£¼ë³„ ìˆ˜ì…ëŸ‰)**"); st.line_chart(trade_weekly['ìˆ˜ì…ëŸ‰(KG)'])

with tab3:
    st.header("3. ìµœì¢… í†µí•© ë°ì´í„°ì…‹")
    if 'trade_weekly' in locals() and not trade_weekly.empty:
        dfs_to_concat = [trade_weekly]
        if 'wholesale_weekly' in locals() and not wholesale_weekly.empty: dfs_to_concat.append(wholesale_weekly)
        if 'search_weekly' in locals() and not search_weekly.empty: dfs_to_concat.append(search_weekly)
        
        final_df = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='outer'), dfs_to_concat)
        final_df = final_df.dropna(thresh=2)
        
        st.dataframe(final_df)
        st.subheader("ìµœì¢… ë°ì´í„° ì‹œê°í™”")
        fig = px.line(final_df, labels={'value': 'ê°’', 'index': 'ë‚ ì§œ', 'variable': 'ë°ì´í„° ì¢…ë¥˜'}, title="ìµœì¢… í†µí•© ë°ì´í„° ì‹œê³„ì—´ ì¶”ì´")
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.warning("í†µí•©í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. 2ë‹¨ê³„ í‘œì¤€í™” ê³¼ì •ì„ ë¨¼ì € í™•ì¸í•´ì£¼ì„¸ìš”.")

